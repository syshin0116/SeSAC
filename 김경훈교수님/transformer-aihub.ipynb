{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Annotated Transformer 보다 친절한 트랜스포머 튜토리얼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 데이터 가공\n",
    "- 먼저, 튜토리얼에 사용하기 위해 [**AI Hub**](http://www.aihub.or.kr/)에서 [**한국어-영어 번역 말뭉치**](http://www.aihub.or.kr/aidata/87) 데이터 다운로드를 요청합니다.\n",
    "- 다운로드 요청 후, 약 **2일** 내에 승인 결과가 메일로 전달된다고 합니다.\n",
    "- 말뭉치는 기본적으로 **엑셀 파일**로 제공되지만, 실험의 편의를 위해 **CSV** 파일로 변환해 사용하도록 합니다.\n",
    "\n",
    "_cf. 현재 AI Hub에서는 다양한 한영 번역 데이터셋을 구축해 총 **160만 쌍**의 데이터를 제공해주고 있지만, 모든 문장을 훈련시키기에는 데이터가 과도하므로 본 튜토리얼에서는 **구어체 데이터 1 & 2**만을 사용하도록 합니다._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/syshin/miniforge3/envs/pytorch/lib/python3.10/site-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    }
   ],
   "source": [
    "# 두 개 코퍼스 파일 CSV 포맷으로 변환 저장\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "xls_a = pd.read_excel('./한국어-영어 번역(병렬) 말뭉치/1_구어체(1).xlsx', index_col=None)\n",
    "xls_b = pd.read_excel('./한국어-영어 번역(병렬) 말뭉치/1_구어체(2).xlsx', index_col=None)\n",
    "\n",
    "xls_a.to_csv('./한국어-영어 번역(병렬) 말뭉치/spoken1.csv', encoding='utf-8', index=False)\n",
    "xls_b.to_csv('./한국어-영어 번역(병렬) 말뭉치/spoken2.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제, **CSV**로 변환한 말뭉치 파일을 `pandas` 라이브러리를 이용해 읽어옵니다.\n",
    "- 읽어온 파일의 각 행을 돌며, **한국어 문장**과 **영어 문장**을 각각의 리스트에 저장해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV로 변환한 말뭉치 파일 로드 및 합병\n",
    "\n",
    "data_a = pd.read_csv('./한국어-영어 번역(병렬) 말뭉치/spoken1.csv', encoding='utf-8')\n",
    "data_b = pd.read_csv('./한국어-영어 번역(병렬) 말뭉치/spoken2.csv', encoding='utf-8')\n",
    "\n",
    "data = pd.concat([data_b, data_a], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SID</th>\n",
       "      <th>원문</th>\n",
       "      <th>번역문</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200001</td>\n",
       "      <td>0 설정을 입력하고 안정될 때까지 5분 동안 기다린 후 OK 버튼을 길게 누르십시오.</td>\n",
       "      <td>Enter into 0 setting, and wait for 5 minutes t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200002</td>\n",
       "      <td>0은 그들에게 아무것도 아니었지만 무는 숫자일 수가 없습니다.</td>\n",
       "      <td>The zero was nothing for them but nothing coul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200003</td>\n",
       "      <td>1,015버전에서 핫키 버그가 있습니다.</td>\n",
       "      <td>There is a Hotkey bug in the 1,015 version.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200004</td>\n",
       "      <td>1,390점에서 1,440점을 득점한 사람은 재판을 위해 걸러집니다.</td>\n",
       "      <td>Individuals who got a score between 1,390 and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200005</td>\n",
       "      <td>1,400년보다 오래 전의 유적지에 있는 최초의 성당에서 숭배자들은 그것을 인지했을...</td>\n",
       "      <td>Indeed, worshippers at the very first cathedra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399995</th>\n",
       "      <td>199996</td>\n",
       "      <td>나는 먼저 청소기로 바닥을 밀었어요.</td>\n",
       "      <td>First of all, I vacuumed the floor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399996</th>\n",
       "      <td>199997</td>\n",
       "      <td>나는 먼저 팀 과제를 하고 놀러 갔어요.</td>\n",
       "      <td>I did the team assignment first and went out t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399997</th>\n",
       "      <td>199998</td>\n",
       "      <td>나는 비 같은 멋진 연예인을 좋아해요.</td>\n",
       "      <td>I like cool entertainer like Rain.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399998</th>\n",
       "      <td>199999</td>\n",
       "      <td>나는 멋진 자연 경치를 보고 눈물을 흘렸어.</td>\n",
       "      <td>I cried seeing the amazing scenery.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399999</th>\n",
       "      <td>200000</td>\n",
       "      <td>나는 멋진 중학교 생활을 기대합니다.</td>\n",
       "      <td>I look forward to a great middle school experi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           SID                                                 원문  \\\n",
       "0       200001    0 설정을 입력하고 안정될 때까지 5분 동안 기다린 후 OK 버튼을 길게 누르십시오.   \n",
       "1       200002                 0은 그들에게 아무것도 아니었지만 무는 숫자일 수가 없습니다.   \n",
       "2       200003                             1,015버전에서 핫키 버그가 있습니다.   \n",
       "3       200004             1,390점에서 1,440점을 득점한 사람은 재판을 위해 걸러집니다.   \n",
       "4       200005  1,400년보다 오래 전의 유적지에 있는 최초의 성당에서 숭배자들은 그것을 인지했을...   \n",
       "...        ...                                                ...   \n",
       "399995  199996                               나는 먼저 청소기로 바닥을 밀었어요.   \n",
       "399996  199997                             나는 먼저 팀 과제를 하고 놀러 갔어요.   \n",
       "399997  199998                              나는 비 같은 멋진 연예인을 좋아해요.   \n",
       "399998  199999                           나는 멋진 자연 경치를 보고 눈물을 흘렸어.   \n",
       "399999  200000                               나는 멋진 중학교 생활을 기대합니다.   \n",
       "\n",
       "                                                      번역문  \n",
       "0       Enter into 0 setting, and wait for 5 minutes t...  \n",
       "1       The zero was nothing for them but nothing coul...  \n",
       "2             There is a Hotkey bug in the 1,015 version.  \n",
       "3       Individuals who got a score between 1,390 and ...  \n",
       "4       Indeed, worshippers at the very first cathedra...  \n",
       "...                                                   ...  \n",
       "399995                First of all, I vacuumed the floor.  \n",
       "399996  I did the team assignment first and went out t...  \n",
       "399997                 I like cool entertainer like Rain.  \n",
       "399998                I cried seeing the amazing scenery.  \n",
       "399999  I look forward to a great middle school experi...  \n",
       "\n",
       "[400000 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 합병된 데이터 프레임 확인\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 두 코퍼스를 병합해 총 **400,000개**의 **병렬 문장 쌍**이 만들어진 것을 확인하였습니다!\n",
    "- 이제 해당 데이터 프레임을 돌며, 한국어 문장과 영어 문장을 리스트에 각각 저장해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어, 영어 데이터를 별개 리스트에 저장\n",
    "\n",
    "kor_lines = []\n",
    "eng_lines = []\n",
    "\n",
    "for _, row in data.iterrows():\n",
    "    _, kor, eng = row\n",
    "    kor_lines.append(kor)\n",
    "    eng_lines.append(eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**언어별 데이터**가 잘 저장되었는지 일부 데이터를 출력해 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KOR]: 0 설정을 입력하고 안정될 때까지 5분 동안 기다린 후 OK 버튼을 길게 누르십시오.\n",
      "[ENG]: Enter into 0 setting, and wait for 5 minutes to make it stable, then long-press OK button.\n",
      "\n",
      "[KOR]: 0은 그들에게 아무것도 아니었지만 무는 숫자일 수가 없습니다.\n",
      "[ENG]: The zero was nothing for them but nothing couldn't be a number.\n",
      "\n",
      "[KOR]: 1,015버전에서 핫키 버그가 있습니다.\n",
      "[ENG]: There is a Hotkey bug in the 1,015 version.\n",
      "\n",
      "[KOR]: 1,390점에서 1,440점을 득점한 사람은 재판을 위해 걸러집니다.\n",
      "[ENG]: Individuals who got a score between 1,390 and 1,440 are selected for a judge.\n",
      "\n",
      "[KOR]: 1,400년보다 오래 전의 유적지에 있는 최초의 성당에서 숭배자들은 그것을 인지했을 것입니다.\n",
      "[ENG]: Indeed, worshippers at the very first cathedral on this site, over 1,400 years ago, would have still recognized it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for kor, eng in zip(kor_lines[:5], eng_lines[:5]):\n",
    "    print(f'[KOR]: {kor}')\n",
    "    print(f'[ENG]: {eng}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizers** 라이브러리를 학습시키기 위한 **훈련용 텍스트 파일**을 만들어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 토크나이저 훈련 데이터 제작\n",
    "\n",
    "with open('train_korean.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in kor_lines:\n",
    "        print(line, file=f)\n",
    "\n",
    "\n",
    "# 영어 토크나이저 훈련 데이터 제작\n",
    "\n",
    "with open('train_english.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in eng_lines:\n",
    "        print(line, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. BPE 토크나이저 학습\n",
    "- 앞서 가공한 데이터들을 활용해 **BPE 토크나이저**를 학습시킵니다.\n",
    "- 토크나이저를 학습시키기 앞서 프로젝트 전반에 사용될 변수 사전을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'batch_size': 64,\n",
    "    'num_epoch': 15,\n",
    "    'dropout': 0.1,\n",
    "    'min_frequency': 3,\n",
    "    \n",
    "    'vocab_size': 20000,\n",
    "    'num_layers': 6,\n",
    "    'num_heads': 8,\n",
    "    'hidden_dim': 512,\n",
    "    'ffn_dim': 2048,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 한국어와 영어 토크나이저를 **별도로 초기화해 훈련**시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 토크나이저 초기화\n",
    "kor_tokenizer = Tokenizer(BPE())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwargs option suffix\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 한국어 토크나이저 훈련\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=params['vocab_size'],\n",
    "    min_frequency=params['min_frequency'],\n",
    "    special_tokens=['[PAD]', '[SOS]', '[EOS]', '[UNK]'],\n",
    "    suffix=''\n",
    ")\n",
    "kor_tokenizer.train(files=['train_korean.txt'], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 영어 토크나이저 초기화\n",
    "\n",
    "eng_tokenizer = Tokenizer(BPE())\n",
    "\n",
    "\n",
    "# 영어 토크나이저 훈련\n",
    "\n",
    "eng_tokenizer.train(files=['train_english.txt'], trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**패딩 옵션**과 **후처리 작업** 등에 사용될 **스페셜 토큰**들의 아이디를 저장해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = kor_tokenizer.token_to_id('[PAD]')\n",
    "sos_idx = kor_tokenizer.token_to_id('[SOS]')\n",
    "eos_idx = kor_tokenizer.token_to_id('[EOS]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 훈련된 토크나이저로 토큰화 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizers의 `encode_batch` 함수를 활용해 각 데이터들에 대해 **토큰화 작업을 수행**해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_encoded_data = kor_tokenizer.encode_batch(kor_lines)\n",
    "eng_encoded_data = eng_tokenizer.encode_batch(eng_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰화 작업이 잘 수행되었는지 기존 데이터와 비교해 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Orig]: 0 설정을 입력하고 안정될 때까지 5분 동안 기다린 후 OK 버튼을 길게 누르십시오.\n",
      "[Proc]: ['0', ' ', '설정을 ', '입력', '하고 ', '안정', '될 때까지 ', '5', '분 동안 ', '기다', '린 ', '후 ', 'O', 'K', ' 버튼을 ', '길게 ', '누르', '십', '시', '오', '.']\n",
      "\n",
      "[Orig]: 0은 그들에게 아무것도 아니었지만 무는 숫자일 수가 없습니다.\n",
      "[Proc]: ['0', '은 ', '그들에게 ', '아무것도 ', '아니', '었지만 ', '무', '는 ', '숫자', '일', ' 수가 없', '습니다', '.']\n",
      "\n",
      "[Orig]: 1,015버전에서 핫키 버그가 있습니다.\n",
      "[Proc]: ['1,', '0', '15', '버전', '에서 ', '핫', '키', ' 버', '그', '가 있', '습니다', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 한국어 데이터 토큰화 작업 결과 출력\n",
    "\n",
    "for origin, processed in zip(kor_lines[:3], kor_encoded_data[:3]):\n",
    "    print(f'[Orig]: {origin}')\n",
    "    print(f'[Proc]: {processed.tokens}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Orig]: Enter into 0 setting, and wait for 5 minutes to make it stable, then long-press OK button.\n",
      "[Proc]: ['En', 'ter ', 'into ', '0 ', 'sett', 'ing, and ', 'wait for ', '5 ', 'minutes to ', 'make it ', 'st', 'able', ', then ', 'long-', 'press ', 'O', 'K ', 'button', '.']\n",
      "\n",
      "[Orig]: The zero was nothing for them but nothing couldn't be a number.\n",
      "[Proc]: ['The ', 'zer', 'o ', 'was ', 'nothing ', 'for', ' them ', 'but ', 'nothing ', \"couldn't \", 'be a ', 'number', '.']\n",
      "\n",
      "[Orig]: There is a Hotkey bug in the 1,015 version.\n",
      "[Proc]: ['There is a ', 'Hot', 'key ', 'bug ', 'in the ', '1,', '0', '15 ', 'vers', 'ion', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 영어 데이터 토큰화 작업 결과 출력\n",
    "\n",
    "for origin, processed in zip(eng_lines[:3], eng_encoded_data[:3]):\n",
    "    print(f'[Orig]: {origin}')\n",
    "    print(f'[Proc]: {processed.tokens}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 토큰화 결과에 후처리 로직 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제 토큰화 작업이 수행된 결과에 **[PAD]** 토큰을 붙여줄 차례입니다.\n",
    "- **[PAD]** 토큰은 모델이 입력으로 받는 **최대 길이** 보다 길이가 짧은 문장들에 한해 부여되는 토큰이므로, \n",
    "- **최대 길이**로 설정할 적정 길이를 찾기 위해 각 언어 쌍의 평균 길이와 최대 길이를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11.6913875, 53)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한국어 데이터 평균 및 최대 길이 계산\n",
    "\n",
    "kor_len_max = max(len(line.tokens) for line in kor_encoded_data)\n",
    "kor_len = 0\n",
    "\n",
    "for line in kor_encoded_data:\n",
    "    kor_len += len(line.tokens)\n",
    "kor_len_avg = kor_len / len(kor_encoded_data)\n",
    "\n",
    "kor_len_avg, kor_len_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.749995, 65)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 영어 데이터 평균 및 최대 길이 계산\n",
    "\n",
    "eng_len_max = max(len(line.tokens) for line in eng_encoded_data)\n",
    "eng_len = 0\n",
    "\n",
    "for line in eng_encoded_data:\n",
    "    eng_len += len(line.tokens)\n",
    "eng_len_avg = eng_len / len(eng_encoded_data)\n",
    "\n",
    "eng_len_avg, eng_len_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋 내 문장들이 그렇게 긴 편이 아니므로 **32**로 입력 값의 **최대 길이**로 정해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['max_len'] = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 **[PAD]** 토큰을 붙여주는 `pad_sentence` 함수와 \n",
    "\n",
    "문장의 시작과 끝을 알리는 **[SOS]**, **[EOS]** 토큰을 붙여주는 후처리 함수 `postprocess`를 정의해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence(input_ids):\n",
    "    '''최대 길이보다 짧은 문장들에 [PAD] 토큰 부여'''\n",
    "\n",
    "    num_pad = params['max_len'] - len(input_ids)\n",
    "    input_ids.extend([pad_idx] * num_pad)\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(input_ids):\n",
    "    '''입력 문장에 [SOS] 토큰과 [EOS] 토큰 부여'''\n",
    "    \n",
    "    input_ids = pad_sentence(input_ids)\n",
    "    \n",
    "    input_ids = [sos_idx] + input_ids\n",
    "    \n",
    "    input_ids = input_ids[:params['max_len']]\n",
    "\n",
    "    if pad_idx in input_ids:\n",
    "        pad_start = input_ids.index(pad_idx)\n",
    "        input_ids[pad_start] = eos_idx\n",
    "    else:\n",
    "        input_ids[-1] = eos_idx\n",
    "    \n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 정의한 두 함수를 이용하면 결과 값이 다음과 같이 바뀝니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 결과: ['우리 ', '진짜 ', '별', '나', '대 ', '그냥 ', '내가 너무 ', '좋아', '해 ', '넌 ', '그걸 ', '너무 ', '잘 ', '알고 ', '날 ', '쥐', '락', '펴', '락', '해 ', '나도 ', '마찬가지', '인', '걸']\n"
     ]
    }
   ],
   "source": [
    "# 기본 토큰화 작업 결과 \n",
    "\n",
    "sent = '우리 진짜 별나대 그냥 내가 너무 좋아해 넌 그걸 너무 잘 알고 날 쥐락펴락해 나도 마찬가지인걸'\n",
    "\n",
    "proc_sent = kor_tokenizer.encode(sent)\n",
    "print(f'토큰화 결과: {proc_sent.tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "후처리 결과: [1, 2152, 4099, 944, 407, 2473, 3010, 13818, 2125, 2054, 3573, 4313, 2180, 2137, 6523, 2468, 1482, 698, 1859, 698, 2054, 2464, 5344, 1399, 218, 2, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "후처리 해석: 우리  진짜  별 나 대  그냥  내가 너무  좋아 해  넌  그걸  너무  잘  알고  날  쥐 락 펴 락 해  나도  마찬가지 인 걸\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 + 후처리 작업 결과\n",
    "\n",
    "post_proc_sent = postprocess(proc_sent.ids)\n",
    "\n",
    "print(f'후처리 결과: {post_proc_sent}\\n')\n",
    "print(f'후처리 해석: {kor_tokenizer.decode(post_proc_sent)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 후처리 함수를 활용해 모든 데이터셋들에 대해 **후처리 작업을 진행**해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_processed_data = [postprocess(data.ids) for data in kor_encoded_data]\n",
    "eng_processed_data = [postprocess(data.ids) for data in eng_encoded_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모든 데이터셋을 텐서형 데이터로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 전처리와 후처리를 모두 마친 데이터들을 `torch.Tensor`로 변환해줍니다.\n",
    "- 변환 후, `DataLoader`를 활용해 데이터들을 **배치**로 만들어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = torch.tensor(self.dataframe.iloc[idx]['features'])\n",
    "        label = torch.tensor(self.dataframe.iloc[idx]['labels'])\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "kor_tensors = [torch.LongTensor(line).to(device) for line in kor_processed_data]\n",
    "eng_tensors = [torch.LongTensor(line).to(device) for line in eng_processed_data]\n",
    "\n",
    "src_iter = DataLoader(kor_tensors, batch_size=params['batch_size'])\n",
    "tgt_iter = DataLoader(eng_tensors, batch_size=params['batch_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 데이터가 잘 생성되었는지 출력을 통해 확인합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transformer 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 먼저 모델 구현에 필요한 라이브러리들을 모두 임포트합니다.\n",
    "- 실험을 함에 있어 항상 실험의 **Reproducibility**를 보장하기 위해 Seed 설정을 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(32)\n",
    "torch.cuda.manual_seed(32)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-1. (Masked) Multi-Head Attention 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Sub Layers\n",
    "### Multi-Head Attention\n",
    "\n",
    "![](img/mha.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''멀티 헤드 어텐션 레이어'''\n",
    "    def __init__(self, params):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert params['hidden_dim'] % params['num_heads'] == 0, \"hidden dimension must be divisible by the number of heads\"\n",
    "        self.num_heads = params['num_heads']\n",
    "        self.attn_dim = params['hidden_dim'] // self.num_heads\n",
    "        \n",
    "        self.q_w = nn.Linear(params['hidden_dim'], self.num_heads * self.attn_dim)\n",
    "        self.k_w = nn.Linear(params['hidden_dim'], self.num_heads * self.attn_dim)\n",
    "        self.v_w = nn.Linear(params['hidden_dim'], self.num_heads * self.attn_dim)\n",
    "        \n",
    "        self.o_w = nn.Linear(self.num_heads * self.attn_dim, params['hidden_dim'])\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \" q, k, v = [배치 사이즈, 문장 길이, 은닉 차원] \"\n",
    "        \n",
    "        batch_size = q.size(0)\n",
    "        \n",
    "        q = self.q_w(q).view(batch_size, -1, self.num_heads, self.attn_dim).transpose(1, 2)\n",
    "        k = self.k_w(k).view(batch_size, -1, self.num_heads, self.attn_dim).transpose(1, 2)\n",
    "        v = self.v_w(v).view(batch_size, -1, self.num_heads, self.attn_dim).transpose(1, 2)\n",
    "        # q, k, v = [배치 사이즈, 헤드 갯수, 문장 길이, 어텐션 차원]\n",
    "        \n",
    "        attn = torch.matmul(q, k.transpose(-1, -2))\n",
    "        # attn = [배치 사이즈, 헤드 갯수, 문장 길이, 문장 길이]\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            attn.masked_fill(mask==0, -1e9)\n",
    "        \n",
    "        score = F.softmax(attn, dim=-1)\n",
    "        # score = [배치 사이즈, 헤드 갯수, 문장 길이, 문장 길이]\n",
    "        \n",
    "        output = torch.matmul(score, v)\n",
    "        # output = [배치 사이즈, 헤드 갯수, 문장 길이, 어텐션 차원]\n",
    "        \n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        # output = [배치 사이즈, 문장 길이, 헤드 갯수, 어텐션 차원]\n",
    "        \n",
    "        output = output.view(batch_size, -1, self.num_heads * self.attn_dim)\n",
    "        # output = [배치 사이즈, 문장 길이, 은닉 차원]\n",
    "        \n",
    "        output = self.o_w(output)\n",
    "        # output = [배치 사이즈, 문장 길이, 은닉 차원]\n",
    "        \n",
    "        return output, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masked Multi Head Attention을 위한 Subsequent Mask를 생성해주어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subsequent_mask(tgt):\n",
    "    batch_size, tgt_len = tgt.size()\n",
    "    \n",
    "    subsequent_mask = torch.triu(torch.ones(tgt_len, tgt_len), diagonal=1).bool()\n",
    "    # subsequent_mask = [타겟 문장 길이, 타겟 문장 길이]\n",
    "    \n",
    "    subsequent_mask = subsequent_mask.unsqueeze(0).repeat(batch_size, 1, 1).to(device)\n",
    "    # subsquent_mask = [배치 사이즈, 타겟 문장 길이, 타겟 문장 길이]\n",
    "    \n",
    "    return subsequent_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용 예는 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x168456260>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_sent = '왜들 그리 다운돼있어? 뭐가 문제야 say something 분위기가 겁나 싸해'\n",
    "test_tensor = kor_tokenizer.encode(test_sent)\n",
    "test_tensor = torch.LongTensor(test_tensor.ids).to(device).unsqueeze(0)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(create_subsequent_mask(test_tensor).cpu()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_src_mask(src):\n",
    "    \" source = [배치 사이즈, 소스 문장 길이] \"\n",
    "\n",
    "    src_len = src.size(1)\n",
    "    \n",
    "    src_mask = (src == pad_idx)\n",
    "    # src_mask = [배치 사이즈, 소스 문장 길이]\n",
    "    \n",
    "    src_mask = src_mask.unsqueeze(1).repeat(1, src_len, 1)\n",
    "    # src_mask = [배치 사이즈, 소스 문장 길이, 소스 문장 길이]\n",
    "\n",
    "    return src_mask.to(device)\n",
    "\n",
    "\n",
    "def create_tgt_mask(src, tgt):\n",
    "    \" src = [배치 사이즈, 소스 문장 길이] \"\n",
    "    \" tgt = [배치 사이즈, 타겟 문장 길이] \"\n",
    "    \n",
    "    batch_size, tgt_len = tgt.size()\n",
    "    \n",
    "    subsequent_mask = create_subsequent_mask(tgt)\n",
    "    \n",
    "    enc_dec_mask = (src == pad_idx)\n",
    "    tgt_mask = (tgt == pad_idx)\n",
    "    # src_mask = [배치 사이즈, 소스 문장 길이]\n",
    "    # tgt_mask = [배치 사이즈, 타겟 문장 길이]\n",
    "    \n",
    "    enc_dec_mask = enc_dec_mask.unsqueeze(1).repeat(1, tgt_len, 1).to(device)\n",
    "    tgt_mask = tgt_mask.unsqueeze(1).repeat(1, tgt_len, 1).to(device)\n",
    "    # src_mask = [배치 사이즈, 타겟 문장 길이, 소스 문장 길이]\n",
    "    # tgt_mask = [배치 사이즈, 타겟 문장 길이, 타겟 문장 길이]\n",
    "\n",
    "    tgt_mask = tgt_mask | subsequent_mask\n",
    "    \n",
    "    return enc_dec_mask, tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "소스 문장의 마스크 생성 예는 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16853bb80>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "src = torch.tensor([[1, 2, 3, 4, 2, 11, 28, 7, 0, 0, 0]])\n",
    "src_mask = create_src_mask(src)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(src_mask.cpu()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "타겟 문장의 마스크 생성 예는 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt = torch.tensor([[1, 2, 3, 4, 2, 11, 28, 7, 99, 987, 1024, 0, 0]])\n",
    "enc_dec_mask, tgt_mask = create_tgt_mask(src, tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 그림은 타겟 문장이 소스 문장에 Attention을 취할 때 [PAD] 토큰이 마스킹 되는 예입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16853aec0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(enc_dec_mask.cpu()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 예는 타겟 문장에서 Self-Attention 연산이 취해질 때 타임 스텝 상 뒤에 위치하는 토큰들과 [PAD] 토큰들이 마스킹 되는 예입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1685df4c0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(tgt_mask.cpu()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-2. Position-wise Feed-Forward 네트워크 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/positionwise.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 5-2. Position-wise Feed-Forward 네트워크 구현\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    '''포지션 와이즈 피드 포워드 레이어'''\n",
    "    def __init__(self, parmas):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(params['hidden_dim'], params['ffn_dim'])\n",
    "        self.fc2 = nn.Linear(params['ffn_dim'], params['hidden_dim'])\n",
    "        self.dropout = nn.Dropout(params['dropout'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \" x = [배치 사이즈, 문장 길이, 은닉 차원] \"\n",
    "    \n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-3. Positional Encoding 레이어 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/pos.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        sinusoid = np.array([pos / np.power(10000, 2 * i / params['hidden_dim'])\n",
    "                            for pos in range(params['max_len']) for i in range(params['hidden_dim'])])\n",
    "        # sinusoid = [문장 최대 길이 * 은닉 차원]\n",
    "\n",
    "        sinusoid = sinusoid.reshape(params['max_len'], -1)\n",
    "        # sinusoid = [문장 최대 길이, 은닉 차원]\n",
    "\n",
    "        sinusoid[:, 0::2] = np.sin(sinusoid[:, 0::2])\n",
    "        sinusoid[:, 1::2] = np.cos(sinusoid[:, 1::2])\n",
    "        sinusoid = torch.FloatTensor(sinusoid).to(device)\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(sinusoid, freeze=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \" x = [배치 사이즈, 문장 길이] \"\n",
    "        \n",
    "        pos = torch.arange(x.size(-1), dtype=torch.long).to(device)\n",
    "        # pos = [배치 사이즈, 문장 길이]\n",
    "\n",
    "        embed = self.embedding(pos)\n",
    "        # embed = [배치 사이즈, 문장 길이, 은닉 차원]\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-4. Transformer 인코더 부 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/encoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    '''인코더 레이어'''\n",
    "    def __init__(self, params):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(params)\n",
    "        self.layer_norm1 = nn.LayerNorm(params['hidden_dim'])\n",
    "        self.feed_forward = PositionwiseFeedForward(params)\n",
    "        self.layer_norm2 = nn.LayerNorm(params['hidden_dim'])\n",
    "        self.dropout = nn.Dropout(params['dropout'])\n",
    "        \n",
    "    def forward(self, x, src_mask):\n",
    "        \" x = [배치 사이즈, 문장 길이, 은닉 차원] \"\n",
    "        \n",
    "        residual = x\n",
    "        x, _ = self.self_attn(x, x, x, src_mask)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "        x = self.layer_norm1(x)\n",
    "        \n",
    "        residual = x\n",
    "        x = self.feed_forward(x)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "        x = self.layer_norm2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    '''트랜스포머 인코더'''\n",
    "    def __init__(self, params):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.tok_embedding = nn.Embedding(params['vocab_size'], params['hidden_dim'], padding_idx=pad_idx)\n",
    "        self.pos_embedding = PositionalEncoding(params)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(params) for _ in range(params['num_layers'])])\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \" src = [배치 사이즈, 소스 문장 길이] \"\n",
    "\n",
    "        src_mask = create_src_mask(src)\n",
    "        src = self.tok_embedding(src) + self.pos_embedding(src)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "            \n",
    "        # src = [배치 사이즈, 소스 문장 길이, 은닉 차원]\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-5. Transformer 인코더 부 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/decoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    '''디코더 레이어'''\n",
    "    def __init__(self, params):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(params)\n",
    "        self.layer_norm1 = nn.LayerNorm(params['hidden_dim'])\n",
    "\n",
    "        self.enc_dec_attn = MultiHeadAttention(params)\n",
    "        self.layer_norm2 = nn.LayerNorm(params['hidden_dim'])\n",
    "        \n",
    "        self.feed_forward = PositionwiseFeedForward(params)\n",
    "        self.layer_norm3 = nn.LayerNorm(params['hidden_dim'])\n",
    "        \n",
    "        self.dropout = nn.Dropout(params['dropout'])\n",
    "        \n",
    "    def forward(self, x, tgt_mask, enc_output, src_mask):\n",
    "        \" x = [배치 사이즈, 문장 길이, 은닉 차원] \"\n",
    "        \n",
    "        residual = x\n",
    "        x, _ = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "        x = self.layer_norm1(x)\n",
    "        \n",
    "        residual = x\n",
    "        x, attn_map = self.enc_dec_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "        x = self.layer_norm2(x)\n",
    "        \n",
    "        residual = x\n",
    "        x = self.feed_forward(x)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "        x = self.layer_norm3(x)\n",
    "        \n",
    "        return x, attn_map\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    '''트랜스포머 디코더'''\n",
    "    def __init__(self, params):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.tok_embedding = nn.Embedding(params['vocab_size'], params['hidden_dim'], padding_idx=pad_idx)\n",
    "        self.pos_embedding = PositionalEncoding(params)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(params) for _ in range(params['num_layers'])])\n",
    "        \n",
    "    def forward(self, tgt, src, enc_out):\n",
    "        \" tgt = [배치 사이즈, 타겟 문장 길이] \"\n",
    "\n",
    "        src_mask, tgt_mask = create_tgt_mask(src, tgt)\n",
    "        tgt = self.tok_embedding(tgt) + self.pos_embedding(tgt)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            tgt, attn_map = layer(tgt, tgt_mask, enc_out, src_mask)\n",
    "            \n",
    "        tgt = torch.matmul(tgt, self.tok_embedding.weight.transpose(0, 1))\n",
    "        # tgt = [배치 사이즈, 타겟 문장 길이, 은닉 차원]\n",
    "\n",
    "        return tgt, attn_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-6. Transformer 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 앞서 정의한 레이어들을 토대로 **Transformer** 모델을 빌드해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    '''트랜스포머 네트워크'''\n",
    "    def __init__(self, params):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(params)\n",
    "        self.decoder = Decoder(params)\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        \" src = [배치 사이즈, 소스 문장 길이] \"\n",
    "        \" tgt = [배치 사이즈, 타겟 문장 길이] \"\n",
    "        \n",
    "        enc_out = self.encoder(src)\n",
    "        dec_out, attn = self.decoder(tgt, src, enc_out)\n",
    "        return dec_out, attn\n",
    "    \n",
    "    def count_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer 에서는 Adam Optimizer에 일부 스케줄 옵션을 적용해 사용하고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-7. Transformer Optimizer 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/optim.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScheduledOptim:\n",
    "    '''스케줄 옵티마이저'''\n",
    "    def __init__(self, optimizer, warmup_steps):\n",
    "        self.init_lr = np.power(params['hidden_dim'], -0.5)\n",
    "        self.optimizer = optimizer\n",
    "        self.step_num = 0\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def step(self):\n",
    "        self.step_num += 1\n",
    "        lr = self.init_lr * self.get_scale()\n",
    "        \n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = lr\n",
    "            \n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "    \n",
    "    def get_scale(self):\n",
    "        return np.min([\n",
    "            np.power(self.step_num, -0.5),\n",
    "            self.step_num * np.power(self.warmup_steps, -1.5)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 64,618,496 trainable parameters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b0b9f2c8a43441b9e9b0c491ced8c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch: 01 | Train Loss: 10.570\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd7799dc2c448439126cd9e91219a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Train Loss: 7.794\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16fb260d0e9048e3a031a3e0629df6c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m tgt = [배치 사이즈, 타겟 문장 길이] \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 30\u001b[0m logits, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# logits = [배치 사이즈, 타겟 문장 길이, 은닉 차원]\u001b[39;00m\n\u001b[1;32m     33\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[55], line 13\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, tgt)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m tgt = [배치 사이즈, 타겟 문장 길이] \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m enc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(src)\n\u001b[0;32m---> 13\u001b[0m dec_out, attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dec_out, attn\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[54], line 51\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, tgt, src, enc_out)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tgt, src, enc_out):\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m tgt = [배치 사이즈, 타겟 문장 길이] \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 51\u001b[0m     src_mask, tgt_mask \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_tgt_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtok_embedding(tgt) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding(tgt)\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n",
      "Cell \u001b[0;32mIn[42], line 21\u001b[0m, in \u001b[0;36mcreate_tgt_mask\u001b[0;34m(src, tgt)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m tgt = [배치 사이즈, 타겟 문장 길이] \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m batch_size, tgt_len \u001b[38;5;241m=\u001b[39m tgt\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m---> 21\u001b[0m subsequent_mask \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_subsequent_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m enc_dec_mask \u001b[38;5;241m=\u001b[39m (src \u001b[38;5;241m==\u001b[39m pad_idx)\n\u001b[1;32m     24\u001b[0m tgt_mask \u001b[38;5;241m=\u001b[39m (tgt \u001b[38;5;241m==\u001b[39m pad_idx)\n",
      "Cell \u001b[0;32mIn[40], line 7\u001b[0m, in \u001b[0;36mcreate_subsequent_mask\u001b[0;34m(tgt)\u001b[0m\n\u001b[1;32m      4\u001b[0m subsequent_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtriu(torch\u001b[38;5;241m.\u001b[39mones(tgt_len, tgt_len), diagonal\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mbool()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# subsequent_mask = [타겟 문장 길이, 타겟 문장 길이]\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m subsequent_mask \u001b[38;5;241m=\u001b[39m \u001b[43msubsequent_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# subsquent_mask = [배치 사이즈, 타겟 문장 길이, 타겟 문장 길이]\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m subsequent_mask\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 모델 정의\n",
    "model = Transformer(params)\n",
    "model.to(device)\n",
    "print(f'The model has {model.count_params():,} trainable parameters')\n",
    "\n",
    "# 로스 함수 정의\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "criterion.to(device)\n",
    "\n",
    "# 옵티마이저 정의\n",
    "optimizer = ScheduledOptim(\n",
    "    optim.Adam(model.parameters(), betas=[0.9, 0.98], eps=1e-9),\n",
    "    warmup_steps=4000\n",
    ")\n",
    "\n",
    "# 훈련 로직\n",
    "for epoch in range(params['num_epoch']):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # tqdm을 사용하여 진행바 추가\n",
    "    for src, tgt in tqdm(zip(src_iter, tgt_iter), total=len(src_iter)):\n",
    "        \" src = [배치 사이즈, 소스 문장 길이] \"\n",
    "        \" tgt = [배치 사이즈, 타겟 문장 길이] \"\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits, _ = model(src, tgt[:, :-1])\n",
    "        # logits = [배치 사이즈, 타겟 문장 길이, 은닉 차원]\n",
    "        \n",
    "        logits = logits.contiguous().view(-1, logits.size(-1))\n",
    "        # logits = [(배치 사이즈 * 타겟 문장 길이) - 1, 은닉 차원]\n",
    "        golds = tgt[:, 1:].contiguous().view(-1)\n",
    "        # golds = [(배치 사이즈 * 타겟 문장 길이) - 1]\n",
    "\n",
    "        loss = criterion(logits, golds)\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), self.params.clip)\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = epoch_loss / len(src_iter)  # self.train_iter를 src_iter로 변경\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 학습 결과 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 학습된 모델이 어느 정도 성능을 보이는지 확인해 볼 차례입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참고자료\n",
    "- [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "- [jadore801120/attention-is-all-you-need-pytorch](https://github.com/jadore801120/attention-is-all-you-need-pytorch)\n",
    "- [tunz/transformer-pytorch](https://github.com/tunz/transformer-pytorch)\n",
    "- [IgorSusmelj/pytorch-styleguide](https://github.com/IgorSusmelj/pytorch-styleguide)\n",
    "\n",
    "\n",
    "### TODO: 더 추가할 수 있는 것들 !\n",
    "- **Beam Search** 디코딩 추가\n",
    "- **Label Smoothing** 기법 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10(pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
