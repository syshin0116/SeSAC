{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uW-8GLLjmVzW"
   },
   "source": [
    "# The Annotated Transformer 보다 친절한 트랜스포머 튜토리얼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fmDvqKYmVzb"
   },
   "source": [
    "## 0. 데이터 가공\n",
    "- 먼저, 튜토리얼에 사용하기 위해 [**AI Hub**](http://www.aihub.or.kr/)에서 [**한국어-영어 번역 말뭉치**](http://www.aihub.or.kr/aidata/87) 데이터 다운로드를 요청합니다.\n",
    "- 다운로드 요청 후, 약 **2일** 내에 승인 결과가 메일로 전달된다고 합니다.\n",
    "- 말뭉치는 기본적으로 **엑셀 파일**로 제공되지만, 실험의 편의를 위해 **CSV** 파일로 변환해 사용하도록 합니다.\n",
    "\n",
    "_cf. 현재 AI Hub에서는 다양한 한영 번역 데이터셋을 구축해 총 **160만 쌍**의 데이터를 제공해주고 있지만, 모든 문장을 훈련시키기에는 데이터가 과도하므로 본 튜토리얼에서는 **구어체 데이터 1 & 2**만을 사용하도록 합니다._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GvjSIVFImVzc"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KJvRIuY9mVzc",
    "outputId": "8a0214fa-93c2-4e57-c301-dfd57fe1d839"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khkim/anaconda3/envs/torch/lib/python3.8/site-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    }
   ],
   "source": [
    "# 두 개 코퍼스 파일 CSV 포맷으로 변환 저장\n",
    "\n",
    "xls_a = pd.read_excel('data/1_구어체(1).xlsx', index_col=None)\n",
    "xls_b = pd.read_excel('data/1_구어체(2).xlsx', index_col=None)\n",
    "\n",
    "xls_a.to_csv('data/spoken1.csv', encoding='utf-8', index=False)\n",
    "xls_b.to_csv('data/spoken2.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wPtdo2GmVzd"
   },
   "source": [
    "- 이제, **CSV**로 변환한 말뭉치 파일을 `pandas` 라이브러리를 이용해 읽어옵니다.\n",
    "- 읽어온 파일의 각 행을 돌며, **한국어 문장**과 **영어 문장**을 각각의 리스트에 저장해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "RSQv_jasmVzd"
   },
   "outputs": [],
   "source": [
    "# CSV로 변환한 말뭉치 파일 로드 및 합병\n",
    "\n",
    "data_a = pd.read_csv('data/spoken1.csv', encoding='utf-8')\n",
    "data_b = pd.read_csv('data/spoken2.csv', encoding='utf-8')\n",
    "\n",
    "data = pd.concat([data_b, data_a], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "ND_BezRCmVzd",
    "outputId": "b7c405ba-249e-41ad-8408-49146028741c",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SID</th>\n",
       "      <th>원문</th>\n",
       "      <th>번역문</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200001</td>\n",
       "      <td>0 설정을 입력하고 안정될 때까지 5분 동안 기다린 후 OK 버튼을 길게 누르십시오.</td>\n",
       "      <td>Enter into 0 setting, and wait for 5 minutes t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200002</td>\n",
       "      <td>0은 그들에게 아무것도 아니었지만 무는 숫자일 수가 없습니다.</td>\n",
       "      <td>The zero was nothing for them but nothing coul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200003</td>\n",
       "      <td>1,015버전에서 핫키 버그가 있습니다.</td>\n",
       "      <td>There is a Hotkey bug in the 1,015 version.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200004</td>\n",
       "      <td>1,390점에서 1,440점을 득점한 사람은 재판을 위해 걸러집니다.</td>\n",
       "      <td>Individuals who got a score between 1,390 and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200005</td>\n",
       "      <td>1,400년보다 오래 전의 유적지에 있는 최초의 성당에서 숭배자들은 그것을 인지했을...</td>\n",
       "      <td>Indeed, worshippers at the very first cathedra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399995</th>\n",
       "      <td>199996</td>\n",
       "      <td>나는 먼저 청소기로 바닥을 밀었어요.</td>\n",
       "      <td>First of all, I vacuumed the floor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399996</th>\n",
       "      <td>199997</td>\n",
       "      <td>나는 먼저 팀 과제를 하고 놀러 갔어요.</td>\n",
       "      <td>I did the team assignment first and went out t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399997</th>\n",
       "      <td>199998</td>\n",
       "      <td>나는 비 같은 멋진 연예인을 좋아해요.</td>\n",
       "      <td>I like cool entertainer like Rain.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399998</th>\n",
       "      <td>199999</td>\n",
       "      <td>나는 멋진 자연 경치를 보고 눈물을 흘렸어.</td>\n",
       "      <td>I cried seeing the amazing scenery.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399999</th>\n",
       "      <td>200000</td>\n",
       "      <td>나는 멋진 중학교 생활을 기대합니다.</td>\n",
       "      <td>I look forward to a great middle school experi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           SID                                                 원문  \\\n",
       "0       200001    0 설정을 입력하고 안정될 때까지 5분 동안 기다린 후 OK 버튼을 길게 누르십시오.   \n",
       "1       200002                 0은 그들에게 아무것도 아니었지만 무는 숫자일 수가 없습니다.   \n",
       "2       200003                             1,015버전에서 핫키 버그가 있습니다.   \n",
       "3       200004             1,390점에서 1,440점을 득점한 사람은 재판을 위해 걸러집니다.   \n",
       "4       200005  1,400년보다 오래 전의 유적지에 있는 최초의 성당에서 숭배자들은 그것을 인지했을...   \n",
       "...        ...                                                ...   \n",
       "399995  199996                               나는 먼저 청소기로 바닥을 밀었어요.   \n",
       "399996  199997                             나는 먼저 팀 과제를 하고 놀러 갔어요.   \n",
       "399997  199998                              나는 비 같은 멋진 연예인을 좋아해요.   \n",
       "399998  199999                           나는 멋진 자연 경치를 보고 눈물을 흘렸어.   \n",
       "399999  200000                               나는 멋진 중학교 생활을 기대합니다.   \n",
       "\n",
       "                                                      번역문  \n",
       "0       Enter into 0 setting, and wait for 5 minutes t...  \n",
       "1       The zero was nothing for them but nothing coul...  \n",
       "2             There is a Hotkey bug in the 1,015 version.  \n",
       "3       Individuals who got a score between 1,390 and ...  \n",
       "4       Indeed, worshippers at the very first cathedra...  \n",
       "...                                                   ...  \n",
       "399995                First of all, I vacuumed the floor.  \n",
       "399996  I did the team assignment first and went out t...  \n",
       "399997                 I like cool entertainer like Rain.  \n",
       "399998                I cried seeing the amazing scenery.  \n",
       "399999  I look forward to a great middle school experi...  \n",
       "\n",
       "[400000 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 합병된 데이터 프레임 확인\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCiXGKWxmVze"
   },
   "source": [
    "- 두 코퍼스를 병합해 총 **400,000개**의 **병렬 문장 쌍**이 만들어진 것을 확인하였습니다!\n",
    "- 이제 해당 데이터 프레임을 돌며, 한국어 문장과 영어 문장을 리스트에 각각 저장해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-4Vpjx4umVzg"
   },
   "outputs": [],
   "source": [
    "# 한국어, 영어 데이터를 별개 리스트에 저장\n",
    "\n",
    "kor_lines = []\n",
    "eng_lines = []\n",
    "\n",
    "for _, row in data.iterrows():\n",
    "    _, kor, eng = row\n",
    "    kor_lines.append(kor)\n",
    "    eng_lines.append(eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvHcS_uomVzh"
   },
   "source": [
    "**언어별 데이터**가 잘 저장되었는지 일부 데이터를 출력해 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iGOuqpXUmVzh",
    "outputId": "d5189f94-ec6c-4d21-f900-0930f358bc98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KOR]: 0 설정을 입력하고 안정될 때까지 5분 동안 기다린 후 OK 버튼을 길게 누르십시오.\n",
      "[ENG]: Enter into 0 setting, and wait for 5 minutes to make it stable, then long-press OK button.\n",
      "\n",
      "[KOR]: 0은 그들에게 아무것도 아니었지만 무는 숫자일 수가 없습니다.\n",
      "[ENG]: The zero was nothing for them but nothing couldn't be a number.\n",
      "\n",
      "[KOR]: 1,015버전에서 핫키 버그가 있습니다.\n",
      "[ENG]: There is a Hotkey bug in the 1,015 version.\n",
      "\n",
      "[KOR]: 1,390점에서 1,440점을 득점한 사람은 재판을 위해 걸러집니다.\n",
      "[ENG]: Individuals who got a score between 1,390 and 1,440 are selected for a judge.\n",
      "\n",
      "[KOR]: 1,400년보다 오래 전의 유적지에 있는 최초의 성당에서 숭배자들은 그것을 인지했을 것입니다.\n",
      "[ENG]: Indeed, worshippers at the very first cathedral on this site, over 1,400 years ago, would have still recognized it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for kor, eng in zip(kor_lines[:5], eng_lines[:5]):\n",
    "    print(f'[KOR]: {kor}')\n",
    "    print(f'[ENG]: {eng}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYO4gZe3mVzh"
   },
   "source": [
    "**Tokenizers** 라이브러리를 학습시키기 위한 **훈련용 텍스트 파일**을 만들어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "eDtzOzNxmVzh"
   },
   "outputs": [],
   "source": [
    "# 한국어 토크나이저 훈련 데이터 제작\n",
    "\n",
    "with open('train_korean.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in kor_lines:\n",
    "        print(line, file=f)\n",
    "\n",
    "\n",
    "# 영어 토크나이저 훈련 데이터 제작\n",
    "\n",
    "with open('train_english.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in eng_lines:\n",
    "        print(line, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7T5tqx9mVzh"
   },
   "source": [
    "## 1. BPE 토크나이저 학습\n",
    "- 앞서 가공한 데이터들을 활용해 **BPE 토크나이저**를 학습시킵니다.\n",
    "- 토크나이저를 학습시키기 앞서 프로젝트 전반에 사용될 변수 사전을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3vWG1FjlmVzi"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'batch_size': 64,\n",
    "    'num_epoch': 15,\n",
    "    'dropout': 0.1,\n",
    "    'min_frequency': 3,\n",
    "\n",
    "    'vocab_size': 20000,\n",
    "    'num_layers': 6,\n",
    "    'num_heads': 8,\n",
    "    'hidden_dim': 512,\n",
    "    'ffn_dim': 2048,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpKMOrjLmVzi"
   },
   "source": [
    "- 한국어와 영어 토크나이저를 **별도로 초기화해 훈련**시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nLgr2gZdnQoH",
    "outputId": "becde644-39b0-4cb3-98fa-6476aae0ebcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/khkim/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tokenizers in /home/khkim/anaconda3/envs/torch/lib/python3.8/site-packages (0.12.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "N_PkctTimVzi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "# 한국어 토크나이저 초기화\n",
    "kor_tokenizer = Tokenizer(BPE())\n",
    "\n",
    "\n",
    "# 한국어 토크나이저 훈련\n",
    "trainer = BpeTrainer(\n",
    "    special_tokens=['[PAD]', '[SOS]', '[EOS]', '[UNK]'],\n",
    "    vocab_size=params['vocab_size'],\n",
    "    min_frequency=params['min_frequency']\n",
    ")\n",
    "kor_tokenizer.train(\n",
    "    files=[\"train_korean.txt\"],\n",
    "    trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "waYezgOCmVzi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 영어 토크나이저 초기화\n",
    "eng_tokenizer = Tokenizer(BPE())\n",
    "\n",
    "\n",
    "# 영어 토크나이저 훈련\n",
    "trainer = BpeTrainer(\n",
    "    special_tokens=['[PAD]', '[SOS]', '[EOS]', '[UNK]'],\n",
    "    vocab_size=params['vocab_size'],\n",
    "    min_frequency=params['min_frequency']\n",
    ")\n",
    "eng_tokenizer.train(\n",
    "    files=[\"train_english.txt\"],\n",
    "    trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gI85JwVdmVzi"
   },
   "source": [
    "**패딩 옵션**과 **후처리 작업** 등에 사용될 **스페셜 토큰**들의 아이디를 저장해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "5VlMmbhOmVzi"
   },
   "outputs": [],
   "source": [
    "pad_idx = kor_tokenizer.token_to_id('[PAD]')\n",
    "sos_idx = kor_tokenizer.token_to_id('[SOS]')\n",
    "eos_idx = kor_tokenizer.token_to_id('[EOS]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hJ6cJyyBmVzj",
    "outputId": "aec60f97-5045-43f9-d6ff-558bccf1e9be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_idx, sos_idx, eos_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3UMwPKzmVzj"
   },
   "source": [
    "## 2. 훈련된 토크나이저로 토큰화 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W39u8JKpmVzj"
   },
   "source": [
    "Tokenizers의 `encode_batch` 함수를 활용해 각 데이터들에 대해 **토큰화 작업을 수행**해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "DyoBvv8ZmVzj"
   },
   "outputs": [],
   "source": [
    "kor_encoded_data = kor_tokenizer.encode_batch(kor_lines)\n",
    "eng_encoded_data = eng_tokenizer.encode_batch(eng_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3cI0PacmVzj"
   },
   "source": [
    "토큰화 작업이 잘 수행되었는지 기존 데이터와 비교해 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m2MMXDi2mVzj",
    "outputId": "83088f7a-c4a1-470e-c610-58567a66d671"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Orig]: 0 설정을 입력하고 안정될 때까지 5분 동안 기다린 후 OK 버튼을 길게 누르십시오.\n",
      "[Proc]: ['0', ' ', '설정을 ', '입력', '하고 ', '안정', '될 때까지 ', '5', '분 동안 ', '기다', '린 ', '후 ', 'O', 'K', ' 버튼을 ', '길게 ', '누르', '십', '시', '오', '.']\n",
      "\n",
      "[Orig]: 0은 그들에게 아무것도 아니었지만 무는 숫자일 수가 없습니다.\n",
      "[Proc]: ['0', '은 ', '그들에게 ', '아무것도 ', '아니', '었지만 ', '무', '는 ', '숫자', '일', ' 수가 없', '습니다', '.']\n",
      "\n",
      "[Orig]: 1,015버전에서 핫키 버그가 있습니다.\n",
      "[Proc]: ['1,', '0', '15', '버전', '에서 ', '핫', '키', ' 버', '그', '가 있', '습니다', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 한국어 데이터 토큰화 작업 결과 출력\n",
    "\n",
    "for origin, processed in zip(kor_lines[:3], kor_encoded_data[:3]):\n",
    "    print(f'[Orig]: {origin}')\n",
    "    print(f'[Proc]: {processed.tokens}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EOPdBPMlmVzj",
    "outputId": "1c147565-48ec-423f-c68c-bc6f6e0ca66a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Orig]: Enter into 0 setting, and wait for 5 minutes to make it stable, then long-press OK button.\n",
      "[Proc]: ['En', 'ter ', 'into ', '0 ', 'sett', 'ing, and ', 'wait for ', '5 ', 'minutes to ', 'make it ', 'st', 'able', ', then ', 'long-', 'press ', 'O', 'K ', 'button', '.']\n",
      "\n",
      "[Orig]: The zero was nothing for them but nothing couldn't be a number.\n",
      "[Proc]: ['The ', 'zer', 'o ', 'was ', 'nothing ', 'for', ' them ', 'but ', 'nothing ', \"couldn't \", 'be a ', 'number', '.']\n",
      "\n",
      "[Orig]: There is a Hotkey bug in the 1,015 version.\n",
      "[Proc]: ['There is a ', 'Hot', 'key ', 'bug ', 'in the ', '1,', '0', '15 ', 'vers', 'ion', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 영어 데이터 토큰화 작업 결과 출력\n",
    "\n",
    "for origin, processed in zip(eng_lines[:3], eng_encoded_data[:3]):\n",
    "    print(f'[Orig]: {origin}')\n",
    "    print(f'[Proc]: {processed.tokens}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rz3Mr0eLmVzk"
   },
   "source": [
    "## 3. 토큰화 결과에 후처리 로직 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCBK_SFqmVzk"
   },
   "source": [
    "- 이제 토큰화 작업이 수행된 결과에 **[PAD]** 토큰을 붙여줄 차례입니다.\n",
    "- **[PAD]** 토큰은 모델이 입력으로 받는 **최대 길이** 보다 길이가 짧은 문장들에 한해 부여되는 토큰이므로,\n",
    "- **최대 길이**로 설정할 적정 길이를 찾기 위해 각 언어 쌍의 평균 길이와 최대 길이를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VkcikVKVmVzk",
    "outputId": "e9b6e9a4-a2dd-427a-dd54-6a35f416ab8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11.6913875, 53)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한국어 데이터 평균 및 최대 길이 계산\n",
    "\n",
    "kor_len_max = max(len(line.tokens) for line in kor_encoded_data)\n",
    "kor_len = 0\n",
    "\n",
    "for line in kor_encoded_data:\n",
    "    kor_len += len(line.tokens)\n",
    "kor_len_avg = kor_len / len(kor_encoded_data)\n",
    "\n",
    "kor_len_avg, kor_len_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JRn8LeLsmVzk",
    "outputId": "ffcdcf40-8309-4add-dd90-dbe381dd3cee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.749995, 65)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 영어 데이터 평균 및 최대 길이 계산\n",
    "\n",
    "eng_len_max = max(len(line.tokens) for line in eng_encoded_data)\n",
    "eng_len = 0\n",
    "\n",
    "for line in eng_encoded_data:\n",
    "    eng_len += len(line.tokens)\n",
    "eng_len_avg = eng_len / len(eng_encoded_data)\n",
    "\n",
    "eng_len_avg, eng_len_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XSlZ7WLmVzk"
   },
   "source": [
    "데이터셋 내 문장들이 그렇게 긴 편이 아니므로 **32**로 입력 값의 **최대 길이**로 정해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "6i1N6xVbmVzk"
   },
   "outputs": [],
   "source": [
    "params['max_len'] = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBMn0qHbmVzk"
   },
   "source": [
    "마지막으로 **[PAD]** 토큰을 붙여주는 `pad_sentence` 함수와\n",
    "\n",
    "문장의 시작과 끝을 알리는 **[SOS]**, **[EOS]** 토큰을 붙여주는 후처리 함수 `postprocess`를 정의해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "g_A8rfR8mVzk"
   },
   "outputs": [],
   "source": [
    "def pad_sentence(input_ids):\n",
    "    '''최대 길이보다 짧은 문장들에 [PAD] 토큰 부여'''\n",
    "\n",
    "    num_pad = params['max_len'] - len(input_ids)\n",
    "    input_ids.extend([pad_idx] * num_pad)\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "PE3n3AgimVzl"
   },
   "outputs": [],
   "source": [
    "def postprocess(input_ids):\n",
    "    '''입력 문장에 [SOS] 토큰과 [EOS] 토큰 부여'''\n",
    "\n",
    "    input_ids = pad_sentence(input_ids)\n",
    "\n",
    "    input_ids = [sos_idx] + input_ids\n",
    "\n",
    "    input_ids = input_ids[:params['max_len']]\n",
    "\n",
    "    if pad_idx in input_ids:\n",
    "        pad_start = input_ids.index(pad_idx)\n",
    "        input_ids[pad_start] = eos_idx\n",
    "    else:\n",
    "        input_ids[-1] = eos_idx\n",
    "\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALFh32OHmVzl"
   },
   "source": [
    "앞서 정의한 두 함수를 이용하면 결과 값이 다음과 같이 바뀝니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M0F93VeKmVzl",
    "outputId": "2dd35f2e-23fe-410f-bce7-91d578050fda",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 결과: ['우리 ', '진짜 ', '별', '나', '대 ', '그냥 ', '내가 너무 ', '좋아', '해 ', '넌 ', '그걸 ', '너무 ', '잘 ', '알고 ', '날 ', '쥐', '락', '펴', '락', '해 ', '나도 ', '마찬가지', '인', '걸']\n"
     ]
    }
   ],
   "source": [
    "# 기본 토큰화 작업 결과\n",
    "\n",
    "sent = '우리 진짜 별나대 그냥 내가 너무 좋아해 넌 그걸 너무 잘 알고 날 쥐락펴락해 나도 마찬가지인걸'\n",
    "\n",
    "proc_sent = kor_tokenizer.encode(sent)\n",
    "print(f'토큰화 결과: {proc_sent.tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GeF2K_EemVzl",
    "outputId": "461ca63c-9e62-46bf-be47-367b713005d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "후처리 결과: [1, 2152, 4099, 944, 407, 2473, 3010, 13818, 2125, 2054, 3573, 4313, 2180, 2137, 6523, 2468, 1482, 698, 1859, 698, 2054, 2464, 5344, 1399, 218, 2, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "후처리 해석: 우리  진짜  별 나 대  그냥  내가 너무  좋아 해  넌  그걸  너무  잘  알고  날  쥐 락 펴 락 해  나도  마찬가지 인 걸\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 + 후처리 작업 결과\n",
    "\n",
    "post_proc_sent = postprocess(proc_sent.ids)\n",
    "\n",
    "print(f'후처리 결과: {post_proc_sent}\\n')\n",
    "print(f'후처리 해석: {kor_tokenizer.decode(post_proc_sent)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qo1kYmnDmVzv"
   },
   "source": [
    "이제 후처리 함수를 활용해 모든 데이터셋들에 대해 **후처리 작업을 진행**해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "KlA4HT5JmVzv"
   },
   "outputs": [],
   "source": [
    "kor_processed_data = [postprocess(data.ids) for data in kor_encoded_data]\n",
    "eng_processed_data = [postprocess(data.ids) for data in eng_encoded_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O39Nj5w1mVzv"
   },
   "source": [
    "## 4. 모든 데이터셋을 텐서형 데이터로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iUcU2NYmVzv"
   },
   "source": [
    "- 전처리와 후처리를 모두 마친 데이터들을 `torch.Tensor`로 변환해줍니다.\n",
    "- 변환 후, `DataLoader`를 활용해 데이터들을 **배치**로 만들어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "GhBTd-oemVzv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = \"cuda:0\"\n",
    "# device = \"cpu\"\n",
    "\n",
    "kor_tensors = [torch.LongTensor(line).to(device) for line in kor_processed_data]\n",
    "eng_tensors = [torch.LongTensor(line).to(device) for line in eng_processed_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "yVbyUr1GmVzv"
   },
   "outputs": [],
   "source": [
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "EHUZ-LTomVzv"
   },
   "outputs": [],
   "source": [
    "src_iter = torch.utils.data.DataLoader(kor_tensors, batch_size=params['batch_size'])\n",
    "tgt_iter = torch.utils.data.DataLoader(eng_tensors, batch_size=params['batch_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eb8ptifjmVzw"
   },
   "source": [
    "배치 데이터가 잘 생성되었는지 출력을 통해 확인합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSA7ibiemVzw"
   },
   "source": [
    "## 5. Transformer 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qn5TCS-NmVzw"
   },
   "source": [
    "- 먼저 모델 구현에 필요한 라이브러리들을 모두 임포트합니다.\n",
    "- 실험을 함에 있어 항상 실험의 **Reproducibility**를 보장하기 위해 Seed 설정을 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "NYZQJjSBmVzw"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(32)\n",
    "torch.cuda.manual_seed(32)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUy5MhVEmVzw"
   },
   "source": [
    "## 5-1. (Masked) Multi-Head Attention 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsUyEmx7mVzw"
   },
   "source": [
    "## 2.1 Sub Layers\n",
    "### Multi-Head Attention\n",
    "\n",
    "![](img/mha.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "PHiG4Ev4mVzw"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''멀티 헤드 어텐션 레이어'''\n",
    "    def __init__(self, params):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert params['hidden_dim'] % params['num_heads'] == 0, \"hidden dimension must be divisible by the number of heads\"\n",
    "        self.num_heads = params['num_heads']\n",
    "        self.attn_dim = params['hidden_dim'] // self.num_heads\n",
    "\n",
    "        self.q_w = nn.Linear(params['hidden_dim'], self.num_heads * self.attn_dim)\n",
    "        self.k_w = nn.Linear(params['hidden_dim'], self.num_heads * self.attn_dim)\n",
    "        self.v_w = nn.Linear(params['hidden_dim'], self.num_heads * self.attn_dim)\n",
    "\n",
    "        self.o_w = nn.Linear(self.num_heads * self.attn_dim, params['hidden_dim'])\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \" q, k, v = [배치 사이즈, 문장 길이, 은닉 차원] \"\n",
    "\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        q = self.q_w(q).view(batch_size, -1, self.num_heads, self.attn_dim).transpose(1, 2)\n",
    "        k = self.k_w(k).view(batch_size, -1, self.num_heads, self.attn_dim).transpose(1, 2)\n",
    "        v = self.v_w(v).view(batch_size, -1, self.num_heads, self.attn_dim).transpose(1, 2)\n",
    "        # q, k, v = [배치 사이즈, 헤드 갯수, 문장 길이, 어텐션 차원]\n",
    "\n",
    "        attn = torch.matmul(q, k.transpose(-1, -2))\n",
    "        # attn = [배치 사이즈, 헤드 갯수, 문장 길이, 문장 길이]\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            attn.masked_fill(mask==0, -1e9)\n",
    "\n",
    "        score = F.softmax(attn, dim=-1)\n",
    "        # score = [배치 사이즈, 헤드 갯수, 문장 길이, 문장 길이]\n",
    "\n",
    "        output = torch.matmul(score, v)\n",
    "        # output = [배치 사이즈, 헤드 갯수, 문장 길이, 어텐션 차원]\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        # output = [배치 사이즈, 문장 길이, 헤드 갯수, 어텐션 차원]\n",
    "\n",
    "        output = output.view(batch_size, -1, self.num_heads * self.attn_dim)\n",
    "        # output = [배치 사이즈, 문장 길이, 은닉 차원]\n",
    "\n",
    "        output = self.o_w(output)\n",
    "        # output = [배치 사이즈, 문장 길이, 은닉 차원]\n",
    "\n",
    "        return output, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQH1Jk1pmVzw"
   },
   "source": [
    "Masked Multi Head Attention을 위한 Subsequent Mask를 생성해주어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "ExdFPIqSmVzw"
   },
   "outputs": [],
   "source": [
    "def create_subsequent_mask(tgt):\n",
    "    batch_size, tgt_len = tgt.size()\n",
    "\n",
    "    subsequent_mask = torch.triu(torch.ones(tgt_len, tgt_len), diagonal=1).bool()\n",
    "    # subsequent_mask = [타겟 문장 길이, 타겟 문장 길이]\n",
    "\n",
    "    subsequent_mask = subsequent_mask.unsqueeze(0).repeat(batch_size, 1, 1).to(device)\n",
    "    # subsquent_mask = [배치 사이즈, 타겟 문장 길이, 타겟 문장 길이]\n",
    "\n",
    "    return subsequent_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ra_j0Q3UmVzw"
   },
   "source": [
    "사용 예는 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "id": "A1yS2qkCmVzx",
    "outputId": "ca79dfc1-7d12-472e-e517-a100820ac18b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6db7573ee0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_sent = '왜들 그리 다운돼있어? 뭐가 문제야 say something 분위기가 겁나 싸해'\n",
    "test_tensor = kor_tokenizer.encode(test_sent)\n",
    "test_tensor = torch.LongTensor(test_tensor.ids).to(device).unsqueeze(0)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(create_subsequent_mask(test_tensor).cpu()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "8NZ7K-V5mVzx"
   },
   "outputs": [],
   "source": [
    "def create_src_mask(src):\n",
    "    \" source = [배치 사이즈, 소스 문장 길이] \"\n",
    "\n",
    "    src_len = src.size(1)\n",
    "\n",
    "    src_mask = (src == pad_idx)\n",
    "    # src_mask = [배치 사이즈, 소스 문장 길이]\n",
    "\n",
    "    src_mask = src_mask.unsqueeze(1).repeat(1, src_len, 1)\n",
    "    # src_mask = [배치 사이즈, 소스 문장 길이, 소스 문장 길이]\n",
    "\n",
    "    return src_mask.to(device)\n",
    "\n",
    "\n",
    "def create_tgt_mask(src, tgt):\n",
    "    \" src = [배치 사이즈, 소스 문장 길이] \"\n",
    "    \" tgt = [배치 사이즈, 타겟 문장 길이] \"\n",
    "\n",
    "    batch_size, tgt_len = tgt.size()\n",
    "\n",
    "    subsequent_mask = create_subsequent_mask(tgt)\n",
    "\n",
    "    enc_dec_mask = (src == pad_idx)\n",
    "    tgt_mask = (tgt == pad_idx)\n",
    "    # src_mask = [배치 사이즈, 소스 문장 길이]\n",
    "    # tgt_mask = [배치 사이즈, 타겟 문장 길이]\n",
    "\n",
    "    enc_dec_mask = enc_dec_mask.unsqueeze(1).repeat(1, tgt_len, 1).to(device)\n",
    "    tgt_mask = tgt_mask.unsqueeze(1).repeat(1, tgt_len, 1).to(device)\n",
    "    # src_mask = [배치 사이즈, 타겟 문장 길이, 소스 문장 길이]\n",
    "    # tgt_mask = [배치 사이즈, 타겟 문장 길이, 타겟 문장 길이]\n",
    "\n",
    "    tgt_mask = tgt_mask | subsequent_mask\n",
    "\n",
    "    return enc_dec_mask, tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-P4WlowmVzx"
   },
   "source": [
    "소스 문장의 마스크 생성 예는 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "id": "4RMEPt1ImVzx",
    "outputId": "4f3ab7cf-fd26-467b-f657-ed7c27535d89",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6d9c588940>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "src = torch.tensor([[1, 2, 3, 4, 2, 11, 28, 7, 0, 0, 0]])\n",
    "src_mask = create_src_mask(src)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(src_mask.cpu()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYjOP0cZmVzx"
   },
   "source": [
    "타겟 문장의 마스크 생성 예는 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "tc33SAWjmVzx"
   },
   "outputs": [],
   "source": [
    "tgt = torch.tensor([[1, 2, 3, 4, 2, 11, 28, 7, 99, 987, 1024, 0, 0]])\n",
    "enc_dec_mask, tgt_mask = create_tgt_mask(src, tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PML8ptsemVzx"
   },
   "source": [
    "아래 그림은 타겟 문장이 소스 문장에 Attention을 취할 때 [PAD] 토큰이 마스킹 되는 예입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "id": "qjtAzCL9mVzx",
    "outputId": "d9ac2031-853c-43f5-a2cd-6971ed3d859f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6d9c4f8820>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(enc_dec_mask.cpu()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zi5CbeYDmVzy"
   },
   "source": [
    "아래 예는 타겟 문장에서 Self-Attention 연산이 취해질 때 타임 스텝 상 뒤에 위치하는 토큰들과 [PAD] 토큰들이 마스킹 되는 예입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "id": "Xg4XzEBLmVzy",
    "outputId": "e1fba43b-d5ce-40b5-a158-5d8aa334501a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6d9c4736a0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(tgt_mask.cpu()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mr3_99fSmVzy"
   },
   "source": [
    "## 5-2. Position-wise Feed-Forward 네트워크 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpvP7TP_mVzy"
   },
   "source": [
    "![](img/positionwise.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "AlZ51wjZmVzy",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 5-2. Position-wise Feed-Forward 네트워크 구현\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    '''포지션 와이즈 피드 포워드 레이어'''\n",
    "    def __init__(self, parmas):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(params['hidden_dim'], params['ffn_dim'])\n",
    "        self.fc2 = nn.Linear(params['ffn_dim'], params['hidden_dim'])\n",
    "        self.dropout = nn.Dropout(params['dropout'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \" x = [배치 사이즈, 문장 길이, 은닉 차원] \"\n",
    "\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9sBKZZcbmVzy"
   },
   "source": [
    "## 5-3. Positional Encoding 레이어 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wd-xSqFimVzy"
   },
   "source": [
    "![](img/pos.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "7QxrMXlnmVzy"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        sinusoid = np.array([pos / np.power(10000, 2 * i / params['hidden_dim'])\n",
    "                            for pos in range(params['max_len']) for i in range(params['hidden_dim'])])\n",
    "        # sinusoid = [문장 최대 길이 * 은닉 차원]\n",
    "\n",
    "        sinusoid = sinusoid.reshape(params['max_len'], -1)\n",
    "        # sinusoid = [문장 최대 길이, 은닉 차원]\n",
    "\n",
    "        sinusoid[:, 0::2] = np.sin(sinusoid[:, 0::2])\n",
    "        sinusoid[:, 1::2] = np.cos(sinusoid[:, 1::2])\n",
    "        sinusoid = torch.FloatTensor(sinusoid).to(device)\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(sinusoid, freeze=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \" x = [배치 사이즈, 문장 길이] \"\n",
    "\n",
    "        pos = torch.arange(x.size(-1), dtype=torch.long).to(device)\n",
    "        # pos = [배치 사이즈, 문장 길이]\n",
    "\n",
    "        embed = self.embedding(pos)\n",
    "        # embed = [배치 사이즈, 문장 길이, 은닉 차원]\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReOUIvrtmVzy"
   },
   "source": [
    "## 5-4. Transformer 인코더 부 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmSrqOfMmVzy"
   },
   "source": [
    "![](img/encoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "qys2jfiAmVzz"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    '''인코더 레이어'''\n",
    "    def __init__(self, params):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(params)\n",
    "        self.layer_norm1 = nn.LayerNorm(params['hidden_dim'])\n",
    "        self.feed_forward = PositionwiseFeedForward(params)\n",
    "        self.layer_norm2 = nn.LayerNorm(params['hidden_dim'])\n",
    "        self.dropout = nn.Dropout(params['dropout'])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        \" x = [배치 사이즈, 문장 길이, 은닉 차원] \"\n",
    "\n",
    "        residual = x\n",
    "        x, _ = self.self_attn(x, x, x, src_mask)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "        x = self.layer_norm1(x)\n",
    "\n",
    "        residual = x\n",
    "        x = self.feed_forward(x)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "        x = self.layer_norm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    '''트랜스포머 인코더'''\n",
    "    def __init__(self, params):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.tok_embedding = nn.Embedding(params['vocab_size'], params['hidden_dim'], padding_idx=pad_idx)\n",
    "        self.pos_embedding = PositionalEncoding(params)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(params) for _ in range(params['num_layers'])])\n",
    "\n",
    "    def forward(self, src):\n",
    "        \" src = [배치 사이즈, 소스 문장 길이] \"\n",
    "\n",
    "        src_mask = create_src_mask(src)\n",
    "        src = self.tok_embedding(src) + self.pos_embedding(src)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "\n",
    "        # src = [배치 사이즈, 소스 문장 길이, 은닉 차원]\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2q2VJHsmVzz"
   },
   "source": [
    "## 5-5. Transformer 인코더 부 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtJA5r7OmVzz"
   },
   "source": [
    "![](img/decoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "4FkCmh4rmVzz"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    '''디코더 레이어'''\n",
    "    def __init__(self, params):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(params)\n",
    "        self.layer_norm1 = nn.LayerNorm(params['hidden_dim'])\n",
    "\n",
    "        self.enc_dec_attn = MultiHeadAttention(params)\n",
    "        self.layer_norm2 = nn.LayerNorm(params['hidden_dim'])\n",
    "\n",
    "        self.feed_forward = PositionwiseFeedForward(params)\n",
    "        self.layer_norm3 = nn.LayerNorm(params['hidden_dim'])\n",
    "\n",
    "        self.dropout = nn.Dropout(params['dropout'])\n",
    "\n",
    "    def forward(self, x, tgt_mask, enc_output, src_mask):\n",
    "        \" x = [배치 사이즈, 문장 길이, 은닉 차원] \"\n",
    "\n",
    "        residual = x\n",
    "        x, _ = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "        x = self.layer_norm1(x)\n",
    "\n",
    "        residual = x\n",
    "        x, attn_map = self.enc_dec_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "        x = self.layer_norm2(x)\n",
    "\n",
    "        residual = x\n",
    "        x = self.feed_forward(x)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "        x = self.layer_norm3(x)\n",
    "\n",
    "        return x, attn_map\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    '''트랜스포머 디코더'''\n",
    "    def __init__(self, params):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.tok_embedding = nn.Embedding(params['vocab_size'], params['hidden_dim'], padding_idx=pad_idx)\n",
    "        self.pos_embedding = PositionalEncoding(params)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(params) for _ in range(params['num_layers'])])\n",
    "\n",
    "    def forward(self, tgt, src, enc_out):\n",
    "        \" tgt = [배치 사이즈, 타겟 문장 길이] \"\n",
    "\n",
    "        src_mask, tgt_mask = create_tgt_mask(src, tgt)\n",
    "        tgt = self.tok_embedding(tgt) + self.pos_embedding(tgt)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            tgt, attn_map = layer(tgt, tgt_mask, enc_out, src_mask)\n",
    "\n",
    "        tgt = torch.matmul(tgt, self.tok_embedding.weight.transpose(0, 1))\n",
    "        # tgt = [배치 사이즈, 타겟 문장 길이, 은닉 차원]\n",
    "\n",
    "        return tgt, attn_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thGW36yDmVzz"
   },
   "source": [
    "## 5-6. Transformer 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASam4eYjmVzz"
   },
   "source": [
    "이제 앞서 정의한 레이어들을 토대로 **Transformer** 모델을 빌드해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "VVHiGPyBmVzz"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    '''트랜스포머 네트워크'''\n",
    "    def __init__(self, params):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(params)\n",
    "        self.decoder = Decoder(params)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \" src = [배치 사이즈, 소스 문장 길이] \"\n",
    "        \" tgt = [배치 사이즈, 타겟 문장 길이] \"\n",
    "\n",
    "        enc_out = self.encoder(src)\n",
    "        dec_out, attn = self.decoder(tgt, src, enc_out)\n",
    "        return dec_out, attn\n",
    "\n",
    "    def count_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vcWvJd4mVzz"
   },
   "source": [
    "Transformer 에서는 Adam Optimizer에 일부 스케줄 옵션을 적용해 사용하고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQtmIIwPmVz0"
   },
   "source": [
    "## 5-7. Transformer Optimizer 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFgCVOccmVz0"
   },
   "source": [
    "![](img/optim.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "0sbT1s7vmVz0"
   },
   "outputs": [],
   "source": [
    "class ScheduledOptim:\n",
    "    '''스케줄 옵티마이저'''\n",
    "    def __init__(self, optimizer, warmup_steps):\n",
    "        self.init_lr = np.power(params['hidden_dim'], -0.5)\n",
    "        self.optimizer = optimizer\n",
    "        self.step_num = 0\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def step(self):\n",
    "        self.step_num += 1\n",
    "        lr = self.init_lr * self.get_scale()\n",
    "\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = lr\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def get_scale(self):\n",
    "        return np.min([\n",
    "            np.power(self.step_num, -0.5),\n",
    "            self.step_num * np.power(self.warmup_steps, -1.5)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHcMCYQ_mVz0"
   },
   "source": [
    "## 6. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ILqqpABNmVz0",
    "outputId": "24f8f105-1265-4472-8f07-187c2f78fb8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 64,618,496 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# 모델 정의\n",
    "\n",
    "model = Transformer(params)\n",
    "model.to(device)\n",
    "print(f'The model has {model.count_params():,} trainable parameters')\n",
    "\n",
    "\n",
    "# 로스 함수 정의\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "criterion.to(device)\n",
    "\n",
    "\n",
    "# 옵티마이저 정의\n",
    "\n",
    "optimizer = ScheduledOptim(\n",
    "    optim.Adam(model.parameters(), betas=[0.9, 0.98], eps=1e-9),\n",
    "    warmup_steps=4000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "ILSrc2MOmVz0"
   },
   "outputs": [],
   "source": [
    "params['num_epoch'] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "JBD7d5rfmVz0",
    "outputId": "6dc17709-1d50-4bb0-944d-99ce76316c2a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   0%|          | 0/20 [00:00<?, ?it/s]\n",
      "training:   0%|          | 0/6250 [00:00<?, ?it/s]\u001b[A\n",
      "training:   1%|▏         | 89/6250 [00:10<11:38,  8.82it/s]\u001b[A\n",
      "training:   3%|▎         | 178/6250 [00:20<11:38,  8.69it/s]\u001b[A\n",
      "training:   4%|▍         | 281/6250 [00:30<10:34,  9.41it/s]\u001b[A\n",
      "training:   6%|▌         | 385/6250 [00:40<09:59,  9.78it/s]\u001b[A\n",
      "training:   8%|▊         | 490/6250 [00:50<09:35, 10.01it/s]\u001b[A\n",
      "training:  10%|▉         | 595/6250 [01:01<09:25,  9.99it/s]\u001b[A\n",
      "training:  11%|█         | 696/6250 [01:11<09:14, 10.02it/s]\u001b[A\n",
      "training:  13%|█▎        | 797/6250 [01:21<09:11,  9.88it/s]\u001b[A\n",
      "training:  14%|█▍        | 893/6250 [01:32<09:17,  9.61it/s]\u001b[A\n",
      "training:  16%|█▌        | 984/6250 [01:43<09:29,  9.25it/s]\u001b[A\n",
      "training:  17%|█▋        | 1075/6250 [01:53<09:23,  9.19it/s]\u001b[A\n",
      "training:  19%|█▉        | 1176/6250 [02:03<08:56,  9.45it/s]\u001b[A\n",
      "training:  19%|█▉        | 1176/6250 [02:13<08:56,  9.45it/s]\u001b[A\n",
      "training:  20%|██        | 1267/6250 [02:13<08:55,  9.30it/s]\u001b[A\n",
      "training:  22%|██▏       | 1357/6250 [02:23<08:57,  9.11it/s]\u001b[A\n",
      "training:  23%|██▎       | 1446/6250 [02:33<08:52,  9.02it/s]\u001b[A\n",
      "training:  25%|██▍       | 1535/6250 [02:45<09:05,  8.64it/s]\u001b[A\n",
      "training:  26%|██▌       | 1622/6250 [02:55<08:55,  8.64it/s]\u001b[A\n",
      "training:  27%|██▋       | 1712/6250 [03:05<08:40,  8.72it/s]\u001b[A\n",
      "training:  29%|██▉       | 1810/6250 [03:15<08:12,  9.02it/s]\u001b[A\n",
      "training:  31%|███       | 1908/6250 [03:26<08:03,  8.98it/s]\u001b[A\n",
      "training:  32%|███▏      | 1998/6250 [03:37<08:11,  8.64it/s]\u001b[A\n",
      "training:  33%|███▎      | 2083/6250 [03:47<08:04,  8.60it/s]\u001b[A\n",
      "training:  35%|███▍      | 2168/6250 [03:57<07:57,  8.55it/s]\u001b[A\n",
      "training:  36%|███▌      | 2254/6250 [04:07<07:46,  8.56it/s]\u001b[A\n",
      "training:  37%|███▋      | 2340/6250 [04:18<07:40,  8.49it/s]\u001b[A\n",
      "training:  39%|███▉      | 2426/6250 [04:28<07:30,  8.50it/s]\u001b[A\n",
      "training:  40%|████      | 2512/6250 [04:38<07:22,  8.45it/s]\u001b[A\n",
      "training:  42%|████▏     | 2597/6250 [04:48<07:12,  8.45it/s]\u001b[A\n",
      "training:  43%|████▎     | 2695/6250 [04:58<06:41,  8.84it/s]\u001b[A\n",
      "training:  45%|████▍     | 2793/6250 [05:09<06:31,  8.84it/s]\u001b[A\n",
      "training:  46%|████▌     | 2882/6250 [05:20<06:24,  8.76it/s]\u001b[A\n",
      "training:  48%|████▊     | 2983/6250 [05:30<05:57,  9.15it/s]\u001b[A\n",
      "training:  49%|████▉     | 3084/6250 [05:40<05:41,  9.28it/s]\u001b[A\n",
      "training:  51%|█████     | 3180/6250 [05:52<05:40,  9.01it/s]\u001b[A\n",
      "training:  52%|█████▏    | 3274/6250 [06:02<05:27,  9.10it/s]\u001b[A\n",
      "training:  54%|█████▍    | 3368/6250 [06:12<05:16,  9.12it/s]\u001b[A\n",
      "training:  54%|█████▍    | 3368/6250 [06:23<05:16,  9.12it/s]\u001b[A\n",
      "training:  55%|█████▌    | 3460/6250 [06:23<05:12,  8.92it/s]\u001b[A\n",
      "training:  57%|█████▋    | 3551/6250 [06:33<05:00,  8.97it/s]\u001b[A\n",
      "training:  58%|█████▊    | 3642/6250 [06:43<04:54,  8.85it/s]\u001b[A\n",
      "training:  60%|█████▉    | 3733/6250 [06:53<04:42,  8.92it/s]\u001b[A\n",
      "training:  61%|██████    | 3824/6250 [07:03<04:30,  8.97it/s]\u001b[A\n",
      "training:  63%|██████▎   | 3915/6250 [07:14<04:23,  8.85it/s]\u001b[A\n",
      "training:  64%|██████▍   | 4004/6250 [07:24<04:13,  8.85it/s]\u001b[A\n",
      "training:  65%|██████▌   | 4093/6250 [07:35<04:08,  8.67it/s]\u001b[A\n",
      "training:  67%|██████▋   | 4189/6250 [07:45<03:50,  8.94it/s]\u001b[A\n",
      "training:  69%|██████▊   | 4285/6250 [07:56<03:39,  8.95it/s]\u001b[A\n",
      "training:  70%|███████   | 4376/6250 [08:06<03:28,  8.98it/s]\u001b[A\n",
      "training:  72%|███████▏  | 4471/6250 [08:16<03:15,  9.10it/s]\u001b[A\n",
      "training:  73%|███████▎  | 4569/6250 [08:26<03:00,  9.30it/s]\u001b[A\n",
      "training:  75%|███████▍  | 4673/6250 [08:36<02:43,  9.62it/s]\u001b[A\n",
      "training:  76%|███████▋  | 4777/6250 [08:47<02:33,  9.61it/s]\u001b[A\n",
      "training:  78%|███████▊  | 4873/6250 [08:57<02:26,  9.40it/s]\u001b[A\n",
      "training:  79%|███████▉  | 4963/6250 [09:08<02:19,  9.25it/s]\u001b[A\n",
      "training:  81%|████████  | 5063/6250 [09:18<02:05,  9.44it/s]\u001b[A\n",
      "training:  83%|████████▎ | 5163/6250 [09:29<01:58,  9.14it/s]\u001b[A\n",
      "training:  84%|████████▍ | 5253/6250 [09:39<01:49,  9.08it/s]\u001b[A\n",
      "training:  86%|████████▌ | 5352/6250 [09:49<01:36,  9.31it/s]\u001b[A\n",
      "training:  87%|████████▋ | 5456/6250 [09:59<01:22,  9.62it/s]\u001b[A\n",
      "training:  89%|████████▉ | 5560/6250 [10:10<01:11,  9.70it/s]\u001b[A\n",
      "training:  91%|█████████ | 5659/6250 [10:20<01:01,  9.66it/s]\u001b[A\n",
      "training:  92%|█████████▏| 5764/6250 [10:30<00:49,  9.88it/s]\u001b[A\n",
      "training:  94%|█████████▍| 5869/6250 [10:42<00:39,  9.68it/s]\u001b[A\n",
      "training:  95%|█████████▌| 5967/6250 [10:52<00:29,  9.71it/s]\u001b[A\n",
      "training:  97%|█████████▋| 6065/6250 [11:03<00:19,  9.52it/s]\u001b[A\n",
      "training:  99%|█████████▊| 6168/6250 [11:13<00:08,  9.73it/s]\u001b[A\n",
      "                                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Train Loss: 2.502 | Time: 682.102511882782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   5%|▌         | 1/20 [11:24<3:36:50, 684.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장한 파일 이름 : ./epoch_0.pth  저장하는 데 걸린 시간 : 2.6553115844726562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training:   0%|          | 0/6250 [00:00<?, ?it/s]\u001b[A\n",
      "training:   2%|▏         | 98/6250 [00:10<10:32,  9.73it/s]\u001b[A\n",
      "training:   3%|▎         | 196/6250 [00:21<11:16,  8.95it/s]\u001b[A\n",
      "training:   5%|▍         | 288/6250 [00:31<10:59,  9.04it/s]\u001b[A\n",
      "training:   6%|▌         | 380/6250 [00:42<10:57,  8.93it/s]\u001b[A\n",
      "training:   7%|▋         | 468/6250 [00:52<10:56,  8.81it/s]\u001b[A\n",
      "training:   9%|▉         | 559/6250 [01:02<10:40,  8.88it/s]\u001b[A\n",
      "training:  10%|█         | 650/6250 [01:13<10:37,  8.78it/s]\u001b[A\n",
      "training:  12%|█▏        | 744/6250 [01:23<10:14,  8.95it/s]\u001b[A\n",
      "training:  13%|█▎        | 838/6250 [01:33<10:04,  8.96it/s]\u001b[A\n",
      "training:  15%|█▍        | 934/6250 [01:43<09:42,  9.12it/s]\u001b[A\n",
      "training:  16%|█▋        | 1030/6250 [01:53<09:26,  9.21it/s]\u001b[A\n",
      "training:  18%|█▊        | 1127/6250 [02:04<09:08,  9.34it/s]\u001b[A\n",
      "training:  20%|█▉        | 1224/6250 [02:14<09:06,  9.19it/s]\u001b[A\n",
      "training:  21%|██        | 1315/6250 [02:25<08:59,  9.14it/s]\u001b[A\n",
      "training:  23%|██▎       | 1412/6250 [02:35<08:40,  9.29it/s]\u001b[A\n",
      "training:  24%|██▍       | 1509/6250 [02:45<08:33,  9.23it/s]\u001b[A\n",
      "training:  26%|██▌       | 1600/6250 [02:55<08:28,  9.15it/s]\u001b[A\n",
      "training:  27%|██▋       | 1690/6250 [03:06<08:23,  9.05it/s]\u001b[A\n",
      "training:  29%|██▊       | 1789/6250 [03:16<07:59,  9.30it/s]\u001b[A\n",
      "training:  30%|███       | 1893/6250 [03:26<07:32,  9.62it/s]\u001b[A\n",
      "training:  32%|███▏      | 1997/6250 [03:36<07:16,  9.74it/s]\u001b[A\n",
      "training:  34%|███▎      | 2098/6250 [03:47<07:15,  9.53it/s]\u001b[A\n",
      "training:  35%|███▌      | 2190/6250 [03:57<07:10,  9.43it/s]\u001b[A\n",
      "training:  37%|███▋      | 2292/6250 [04:07<06:50,  9.64it/s]\u001b[A\n",
      "training:  37%|███▋      | 2292/6250 [04:18<06:50,  9.64it/s]\u001b[A\n",
      "training:  38%|███▊      | 2389/6250 [04:18<06:50,  9.40it/s]\u001b[A\n",
      "training:  40%|███▉      | 2478/6250 [04:28<06:51,  9.17it/s]\u001b[A\n",
      "training:  41%|████      | 2575/6250 [04:39<06:35,  9.30it/s]\u001b[A\n",
      "training:  43%|████▎     | 2672/6250 [04:49<06:21,  9.38it/s]\u001b[A\n",
      "training:  44%|████▍     | 2768/6250 [04:59<06:12,  9.35it/s]\u001b[A\n",
      "training:  46%|████▌     | 2861/6250 [05:10<06:08,  9.20it/s]\u001b[A\n",
      "training:  47%|████▋     | 2960/6250 [05:20<05:50,  9.38it/s]\u001b[A\n",
      "training:  49%|████▉     | 3059/6250 [05:30<05:38,  9.43it/s]\u001b[A\n",
      "training:  51%|█████     | 3159/6250 [05:40<05:22,  9.59it/s]\u001b[A\n",
      "training:  52%|█████▏    | 3263/6250 [05:50<05:04,  9.82it/s]\u001b[A\n",
      "training:  54%|█████▍    | 3368/6250 [06:00<04:48, 10.00it/s]\u001b[A\n",
      "training:  56%|█████▌    | 3473/6250 [06:11<04:40,  9.88it/s]\u001b[A\n",
      "training:  57%|█████▋    | 3570/6250 [06:21<04:32,  9.82it/s]\u001b[A\n",
      "training:  59%|█████▊    | 3667/6250 [06:33<04:36,  9.36it/s]\u001b[A\n",
      "training:  60%|██████    | 3751/6250 [06:43<04:41,  8.88it/s]\u001b[A\n",
      "training:  61%|██████▏   | 3829/6250 [06:54<04:46,  8.46it/s]\u001b[A\n",
      "training:  63%|██████▎   | 3915/6250 [07:04<04:35,  8.48it/s]\u001b[A\n",
      "training:  64%|██████▍   | 4005/6250 [07:14<04:20,  8.60it/s]\u001b[A\n",
      "training:  66%|██████▌   | 4095/6250 [07:24<04:09,  8.65it/s]\u001b[A\n",
      "training:  67%|██████▋   | 4183/6250 [07:35<04:01,  8.57it/s]\u001b[A\n",
      "training:  68%|██████▊   | 4271/6250 [07:45<03:49,  8.63it/s]\u001b[A\n",
      "training:  70%|██████▉   | 4359/6250 [07:55<03:40,  8.58it/s]\u001b[A\n",
      "training:  71%|███████   | 4452/6250 [08:05<03:24,  8.77it/s]\u001b[A\n",
      "training:  73%|███████▎  | 4545/6250 [08:15<03:11,  8.90it/s]\u001b[A\n",
      "training:  74%|███████▍  | 4638/6250 [08:26<03:03,  8.80it/s]\u001b[A\n",
      "training:  76%|███████▌  | 4727/6250 [08:36<02:52,  8.81it/s]\u001b[A\n",
      "training:  77%|███████▋  | 4816/6250 [08:47<02:45,  8.66it/s]\u001b[A\n",
      "training:  78%|███████▊  | 4900/6250 [08:57<02:39,  8.48it/s]\u001b[A\n",
      "training:  80%|███████▉  | 4986/6250 [09:08<02:28,  8.50it/s]\u001b[A\n",
      "training:  81%|████████▏ | 5083/6250 [09:18<02:12,  8.83it/s]\u001b[A\n",
      "training:  81%|████████▏ | 5083/6250 [09:28<02:12,  8.83it/s]\u001b[A\n",
      "training:  83%|████████▎ | 5172/6250 [09:28<02:03,  8.72it/s]\u001b[A\n",
      "training:  84%|████████▍ | 5259/6250 [09:38<01:53,  8.70it/s]\u001b[A\n",
      "training:  86%|████████▌ | 5349/6250 [09:48<01:42,  8.79it/s]\u001b[A\n",
      "training:  87%|████████▋ | 5439/6250 [09:58<01:32,  8.81it/s]\u001b[A\n",
      "training:  88%|████████▊ | 5528/6250 [10:09<01:24,  8.57it/s]\u001b[A\n",
      "training:  90%|████████▉ | 5609/6250 [10:20<01:16,  8.35it/s]\u001b[A\n",
      "training:  91%|█████████ | 5696/6250 [10:30<01:05,  8.45it/s]\u001b[A\n",
      "training:  93%|█████████▎| 5789/6250 [10:40<00:53,  8.69it/s]\u001b[A\n",
      "training:  94%|█████████▍| 5882/6250 [10:50<00:42,  8.71it/s]\u001b[A\n",
      "training:  96%|█████████▌| 5970/6250 [11:03<00:34,  8.22it/s]\u001b[A\n",
      "training:  97%|█████████▋| 6054/6250 [11:13<00:23,  8.25it/s]\u001b[A\n",
      "training:  98%|█████████▊| 6138/6250 [11:24<00:13,  8.00it/s]\u001b[A\n",
      "training:  99%|█████████▉| 6218/6250 [11:34<00:04,  8.00it/s]\u001b[A\n",
      "                                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Train Loss: 2.302 | Time: 698.5236737728119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  10%|█         | 2/20 [23:05<3:28:19, 694.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장한 파일 이름 : ./epoch_1.pth  저장하는 데 걸린 시간 : 2.6784427165985107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training:   0%|          | 0/6250 [00:00<?, ?it/s]\u001b[A\n",
      "training:   1%|          | 78/6250 [00:10<13:15,  7.76it/s]\u001b[A\n",
      "training:   2%|▏         | 156/6250 [00:20<13:09,  7.72it/s]\u001b[A\n",
      "training:   4%|▍         | 240/6250 [00:30<12:31,  8.00it/s]\u001b[A\n",
      "training:   5%|▌         | 324/6250 [00:40<12:12,  8.09it/s]\u001b[A\n",
      "training:   7%|▋         | 417/6250 [00:50<11:24,  8.52it/s]\u001b[A\n",
      "training:   8%|▊         | 510/6250 [01:01<11:13,  8.52it/s]\u001b[A\n",
      "training:  10%|▉         | 600/6250 [01:11<10:51,  8.67it/s]\u001b[A\n",
      "training:  11%|█         | 697/6250 [01:21<10:19,  8.97it/s]\u001b[A\n",
      "training:  13%|█▎        | 794/6250 [01:31<10:00,  9.08it/s]\u001b[A\n",
      "training:  14%|█▍        | 889/6250 [01:41<09:42,  9.21it/s]\u001b[A\n",
      "training:  16%|█▌        | 984/6250 [01:52<09:42,  9.04it/s]\u001b[A\n",
      "training:  17%|█▋        | 1071/6250 [02:03<09:46,  8.83it/s]\u001b[A\n",
      "training:  18%|█▊        | 1155/6250 [02:14<09:59,  8.49it/s]\u001b[A\n",
      "training:  20%|█▉        | 1233/6250 [02:24<10:17,  8.13it/s]\u001b[A\n",
      "training:  21%|██        | 1312/6250 [02:34<10:14,  8.04it/s]\u001b[A\n",
      "training:  22%|██▏       | 1393/6250 [02:45<10:05,  8.02it/s]\u001b[A\n",
      "training:  24%|██▎       | 1473/6250 [02:55<10:03,  7.91it/s]\u001b[A\n",
      "training:  25%|██▍       | 1562/6250 [03:05<09:32,  8.18it/s]\u001b[A\n",
      "training:  26%|██▋       | 1651/6250 [03:16<09:24,  8.15it/s]\u001b[A\n",
      "training:  28%|██▊       | 1736/6250 [03:26<09:07,  8.24it/s]\u001b[A\n",
      "training:  29%|██▉       | 1826/6250 [03:36<08:43,  8.45it/s]\u001b[A\n",
      "training:  31%|███       | 1916/6250 [03:46<08:27,  8.55it/s]\u001b[A\n",
      "training:  31%|███       | 1916/6250 [03:57<08:27,  8.55it/s]\u001b[A\n",
      "training:  32%|███▏      | 1995/6250 [03:57<08:36,  8.23it/s]\u001b[A\n",
      "training:  33%|███▎      | 2076/6250 [04:07<08:29,  8.19it/s]\u001b[A\n",
      "training:  35%|███▍      | 2168/6250 [04:17<08:01,  8.47it/s]\u001b[A\n",
      "training:  36%|███▌      | 2260/6250 [04:28<07:58,  8.34it/s]\u001b[A\n",
      "training:  37%|███▋      | 2343/6250 [04:38<07:49,  8.32it/s]\u001b[A\n",
      "training:  39%|███▉      | 2426/6250 [04:49<07:40,  8.30it/s]\u001b[A\n",
      "training:  40%|████      | 2509/6250 [04:59<07:32,  8.27it/s]\u001b[A\n",
      "training:  41%|████▏     | 2592/6250 [05:09<07:22,  8.27it/s]\u001b[A\n",
      "training:  43%|████▎     | 2675/6250 [05:19<07:14,  8.23it/s]\u001b[A\n",
      "training:  44%|████▍     | 2757/6250 [05:29<07:11,  8.10it/s]\u001b[A\n",
      "training:  45%|████▌     | 2840/6250 [05:39<06:58,  8.16it/s]\u001b[A\n",
      "training:  47%|████▋     | 2923/6250 [05:50<06:57,  7.96it/s]\u001b[A\n",
      "training:  48%|████▊     | 3001/6250 [06:01<06:51,  7.90it/s]\u001b[A\n",
      "training:  49%|████▉     | 3089/6250 [06:11<06:28,  8.13it/s]\u001b[A\n",
      "training:  51%|█████     | 3176/6250 [06:22<06:22,  8.03it/s]\u001b[A\n",
      "training:  52%|█████▏    | 3259/6250 [06:32<06:09,  8.09it/s]\u001b[A\n",
      "training:  53%|█████▎    | 3342/6250 [06:42<05:57,  8.14it/s]\u001b[A\n",
      "training:  55%|█████▌    | 3443/6250 [06:52<05:23,  8.69it/s]\u001b[A\n",
      "training:  57%|█████▋    | 3543/6250 [07:02<04:59,  9.04it/s]\u001b[A\n",
      "training:  58%|█████▊    | 3642/6250 [07:14<04:54,  8.87it/s]\u001b[A\n",
      "training:  60%|█████▉    | 3740/6250 [07:24<04:34,  9.13it/s]\u001b[A\n",
      "training:  61%|██████▏   | 3838/6250 [07:34<04:22,  9.18it/s]\u001b[A\n",
      "training:  63%|██████▎   | 3931/6250 [07:46<04:25,  8.74it/s]\u001b[A\n",
      "training:  64%|██████▍   | 4010/6250 [07:56<04:23,  8.50it/s]\u001b[A\n",
      "training:  65%|██████▌   | 4093/6250 [08:06<04:15,  8.44it/s]\u001b[A\n",
      "training:  67%|██████▋   | 4187/6250 [08:16<03:57,  8.69it/s]\u001b[A\n",
      "training:  67%|██████▋   | 4187/6250 [08:27<03:57,  8.69it/s]\u001b[A\n",
      "training:  68%|██████▊   | 4271/6250 [08:27<03:54,  8.43it/s]\u001b[A\n",
      "training:  70%|██████▉   | 4351/6250 [08:37<03:49,  8.29it/s]\u001b[A\n",
      "training:  71%|███████   | 4433/6250 [08:47<03:39,  8.26it/s]\u001b[A\n",
      "training:  72%|███████▏  | 4515/6250 [08:57<03:31,  8.19it/s]\u001b[A\n",
      "training:  74%|███████▎  | 4596/6250 [09:07<03:22,  8.16it/s]\u001b[A\n",
      "training:  75%|███████▍  | 4680/6250 [09:17<03:10,  8.22it/s]\u001b[A\n",
      "training:  76%|███████▌  | 4765/6250 [09:27<02:58,  8.30it/s]\u001b[A\n",
      "training:  78%|███████▊  | 4851/6250 [09:37<02:47,  8.37it/s]\u001b[A\n",
      "training:  79%|███████▉  | 4937/6250 [09:48<02:39,  8.25it/s]\u001b[A\n",
      "training:  80%|████████  | 5017/6250 [09:59<02:33,  8.05it/s]\u001b[A\n",
      "training:  82%|████████▏ | 5098/6250 [10:09<02:23,  8.04it/s]\u001b[A\n",
      "training:  83%|████████▎ | 5190/6250 [10:19<02:06,  8.35it/s]\u001b[A\n",
      "training:  85%|████████▍ | 5282/6250 [10:31<01:58,  8.20it/s]\u001b[A\n",
      "training:  86%|████████▌ | 5364/6250 [10:41<01:48,  8.19it/s]\u001b[A\n",
      "training:  87%|████████▋ | 5453/6250 [10:51<01:34,  8.39it/s]\u001b[A\n",
      "training:  89%|████████▊ | 5542/6250 [11:02<01:25,  8.26it/s]\u001b[A\n",
      "training:  90%|█████████ | 5636/6250 [11:12<01:11,  8.59it/s]\u001b[A\n",
      "training:  92%|█████████▏| 5730/6250 [11:23<01:00,  8.55it/s]\u001b[A\n",
      "training:  93%|█████████▎| 5815/6250 [11:34<00:52,  8.34it/s]\u001b[A\n",
      "training:  94%|█████████▍| 5901/6250 [11:44<00:41,  8.41it/s]\u001b[A\n",
      "training:  96%|█████████▌| 5991/6250 [11:54<00:30,  8.57it/s]\u001b[A\n",
      "training:  97%|█████████▋| 6088/6250 [12:04<00:18,  8.90it/s]\u001b[A\n",
      "training:  99%|█████████▉| 6186/6250 [12:14<00:06,  9.15it/s]\u001b[A\n",
      "                                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Train Loss: 2.259 | Time: 740.8336136341095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  15%|█▌        | 3/20 [35:29<3:23:06, 716.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장한 파일 이름 : ./epoch_2.pth  저장하는 데 걸린 시간 : 2.658512830734253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training:   0%|          | 0/6250 [00:00<?, ?it/s]\u001b[A\n",
      "training:   1%|▏         | 93/6250 [00:10<11:07,  9.22it/s]\u001b[A\n",
      "training:   3%|▎         | 186/6250 [00:21<12:05,  8.36it/s]\u001b[A\n",
      "training:   4%|▍         | 268/6250 [00:31<12:02,  8.28it/s]\u001b[A\n",
      "training:   6%|▌         | 350/6250 [00:42<12:20,  7.97it/s]\u001b[A\n",
      "training:   7%|▋         | 438/6250 [00:52<11:44,  8.25it/s]\u001b[A\n",
      "training:   7%|▋         | 438/6250 [01:03<11:44,  8.25it/s]\u001b[A\n",
      "training:   8%|▊         | 523/6250 [01:04<11:52,  8.04it/s]\u001b[A\n",
      "training:  10%|▉         | 602/6250 [01:14<11:46,  7.99it/s]\u001b[A\n",
      "training:  11%|█         | 691/6250 [01:24<11:12,  8.27it/s]\u001b[A\n",
      "training:  13%|█▎        | 784/6250 [01:34<10:37,  8.57it/s]\u001b[A\n",
      "training:  14%|█▍        | 877/6250 [01:45<10:34,  8.47it/s]\u001b[A\n",
      "training:  15%|█▌        | 960/6250 [01:55<10:30,  8.39it/s]\u001b[A\n",
      "training:  17%|█▋        | 1050/6250 [02:05<10:06,  8.57it/s]\u001b[A\n",
      "training:  18%|█▊        | 1146/6250 [02:15<09:35,  8.86it/s]\u001b[A\n",
      "training:  20%|█▉        | 1242/6250 [02:25<09:14,  9.03it/s]\u001b[A\n",
      "training:  21%|██▏       | 1337/6250 [02:36<09:08,  8.95it/s]\u001b[A\n",
      "training:  23%|██▎       | 1425/6250 [02:46<09:05,  8.84it/s]\u001b[A\n",
      "training:  24%|██▍       | 1511/6250 [02:58<09:26,  8.36it/s]\u001b[A\n",
      "training:  25%|██▌       | 1585/6250 [03:09<09:49,  7.91it/s]\u001b[A\n",
      "training:  27%|██▋       | 1657/6250 [03:19<09:56,  7.70it/s]\u001b[A\n",
      "training:  28%|██▊       | 1730/6250 [03:29<09:56,  7.58it/s]\u001b[A\n",
      "training:  29%|██▉       | 1803/6250 [03:39<09:54,  7.48it/s]\u001b[A\n",
      "training:  30%|███       | 1876/6250 [03:49<09:51,  7.40it/s]\u001b[A\n",
      "training:  31%|███       | 1948/6250 [04:00<09:58,  7.19it/s]\u001b[A\n",
      "training:  32%|███▏      | 2029/6250 [04:10<09:26,  7.45it/s]\u001b[A\n",
      "training:  34%|███▍      | 2110/6250 [04:23<09:49,  7.03it/s]\u001b[A\n",
      "training:  35%|███▍      | 2174/6250 [04:33<09:55,  6.85it/s]\u001b[A\n",
      "training:  36%|███▌      | 2251/6250 [04:43<09:26,  7.07it/s]\u001b[A\n",
      "training:  37%|███▋      | 2327/6250 [04:53<09:05,  7.19it/s]\u001b[A\n",
      "training:  38%|███▊      | 2402/6250 [05:03<08:55,  7.19it/s]\u001b[A\n",
      "training:  38%|███▊      | 2402/6250 [05:13<08:55,  7.19it/s]\u001b[A\n",
      "training:  40%|███▉      | 2475/6250 [05:14<08:43,  7.21it/s]\u001b[A\n",
      "training:  41%|████      | 2549/6250 [05:24<08:29,  7.26it/s]\u001b[A\n",
      "training:  42%|████▏     | 2623/6250 [05:34<08:24,  7.19it/s]\u001b[A\n",
      "training:  43%|████▎     | 2699/6250 [05:44<08:06,  7.30it/s]\u001b[A\n",
      "training:  45%|████▍     | 2791/6250 [05:54<07:20,  7.85it/s]\u001b[A\n",
      "training:  46%|████▌     | 2883/6250 [06:06<07:07,  7.87it/s]\u001b[A\n",
      "training:  48%|████▊     | 2978/6250 [06:16<06:32,  8.34it/s]\u001b[A\n",
      "training:  49%|████▉     | 3073/6250 [06:28<06:32,  8.09it/s]\u001b[A\n",
      "training:  50%|█████     | 3150/6250 [06:39<06:37,  7.80it/s]\u001b[A\n",
      "training:  52%|█████▏    | 3222/6250 [06:49<06:38,  7.60it/s]\u001b[A\n",
      "training:  53%|█████▎    | 3293/6250 [07:00<06:41,  7.36it/s]\u001b[A\n",
      "training:  54%|█████▍    | 3376/6250 [07:10<06:17,  7.61it/s]\u001b[A\n",
      "training:  55%|█████▌    | 3459/6250 [07:20<05:57,  7.80it/s]\u001b[A\n",
      "training:  57%|█████▋    | 3542/6250 [07:30<05:46,  7.81it/s]\u001b[A\n",
      "training:  58%|█████▊    | 3626/6250 [07:40<05:29,  7.97it/s]\u001b[A\n",
      "training:  59%|█████▉    | 3714/6250 [07:51<05:08,  8.21it/s]\u001b[A\n",
      "training:  61%|██████    | 3802/6250 [08:01<04:55,  8.27it/s]\u001b[A\n",
      "training:  62%|██████▏   | 3906/6250 [08:11<04:24,  8.87it/s]\u001b[A\n",
      "training:  64%|██████▍   | 4010/6250 [08:21<04:00,  9.29it/s]\u001b[A\n",
      "training:  66%|██████▌   | 4113/6250 [08:31<03:43,  9.56it/s]\u001b[A\n",
      "training:  67%|██████▋   | 4216/6250 [08:41<03:28,  9.78it/s]\u001b[A\n",
      "training:  69%|██████▉   | 4319/6250 [08:51<03:14,  9.92it/s]\u001b[A\n",
      "training:  71%|███████   | 4422/6250 [09:02<03:06,  9.80it/s]\u001b[A\n",
      "training:  72%|███████▏  | 4518/6250 [09:13<03:01,  9.54it/s]\u001b[A\n",
      "training:  74%|███████▎  | 4609/6250 [09:23<02:54,  9.38it/s]\u001b[A\n",
      "training:  75%|███████▌  | 4700/6250 [09:33<02:47,  9.28it/s]\u001b[A\n",
      "training:  77%|███████▋  | 4791/6250 [09:43<02:40,  9.11it/s]\u001b[A\n",
      "training:  77%|███████▋  | 4791/6250 [09:53<02:40,  9.11it/s]\u001b[A\n",
      "training:  78%|███████▊  | 4881/6250 [09:53<02:30,  9.07it/s]\u001b[A\n",
      "training:  80%|███████▉  | 4971/6250 [10:04<02:24,  8.86it/s]\u001b[A\n",
      "training:  81%|████████  | 5060/6250 [10:14<02:14,  8.86it/s]\u001b[A\n",
      "training:  82%|████████▏ | 5153/6250 [10:24<02:02,  8.98it/s]\u001b[A\n",
      "training:  84%|████████▍ | 5257/6250 [10:34<01:45,  9.39it/s]\u001b[A\n",
      "training:  86%|████████▌ | 5361/6250 [10:45<01:34,  9.44it/s]\u001b[A\n",
      "training:  87%|████████▋ | 5457/6250 [10:56<01:25,  9.30it/s]\u001b[A\n",
      "training:  89%|████████▉ | 5547/6250 [11:06<01:16,  9.18it/s]\u001b[A\n",
      "training:  90%|█████████ | 5647/6250 [11:16<01:04,  9.40it/s]\u001b[A\n",
      "training:  92%|█████████▏| 5747/6250 [11:27<00:53,  9.32it/s]\u001b[A\n",
      "training:  93%|█████████▎| 5839/6250 [11:37<00:44,  9.27it/s]\u001b[A\n",
      "training:  95%|█████████▍| 5931/6250 [11:47<00:34,  9.20it/s]\u001b[A\n",
      "training:  96%|█████████▋| 6022/6250 [11:58<00:25,  9.03it/s]\u001b[A\n",
      "training:  98%|█████████▊| 6118/6250 [12:08<00:14,  9.17it/s]\u001b[A\n",
      "training:  99%|█████████▉| 6214/6250 [12:18<00:03,  9.27it/s]\u001b[A\n",
      "                                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Train Loss: 2.008 | Time: 742.1651451587677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  20%|██        | 4/20 [47:54<3:14:06, 727.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장한 파일 이름 : ./epoch_3.pth  저장하는 데 걸린 시간 : 2.6685421466827393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training:   0%|          | 0/6250 [00:00<?, ?it/s]\u001b[A\n",
      "training:   1%|▏         | 88/6250 [00:10<11:44,  8.74it/s]\u001b[A\n",
      "training:   3%|▎         | 176/6250 [00:20<11:38,  8.70it/s]\u001b[A\n",
      "training:   4%|▍         | 273/6250 [00:30<10:55,  9.12it/s]\u001b[A\n",
      "training:   6%|▌         | 370/6250 [00:41<10:53,  9.00it/s]\u001b[A\n",
      "training:   7%|▋         | 461/6250 [00:51<10:42,  9.01it/s]\u001b[A\n",
      "training:   9%|▉         | 552/6250 [01:01<10:33,  8.99it/s]\u001b[A\n",
      "training:  10%|█         | 642/6250 [01:11<10:27,  8.94it/s]\u001b[A\n",
      "training:  12%|█▏        | 741/6250 [01:21<09:56,  9.23it/s]\u001b[A\n",
      "training:  13%|█▎        | 840/6250 [01:33<09:57,  9.05it/s]\u001b[A\n",
      "training:  15%|█▍        | 930/6250 [01:43<09:48,  9.03it/s]\u001b[A\n",
      "training:  16%|█▋        | 1029/6250 [01:53<09:21,  9.29it/s]\u001b[A\n",
      "training:  18%|█▊        | 1128/6250 [02:04<09:17,  9.19it/s]\u001b[A\n",
      "training:  19%|█▉        | 1218/6250 [02:14<09:16,  9.04it/s]\u001b[A\n",
      "training:  21%|██        | 1305/6250 [02:24<09:13,  8.94it/s]\u001b[A\n",
      "training:  22%|██▏       | 1393/6250 [02:34<09:06,  8.90it/s]\u001b[A\n",
      "training:  24%|██▎       | 1484/6250 [02:44<08:54,  8.92it/s]\u001b[A\n",
      "training:  25%|██▌       | 1576/6250 [02:54<08:39,  9.00it/s]\u001b[A\n",
      "training:  27%|██▋       | 1674/6250 [03:04<08:15,  9.23it/s]\u001b[A\n",
      "training:  28%|██▊       | 1772/6250 [03:15<08:10,  9.14it/s]\u001b[A\n",
      "training:  30%|██▉       | 1862/6250 [03:25<08:02,  9.09it/s]\u001b[A\n",
      "training:  31%|███       | 1952/6250 [03:35<07:54,  9.05it/s]\u001b[A\n",
      "training:  33%|███▎      | 2046/6250 [03:45<07:39,  9.14it/s]\u001b[A\n",
      "training:  34%|███▍      | 2140/6250 [03:56<07:32,  9.08it/s]\u001b[A\n",
      "training:  36%|███▌      | 2233/6250 [04:06<07:20,  9.13it/s]\u001b[A\n",
      "training:  37%|███▋      | 2329/6250 [04:16<07:04,  9.25it/s]\u001b[A\n",
      "training:  39%|███▉      | 2425/6250 [04:26<06:53,  9.26it/s]\u001b[A\n",
      "training:  40%|████      | 2530/6250 [04:36<06:27,  9.60it/s]\u001b[A\n",
      "training:  42%|████▏     | 2635/6250 [04:47<06:10,  9.75it/s]\u001b[A\n",
      "training:  44%|████▍     | 2736/6250 [04:58<06:11,  9.45it/s]\u001b[A\n",
      "training:  45%|████▌     | 2825/6250 [05:08<06:09,  9.28it/s]\u001b[A\n",
      "training:  47%|████▋     | 2929/6250 [05:18<05:46,  9.58it/s]\u001b[A\n",
      "training:  47%|████▋     | 2929/6250 [05:29<05:46,  9.58it/s]\u001b[A\n",
      "training:  49%|████▊     | 3032/6250 [05:29<05:32,  9.69it/s]\u001b[A\n",
      "training:  50%|█████     | 3132/6250 [05:40<05:28,  9.50it/s]\u001b[A\n",
      "training:  52%|█████▏    | 3226/6250 [05:50<05:19,  9.46it/s]\u001b[A\n",
      "training:  53%|█████▎    | 3320/6250 [06:00<05:12,  9.36it/s]\u001b[A\n",
      "training:  55%|█████▍    | 3422/6250 [06:10<04:54,  9.61it/s]\u001b[A\n",
      "training:  56%|█████▋    | 3526/6250 [06:20<04:37,  9.83it/s]\u001b[A\n",
      "training:  58%|█████▊    | 3630/6250 [06:30<04:22,  9.98it/s]\u001b[A\n",
      "training:  60%|█████▉    | 3734/6250 [06:40<04:10, 10.06it/s]\u001b[A\n",
      "training:  61%|██████▏   | 3837/6250 [06:51<04:02,  9.96it/s]\u001b[A\n",
      "training:  63%|██████▎   | 3935/6250 [07:02<03:59,  9.67it/s]\u001b[A\n",
      "training:  64%|██████▍   | 4029/6250 [07:12<03:52,  9.57it/s]\u001b[A\n",
      "training:  66%|██████▌   | 4125/6250 [07:22<03:42,  9.56it/s]\u001b[A\n",
      "training:  68%|██████▊   | 4221/6250 [07:33<03:36,  9.37it/s]\u001b[A\n",
      "training:  69%|██████▉   | 4311/6250 [07:43<03:31,  9.18it/s]\u001b[A\n",
      "training:  70%|███████   | 4399/6250 [07:53<03:26,  8.98it/s]\u001b[A\n",
      "training:  72%|███████▏  | 4488/6250 [08:03<03:17,  8.93it/s]\u001b[A\n",
      "training:  73%|███████▎  | 4577/6250 [08:13<03:07,  8.92it/s]\u001b[A\n",
      "training:  75%|███████▍  | 4676/6250 [08:23<02:50,  9.21it/s]\u001b[A\n",
      "training:  76%|███████▋  | 4775/6250 [08:35<02:42,  9.06it/s]\u001b[A\n",
      "training:  78%|███████▊  | 4868/6250 [08:45<02:31,  9.10it/s]\u001b[A\n",
      "training:  79%|███████▉  | 4961/6250 [08:55<02:21,  9.09it/s]\u001b[A\n",
      "training:  81%|████████  | 5052/6250 [09:05<02:12,  9.02it/s]\u001b[A\n",
      "training:  82%|████████▏ | 5141/6250 [09:16<02:07,  8.69it/s]\u001b[A\n",
      "training:  84%|████████▎ | 5227/6250 [09:26<01:58,  8.66it/s]\u001b[A\n",
      "training:  85%|████████▌ | 5313/6250 [09:37<01:49,  8.57it/s]\u001b[A\n",
      "training:  86%|████████▋ | 5398/6250 [09:47<01:39,  8.53it/s]\u001b[A\n",
      "training:  88%|████████▊ | 5483/6250 [09:57<01:31,  8.37it/s]\u001b[A\n",
      "training:  89%|████████▉ | 5569/6250 [10:08<01:20,  8.43it/s]\u001b[A\n",
      "training:  91%|█████████ | 5660/6250 [10:18<01:08,  8.62it/s]\u001b[A\n",
      "training:  92%|█████████▏| 5751/6250 [10:28<00:58,  8.56it/s]\u001b[A\n",
      "training:  93%|█████████▎| 5843/6250 [10:38<00:46,  8.74it/s]\u001b[A\n",
      "training:  93%|█████████▎| 5843/6250 [10:49<00:46,  8.74it/s]\u001b[A\n",
      "training:  95%|█████████▍| 5935/6250 [10:49<00:35,  8.79it/s]\u001b[A\n",
      "training:  96%|█████████▋| 6024/6250 [11:00<00:26,  8.59it/s]\u001b[A\n",
      "training:  98%|█████████▊| 6112/6250 [11:10<00:15,  8.64it/s]\u001b[A\n",
      "training:  99%|█████████▉| 6200/6250 [11:21<00:05,  8.42it/s]\u001b[A\n",
      "                                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Train Loss: 1.882 | Time: 687.1182317733765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  25%|██▌       | 5/20 [59:24<2:58:32, 714.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장한 파일 이름 : ./epoch_4.pth  저장하는 데 걸린 시간 : 2.65262770652771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training:   0%|          | 0/6250 [00:00<?, ?it/s]\u001b[A\n",
      "training:   1%|▏         | 88/6250 [00:10<11:46,  8.72it/s]\u001b[A\n",
      "training:   3%|▎         | 176/6250 [00:20<11:58,  8.45it/s]\u001b[A\n",
      "training:   4%|▍         | 259/6250 [00:31<12:11,  8.19it/s]\u001b[A\n",
      "training:   5%|▌         | 341/6250 [00:41<12:03,  8.16it/s]\u001b[A\n",
      "training:   7%|▋         | 435/6250 [00:51<11:18,  8.58it/s]\u001b[A\n",
      "training:   8%|▊         | 529/6250 [01:01<10:53,  8.75it/s]\u001b[A\n",
      "training:  10%|▉         | 620/6250 [01:13<11:18,  8.30it/s]\u001b[A\n",
      "training:  11%|█▏        | 706/6250 [01:23<11:01,  8.38it/s]\u001b[A\n",
      "training:  13%|█▎        | 792/6250 [01:34<10:55,  8.33it/s]\u001b[A\n",
      "training:  14%|█▍        | 876/6250 [01:44<10:45,  8.33it/s]\u001b[A\n",
      "training:  15%|█▌        | 968/6250 [01:54<10:15,  8.58it/s]\u001b[A\n",
      "training:  17%|█▋        | 1060/6250 [02:04<09:58,  8.67it/s]\u001b[A\n",
      "training:  18%|█▊        | 1149/6250 [02:15<09:54,  8.58it/s]\u001b[A\n",
      "training:  20%|█▉        | 1233/6250 [02:25<09:49,  8.51it/s]\u001b[A\n",
      "training:  21%|██        | 1317/6250 [02:36<09:51,  8.34it/s]\u001b[A\n",
      "training:  22%|██▏       | 1397/6250 [02:46<09:51,  8.20it/s]\u001b[A\n",
      "training:  24%|██▍       | 1488/6250 [02:56<09:23,  8.45it/s]\u001b[A\n",
      "training:  25%|██▌       | 1579/6250 [03:07<09:12,  8.45it/s]\u001b[A\n",
      "training:  27%|██▋       | 1673/6250 [03:17<08:45,  8.71it/s]\u001b[A\n",
      "training:  28%|██▊       | 1767/6250 [03:28<08:36,  8.68it/s]\u001b[A\n",
      "training:  30%|██▉       | 1854/6250 [03:38<08:29,  8.62it/s]\u001b[A\n",
      "training:  31%|███       | 1943/6250 [03:48<08:15,  8.69it/s]\u001b[A\n",
      "training:  33%|███▎      | 2032/6250 [03:58<08:07,  8.66it/s]\u001b[A\n",
      "training:  34%|███▍      | 2118/6250 [04:09<08:05,  8.52it/s]\u001b[A\n",
      "training:  35%|███▌      | 2204/6250 [04:19<07:54,  8.52it/s]\u001b[A\n",
      "training:  35%|███▌      | 2204/6250 [04:29<07:54,  8.52it/s]\u001b[A\n",
      "training:  37%|███▋      | 2289/6250 [04:29<07:47,  8.47it/s]\u001b[A\n",
      "training:  38%|███▊      | 2385/6250 [04:39<07:19,  8.80it/s]\u001b[A\n",
      "training:  40%|███▉      | 2481/6250 [04:51<07:22,  8.52it/s]\u001b[A\n",
      "training:  41%|████      | 2568/6250 [05:01<07:10,  8.56it/s]\u001b[A\n",
      "training:  42%|████▏     | 2655/6250 [05:12<07:09,  8.37it/s]\u001b[A\n",
      "training:  44%|████▍     | 2740/6250 [05:22<06:58,  8.40it/s]\u001b[A\n",
      "training:  45%|████▌     | 2833/6250 [05:32<06:35,  8.64it/s]\u001b[A\n",
      "training:  47%|████▋     | 2926/6250 [05:43<06:24,  8.64it/s]\u001b[A\n",
      "training:  48%|████▊     | 3013/6250 [05:54<06:22,  8.45it/s]\u001b[A\n",
      "training:  50%|████▉     | 3094/6250 [06:04<06:19,  8.32it/s]\u001b[A\n",
      "training:  51%|█████     | 3176/6250 [06:14<06:12,  8.26it/s]\u001b[A\n",
      "training:  52%|█████▏    | 3265/6250 [06:24<05:54,  8.43it/s]\u001b[A\n",
      "training:  54%|█████▎    | 3356/6250 [06:34<05:36,  8.61it/s]\u001b[A\n",
      "training:  55%|█████▌    | 3451/6250 [06:44<05:16,  8.84it/s]\u001b[A\n",
      "training:  57%|█████▋    | 3545/6250 [06:56<05:14,  8.59it/s]\u001b[A\n",
      "training:  58%|█████▊    | 3626/6250 [07:06<05:13,  8.36it/s]\u001b[A\n",
      "training:  59%|█████▉    | 3709/6250 [07:16<05:05,  8.31it/s]\u001b[A\n",
      "training:  61%|██████    | 3792/6250 [07:27<04:56,  8.28it/s]\u001b[A\n",
      "training:  62%|██████▏   | 3875/6250 [07:37<04:49,  8.21it/s]\u001b[A\n",
      "training:  63%|██████▎   | 3956/6250 [07:47<04:42,  8.13it/s]\u001b[A\n",
      "training:  65%|██████▍   | 4040/6250 [07:57<04:29,  8.19it/s]\u001b[A\n",
      "training:  66%|██████▌   | 4125/6250 [08:07<04:17,  8.25it/s]\u001b[A\n",
      "training:  67%|██████▋   | 4209/6250 [08:18<04:07,  8.25it/s]\u001b[A\n",
      "training:  69%|██████▉   | 4300/6250 [08:28<03:49,  8.50it/s]\u001b[A\n",
      "training:  69%|██████▉   | 4300/6250 [08:39<03:49,  8.50it/s]\u001b[A\n",
      "training:  70%|███████   | 4389/6250 [08:39<03:45,  8.25it/s]\u001b[A\n",
      "training:  72%|███████▏  | 4477/6250 [08:49<03:31,  8.38it/s]\u001b[A\n",
      "training:  73%|███████▎  | 4565/6250 [09:00<03:25,  8.18it/s]\u001b[A\n",
      "training:  74%|███████▍  | 4646/6250 [09:11<03:16,  8.15it/s]\u001b[A\n",
      "training:  76%|███████▌  | 4732/6250 [09:21<03:03,  8.27it/s]\u001b[A\n",
      "training:  77%|███████▋  | 4818/6250 [09:32<02:55,  8.14it/s]\u001b[A\n",
      "training:  78%|███████▊  | 4897/6250 [09:42<02:48,  8.02it/s]\u001b[A\n",
      "training:  80%|███████▉  | 4979/6250 [09:52<02:37,  8.05it/s]\u001b[A\n",
      "training:  81%|████████  | 5061/6250 [10:03<02:30,  7.89it/s]\u001b[A\n",
      "training:  82%|████████▏ | 5137/6250 [10:14<02:26,  7.60it/s]\u001b[A\n",
      "training:  83%|████████▎ | 5213/6250 [10:24<02:16,  7.59it/s]\u001b[A\n",
      "training:  85%|████████▍ | 5289/6250 [10:34<02:07,  7.55it/s]\u001b[A\n",
      "training:  86%|████████▌ | 5369/6250 [10:44<01:54,  7.67it/s]\u001b[A\n",
      "training:  87%|████████▋ | 5450/6250 [10:54<01:42,  7.77it/s]\u001b[A\n",
      "training:  88%|████████▊ | 5531/6250 [11:04<01:32,  7.77it/s]\u001b[A\n",
      "training:  90%|████████▉ | 5609/6250 [11:15<01:24,  7.57it/s]\u001b[A\n",
      "training:  91%|█████████ | 5686/6250 [11:26<01:14,  7.59it/s]\u001b[A\n",
      "training:  92%|█████████▏| 5765/6250 [11:36<01:03,  7.66it/s]\u001b[A\n",
      "training:  94%|█████████▎| 5849/6250 [11:46<00:51,  7.85it/s]\u001b[A\n",
      "training:  95%|█████████▍| 5932/6250 [11:57<00:41,  7.72it/s]\u001b[A\n",
      "training:  96%|█████████▌| 6007/6250 [12:07<00:32,  7.54it/s]\u001b[A\n",
      "training:  97%|█████████▋| 6080/6250 [12:17<00:22,  7.45it/s]\u001b[A\n",
      "training:  98%|█████████▊| 6153/6250 [12:28<00:13,  7.34it/s]\u001b[A\n",
      "training: 100%|█████████▉| 6224/6250 [12:38<00:03,  7.22it/s]\u001b[A\n",
      "                                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Train Loss: 3.072 | Time: 762.3579835891724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  30%|███       | 6/20 [1:12:09<2:50:40, 731.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장한 파일 이름 : ./epoch_5.pth  저장하는 데 걸린 시간 : 2.646153688430786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training:   0%|          | 0/6250 [00:00<?, ?it/s]\u001b[A\n",
      "training:   1%|▏         | 83/6250 [00:10<12:26,  8.26it/s]\u001b[A\n",
      "training:   3%|▎         | 166/6250 [00:21<12:58,  7.81it/s]\u001b[A\n",
      "training:   4%|▍         | 243/6250 [00:31<12:55,  7.75it/s]\u001b[A\n",
      "training:   5%|▌         | 329/6250 [00:41<12:14,  8.06it/s]\u001b[A\n",
      "training:   7%|▋         | 415/6250 [00:52<12:08,  8.01it/s]\u001b[A\n",
      "training:   8%|▊         | 496/6250 [01:02<11:57,  8.02it/s]\u001b[A\n",
      "training:   9%|▉         | 577/6250 [01:13<12:09,  7.78it/s]\u001b[A\n",
      "training:  11%|█         | 662/6250 [01:23<11:40,  7.98it/s]\u001b[A\n",
      "training:  12%|█▏        | 747/6250 [01:33<11:20,  8.09it/s]\u001b[A\n",
      "training:  13%|█▎        | 832/6250 [01:43<11:02,  8.17it/s]\u001b[A\n",
      "training:  13%|█▎        | 832/6250 [01:54<11:02,  8.17it/s]\u001b[A\n",
      "training:  15%|█▍        | 909/6250 [01:54<11:23,  7.81it/s]\u001b[A\n",
      "training:  16%|█▌        | 990/6250 [02:04<11:06,  7.89it/s]\u001b[A\n",
      "training:  17%|█▋        | 1075/6250 [02:14<10:42,  8.06it/s]\u001b[A\n",
      "training:  19%|█▊        | 1160/6250 [02:26<10:51,  7.81it/s]\u001b[A\n",
      "training:  20%|█▉        | 1241/6250 [02:36<10:36,  7.87it/s]\u001b[A\n",
      "training:  21%|██        | 1323/6250 [02:46<10:18,  7.96it/s]\u001b[A\n",
      "training:  23%|██▎       | 1410/6250 [02:56<09:52,  8.17it/s]\u001b[A\n",
      "training:  24%|██▍       | 1497/6250 [03:07<09:48,  8.08it/s]\u001b[A\n",
      "training:  25%|██▌       | 1576/6250 [03:17<09:42,  8.03it/s]\u001b[A\n",
      "training:  26%|██▋       | 1655/6250 [03:27<09:38,  7.94it/s]\u001b[A\n",
      "training:  28%|██▊       | 1739/6250 [03:37<09:18,  8.08it/s]\u001b[A\n",
      "training:  29%|██▉       | 1823/6250 [03:48<09:10,  8.05it/s]\u001b[A\n",
      "training:  31%|███       | 1909/6250 [03:58<08:49,  8.20it/s]\u001b[A\n",
      "training:  32%|███▏      | 1995/6250 [04:09<08:47,  8.07it/s]\u001b[A\n",
      "training:  33%|███▎      | 2073/6250 [04:19<08:45,  7.95it/s]\u001b[A\n",
      "training:  34%|███▍      | 2150/6250 [04:32<09:21,  7.30it/s]\u001b[A\n",
      "training:  35%|███▌      | 2211/6250 [04:44<10:11,  6.60it/s]\u001b[A\n",
      "training:  37%|███▋      | 2287/6250 [04:54<09:37,  6.86it/s]\u001b[A\n",
      "training:  37%|███▋      | 2287/6250 [05:04<09:37,  6.86it/s]\u001b[A\n",
      "training:  37%|███▋      | 2341/6250 [05:04<10:13,  6.37it/s]\u001b[A\n",
      "training:  38%|███▊      | 2396/6250 [05:14<10:29,  6.12it/s]\u001b[A\n",
      "training:  39%|███▉      | 2451/6250 [05:25<11:01,  5.74it/s]\u001b[A\n",
      "training:  40%|████      | 2507/6250 [05:35<10:58,  5.69it/s]\u001b[A\n",
      "training:  41%|████      | 2565/6250 [05:46<10:46,  5.70it/s]\u001b[A\n",
      "training:  42%|████▏     | 2623/6250 [05:58<11:18,  5.35it/s]\u001b[A\n",
      "training:  43%|████▎     | 2677/6250 [06:08<11:07,  5.35it/s]\u001b[A\n",
      "training:  44%|████▎     | 2731/6250 [06:19<11:12,  5.24it/s]\u001b[A\n",
      "training:  44%|████▍     | 2781/6250 [06:29<11:17,  5.12it/s]\u001b[A\n",
      "training:  45%|████▌     | 2832/6250 [06:39<11:11,  5.09it/s]\u001b[A\n",
      "training:  46%|████▌     | 2887/6250 [06:50<10:47,  5.19it/s]\u001b[A\n",
      "training:  47%|████▋     | 2942/6250 [07:00<10:36,  5.20it/s]\u001b[A\n",
      "training:  48%|████▊     | 2995/6250 [07:11<10:38,  5.10it/s]\u001b[A\n",
      "training:  49%|████▉     | 3049/6250 [07:21<10:18,  5.18it/s]\u001b[A\n",
      "training:  50%|████▉     | 3103/6250 [07:31<10:02,  5.23it/s]\u001b[A\n",
      "training:  51%|█████     | 3158/6250 [07:41<09:44,  5.29it/s]\u001b[A\n",
      "training:  51%|█████▏    | 3213/6250 [07:53<09:49,  5.15it/s]\u001b[A\n",
      "training:  52%|█████▏    | 3264/6250 [08:03<09:43,  5.12it/s]\u001b[A\n",
      "training:  53%|█████▎    | 3315/6250 [08:14<09:49,  4.98it/s]\u001b[A\n",
      "training:  53%|█████▎    | 3315/6250 [08:24<09:49,  4.98it/s]\u001b[A\n",
      "training:  54%|█████▍    | 3362/6250 [08:24<09:56,  4.84it/s]\u001b[A\n",
      "training:  55%|█████▍    | 3408/6250 [08:34<09:55,  4.77it/s]\u001b[A\n",
      "training:  55%|█████▌    | 3455/6250 [08:44<09:50,  4.73it/s]\u001b[A\n",
      "training:  56%|█████▌    | 3503/6250 [08:54<09:38,  4.75it/s]\u001b[A\n",
      "training:  57%|█████▋    | 3551/6250 [09:04<09:29,  4.74it/s]\u001b[A\n",
      "training:  58%|█████▊    | 3599/6250 [09:15<09:23,  4.71it/s]\u001b[A\n",
      "training:  58%|█████▊    | 3646/6250 [09:25<09:15,  4.69it/s]\u001b[A\n",
      "training:  59%|█████▉    | 3697/6250 [09:35<08:52,  4.79it/s]\u001b[A\n",
      "training:  60%|█████▉    | 3748/6250 [09:46<08:41,  4.80it/s]\u001b[A\n",
      "training:  61%|██████    | 3797/6250 [09:56<08:39,  4.72it/s]\u001b[A\n",
      "training:  61%|██████▏   | 3843/6250 [10:08<08:54,  4.51it/s]\u001b[A\n",
      "training:  62%|██████▏   | 3890/6250 [10:18<08:39,  4.54it/s]\u001b[A\n",
      "training:  63%|██████▎   | 3939/6250 [10:28<08:19,  4.63it/s]\u001b[A\n",
      "training:  64%|██████▍   | 3988/6250 [10:38<08:03,  4.68it/s]\u001b[A\n",
      "training:  65%|██████▍   | 4037/6250 [10:49<07:55,  4.66it/s]\u001b[A\n",
      "training:  65%|██████▌   | 4084/6250 [10:59<07:46,  4.64it/s]\u001b[A\n",
      "training:  66%|██████▌   | 4130/6250 [11:10<07:45,  4.55it/s]\u001b[A\n",
      "training:  67%|██████▋   | 4182/6250 [11:20<07:17,  4.72it/s]\u001b[A\n",
      "training:  68%|██████▊   | 4234/6250 [11:31<07:09,  4.69it/s]\u001b[A\n",
      "training:  68%|██████▊   | 4281/6250 [11:42<07:14,  4.53it/s]\u001b[A\n",
      "training:  69%|██████▉   | 4327/6250 [11:52<07:03,  4.54it/s]\u001b[A\n",
      "training:  70%|██████▉   | 4374/6250 [12:03<06:50,  4.56it/s]\u001b[A\n",
      "training:  71%|███████   | 4421/6250 [12:13<06:42,  4.55it/s]\u001b[A\n",
      "training:  72%|███████▏  | 4470/6250 [12:23<06:24,  4.62it/s]\u001b[A\n",
      "training:  73%|███████▎  | 4544/6250 [12:33<05:14,  5.43it/s]\u001b[A\n",
      "training:  74%|███████▍  | 4618/6250 [12:43<04:31,  6.00it/s]\u001b[A\n",
      "training:  75%|███████▌  | 4700/6250 [12:53<03:53,  6.63it/s]\u001b[A\n",
      "training:  75%|███████▌  | 4700/6250 [13:04<03:53,  6.63it/s]\u001b[A\n",
      "training:  76%|███████▋  | 4775/6250 [13:04<03:38,  6.74it/s]\u001b[A\n",
      "training:  78%|███████▊  | 4845/6250 [13:14<03:26,  6.81it/s]\u001b[A\n",
      "training:  79%|███████▊  | 4921/6250 [13:24<03:08,  7.04it/s]\u001b[A\n",
      "training:  80%|███████▉  | 4997/6250 [13:34<02:54,  7.19it/s]\u001b[A\n",
      "training:  81%|████████  | 5073/6250 [13:45<02:43,  7.22it/s]\u001b[A\n",
      "training:  82%|████████▏ | 5146/6250 [13:55<02:32,  7.22it/s]\u001b[A\n",
      "training:  84%|████████▍ | 5235/6250 [14:05<02:11,  7.71it/s]\u001b[A\n",
      "training:  85%|████████▌ | 5324/6250 [14:17<02:01,  7.63it/s]\u001b[A\n",
      "training:  87%|████████▋ | 5407/6250 [14:27<01:47,  7.81it/s]\u001b[A\n",
      "training:  88%|████████▊ | 5493/6250 [14:37<01:34,  8.03it/s]\u001b[A\n",
      "training:  89%|████████▉ | 5589/6250 [14:47<01:18,  8.47it/s]\u001b[A\n",
      "training:  91%|█████████ | 5685/6250 [14:57<01:05,  8.64it/s]\u001b[A\n",
      "training:  92%|█████████▏| 5776/6250 [15:08<00:54,  8.68it/s]\u001b[A\n",
      "training:  94%|█████████▍| 5869/6250 [15:18<00:43,  8.84it/s]\u001b[A\n",
      "training:  95%|█████████▌| 5965/6250 [15:28<00:31,  9.03it/s]\u001b[A\n",
      "training:  97%|█████████▋| 6061/6250 [15:39<00:20,  9.03it/s]\u001b[A\n",
      "training:  98%|█████████▊| 6154/6250 [15:49<00:10,  9.09it/s]\u001b[A\n",
      "training: 100%|█████████▉| 6247/6250 [15:59<00:00,  9.04it/s]\u001b[A\n",
      "                                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Train Loss: 3.143 | Time: 960.0327560901642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  35%|███▌      | 7/20 [1:28:11<2:54:51, 807.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장한 파일 이름 : ./epoch_6.pth  저장하는 데 걸린 시간 : 2.6402688026428223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training:   0%|          | 0/6250 [00:00<?, ?it/s]\u001b[A\n",
      "training:   2%|▏         | 98/6250 [00:10<10:29,  9.78it/s]\u001b[A\n",
      "training:   3%|▎         | 196/6250 [00:21<11:14,  8.97it/s]\u001b[A\n",
      "training:   5%|▍         | 283/6250 [00:31<11:15,  8.84it/s]\u001b[A\n",
      "training:   6%|▌         | 384/6250 [00:41<10:30,  9.30it/s]\u001b[A\n",
      "training:   8%|▊         | 488/6250 [00:51<09:56,  9.66it/s]\u001b[A\n",
      "training:   9%|▉         | 592/6250 [01:01<09:31,  9.90it/s]\u001b[A\n",
      "training:  11%|█         | 696/6250 [01:11<09:12, 10.05it/s]\u001b[A\n",
      "training:  11%|█         | 696/6250 [01:21<09:12, 10.05it/s]\u001b[A\n",
      "training:  13%|█▎        | 801/6250 [01:21<08:56, 10.16it/s]\u001b[A\n",
      "training:  14%|█▍        | 906/6250 [01:32<08:46, 10.15it/s]\u001b[A\n",
      "training:  16%|█▌        | 1008/6250 [01:44<09:06,  9.60it/s]\u001b[A\n",
      "training:  18%|█▊        | 1107/6250 [01:54<08:51,  9.68it/s]\u001b[A\n",
      "training:  19%|█▉        | 1206/6250 [02:05<09:02,  9.29it/s]\u001b[A\n",
      "training:  21%|██        | 1291/6250 [02:16<09:10,  9.00it/s]\u001b[A\n",
      "training:  22%|██▏       | 1380/6250 [02:26<09:03,  8.97it/s]\u001b[A\n",
      "training:  24%|██▎       | 1470/6250 [02:36<08:54,  8.94it/s]\u001b[A\n",
      "training:  25%|██▍       | 1559/6250 [02:46<08:45,  8.92it/s]\u001b[A\n",
      "training:  26%|██▋       | 1648/6250 [02:56<08:38,  8.87it/s]\u001b[A\n",
      "training:  28%|██▊       | 1742/6250 [03:06<08:20,  9.00it/s]\u001b[A\n",
      "training:  29%|██▉       | 1836/6250 [03:18<08:27,  8.70it/s]\u001b[A\n",
      "training:  31%|███       | 1923/6250 [03:28<08:18,  8.69it/s]\u001b[A\n",
      "training:  32%|███▏      | 2012/6250 [03:38<08:04,  8.75it/s]\u001b[A\n",
      "training:  34%|███▎      | 2101/6250 [03:48<07:55,  8.72it/s]\u001b[A\n",
      "training:  35%|███▌      | 2188/6250 [03:58<07:46,  8.70it/s]\u001b[A\n",
      "training:  36%|███▋      | 2276/6250 [04:08<07:35,  8.73it/s]\u001b[A\n",
      "training:  38%|███▊      | 2376/6250 [04:18<07:06,  9.08it/s]\u001b[A\n",
      "training:  40%|███▉      | 2476/6250 [04:29<06:54,  9.11it/s]\u001b[A\n",
      "training:  41%|████      | 2568/6250 [04:40<06:47,  9.03it/s]\u001b[A\n",
      "training:  43%|████▎     | 2662/6250 [04:50<06:33,  9.11it/s]\u001b[A\n",
      "training:  44%|████▍     | 2762/6250 [05:00<06:12,  9.37it/s]\u001b[A\n",
      "training:  46%|████▌     | 2862/6250 [05:11<06:05,  9.27it/s]\u001b[A\n",
      "training:  47%|████▋     | 2955/6250 [05:21<05:55,  9.26it/s]\u001b[A\n",
      "training:  47%|████▋     | 2955/6250 [05:31<05:55,  9.26it/s]\u001b[A\n",
      "training:  49%|████▉     | 3047/6250 [05:31<05:53,  9.06it/s]\u001b[A\n",
      "training:  50%|█████     | 3136/6250 [05:41<05:45,  9.01it/s]\u001b[A\n",
      "training:  52%|█████▏    | 3225/6250 [05:52<05:38,  8.93it/s]\u001b[A\n",
      "training:  53%|█████▎    | 3318/6250 [06:02<05:25,  9.02it/s]\u001b[A\n",
      "training:  55%|█████▍    | 3412/6250 [06:12<05:11,  9.12it/s]\u001b[A\n",
      "training:  56%|█████▌    | 3506/6250 [06:23<05:05,  8.99it/s]\u001b[A\n",
      "training:  58%|█████▊    | 3594/6250 [06:33<05:00,  8.84it/s]\u001b[A\n",
      "training:  59%|█████▉    | 3680/6250 [06:43<04:53,  8.75it/s]\u001b[A\n",
      "training:  60%|██████    | 3767/6250 [06:53<04:44,  8.72it/s]\u001b[A\n",
      "training:  62%|██████▏   | 3854/6250 [07:03<04:35,  8.71it/s]\u001b[A\n",
      "training:  63%|██████▎   | 3942/6250 [07:13<04:24,  8.72it/s]\u001b[A\n",
      "training:  65%|██████▍   | 4032/6250 [07:23<04:12,  8.79it/s]\u001b[A\n",
      "training:  66%|██████▌   | 4122/6250 [07:33<04:00,  8.85it/s]\u001b[A\n",
      "training:  67%|██████▋   | 4217/6250 [07:43<03:45,  9.02it/s]\u001b[A\n",
      "training:  69%|██████▉   | 4312/6250 [07:54<03:33,  9.08it/s]\u001b[A\n",
      "training:  70%|███████   | 4405/6250 [08:04<03:25,  8.96it/s]\u001b[A\n",
      "training:  72%|███████▏  | 4495/6250 [08:14<03:15,  8.96it/s]\u001b[A\n",
      "training:  73%|███████▎  | 4585/6250 [08:25<03:07,  8.86it/s]\u001b[A\n",
      "training:  75%|███████▍  | 4672/6250 [08:35<03:01,  8.69it/s]\u001b[A\n",
      "training:  76%|███████▌  | 4757/6250 [08:45<02:53,  8.62it/s]\u001b[A\n",
      "training:  78%|███████▊  | 4846/6250 [08:55<02:41,  8.69it/s]\u001b[A\n",
      "training:  79%|███████▉  | 4944/6250 [09:05<02:25,  9.00it/s]\u001b[A\n",
      "training:  81%|████████  | 5042/6250 [09:18<02:19,  8.63it/s]\u001b[A\n",
      "training:  82%|████████▏ | 5128/6250 [09:28<02:10,  8.62it/s]\u001b[A\n",
      "training:  83%|████████▎ | 5217/6250 [09:38<01:58,  8.69it/s]\u001b[A\n",
      "training:  85%|████████▍ | 5306/6250 [09:48<01:49,  8.58it/s]\u001b[A\n",
      "training:  86%|████████▌ | 5390/6250 [09:59<01:41,  8.49it/s]\u001b[A\n",
      "training:  88%|████████▊ | 5473/6250 [10:09<01:32,  8.39it/s]\u001b[A\n",
      "training:  89%|████████▉ | 5555/6250 [10:19<01:23,  8.30it/s]\u001b[A\n",
      "training:  90%|█████████ | 5642/6250 [10:29<01:12,  8.40it/s]\u001b[A\n",
      "training:  92%|█████████▏| 5731/6250 [10:39<01:00,  8.55it/s]\u001b[A\n",
      "training:  93%|█████████▎| 5827/6250 [10:49<00:47,  8.84it/s]\u001b[A\n",
      "training:  95%|█████████▍| 5923/6250 [11:00<00:37,  8.73it/s]\u001b[A\n",
      "training:  96%|█████████▌| 6009/6250 [11:11<00:28,  8.60it/s]\u001b[A\n",
      "training:  98%|█████████▊| 6096/6250 [11:21<00:17,  8.60it/s]\u001b[A\n",
      "training:  99%|█████████▉| 6186/6250 [11:31<00:07,  8.71it/s]\u001b[A\n",
      "                                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Train Loss: 1.921 | Time: 698.7559933662415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  40%|████      | 8/20 [1:39:53<2:34:40, 773.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장한 파일 이름 : ./epoch_7.pth  저장하는 데 걸린 시간 : 2.6485018730163574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training:   0%|          | 0/6250 [00:00<?, ?it/s]\u001b[A\n",
      "training:   1%|▏         | 88/6250 [00:10<11:43,  8.76it/s]\u001b[A\n",
      "training:   3%|▎         | 176/6250 [00:20<11:41,  8.66it/s]\u001b[A\n",
      "training:   3%|▎         | 176/6250 [00:30<11:41,  8.66it/s]\u001b[A\n",
      "training:   4%|▍         | 255/6250 [00:30<12:09,  8.22it/s]\u001b[A\n",
      "training:   5%|▌         | 338/6250 [00:40<11:59,  8.22it/s]\u001b[A\n",
      "training:   7%|▋         | 428/6250 [00:50<11:26,  8.49it/s]\u001b[A\n",
      "training:   8%|▊         | 518/6250 [01:01<11:12,  8.52it/s]\u001b[A\n",
      "training:  10%|▉         | 604/6250 [01:11<11:13,  8.38it/s]\u001b[A\n",
      "training:  11%|█         | 686/6250 [01:21<11:10,  8.30it/s]\u001b[A\n",
      "training:  12%|█▏        | 768/6250 [01:32<11:13,  8.14it/s]\u001b[A\n",
      "training:  14%|█▎        | 851/6250 [01:42<11:00,  8.18it/s]\u001b[A\n",
      "training:  15%|█▍        | 934/6250 [01:53<11:06,  7.97it/s]\u001b[A\n",
      "training:  16%|█▋        | 1022/6250 [02:03<10:38,  8.18it/s]\u001b[A\n",
      "training:  18%|█▊        | 1109/6250 [02:13<10:18,  8.31it/s]\u001b[A\n",
      "training:  19%|█▉        | 1195/6250 [02:23<10:03,  8.37it/s]\u001b[A\n",
      "training:  20%|██        | 1281/6250 [02:33<09:50,  8.42it/s]\u001b[A\n",
      "training:  22%|██▏       | 1367/6250 [02:44<09:39,  8.43it/s]\u001b[A\n",
      "training:  23%|██▎       | 1464/6250 [02:54<09:04,  8.79it/s]\u001b[A\n",
      "training:  25%|██▍       | 1561/6250 [03:05<08:53,  8.80it/s]\u001b[A\n",
      "training:  26%|██▋       | 1650/6250 [03:16<08:59,  8.52it/s]\u001b[A\n",
      "training:  28%|██▊       | 1730/6250 [03:26<09:00,  8.37it/s]\u001b[A\n",
      "training:  29%|██▉       | 1817/6250 [03:36<08:44,  8.45it/s]\u001b[A\n",
      "training:  30%|███       | 1904/6250 [03:46<08:31,  8.49it/s]\u001b[A\n",
      "training:  32%|███▏      | 1992/6250 [03:56<08:17,  8.57it/s]\u001b[A\n",
      "training:  33%|███▎      | 2088/6250 [04:06<07:49,  8.86it/s]\u001b[A\n",
      "training:  35%|███▍      | 2184/6250 [04:18<07:53,  8.60it/s]\u001b[A\n",
      "training:  36%|███▋      | 2266/6250 [04:28<07:50,  8.47it/s]\u001b[A\n",
      "training:  38%|███▊      | 2348/6250 [04:38<07:49,  8.31it/s]\u001b[A\n",
      "training:  39%|███▉      | 2428/6250 [04:49<07:46,  8.20it/s]\u001b[A\n",
      "training:  40%|████      | 2511/6250 [04:59<07:34,  8.22it/s]\u001b[A\n",
      "training:  42%|████▏     | 2598/6250 [05:09<07:16,  8.36it/s]\u001b[A\n",
      "training:  43%|████▎     | 2686/6250 [05:19<07:00,  8.47it/s]\u001b[A\n",
      "training:  44%|████▍     | 2774/6250 [05:29<06:46,  8.56it/s]\u001b[A\n",
      "training:  46%|████▌     | 2862/6250 [05:40<06:42,  8.43it/s]\u001b[A\n",
      "training:  47%|████▋     | 2950/6250 [05:50<06:27,  8.52it/s]\u001b[A\n",
      "training:  49%|████▊     | 3038/6250 [06:00<06:17,  8.50it/s]\u001b[A\n",
      "training:  49%|████▊     | 3038/6250 [06:10<06:17,  8.50it/s]\u001b[A\n",
      "training:  50%|████▉     | 3123/6250 [06:10<06:09,  8.47it/s]\u001b[A\n",
      "training:  51%|█████▏    | 3210/6250 [06:20<05:56,  8.53it/s]\u001b[A\n",
      "training:  53%|█████▎    | 3301/6250 [06:30<05:40,  8.67it/s]\u001b[A\n",
      "training:  54%|█████▍    | 3392/6250 [06:41<05:29,  8.68it/s]\u001b[A\n",
      "training:  56%|█████▌    | 3479/6250 [06:52<05:27,  8.46it/s]\u001b[A\n",
      "training:  57%|█████▋    | 3573/6250 [07:02<05:07,  8.70it/s]\u001b[A\n",
      "training:  59%|█████▊    | 3667/6250 [07:15<05:16,  8.17it/s]\u001b[A\n",
      "training:  60%|█████▉    | 3747/6250 [07:25<05:08,  8.11it/s]\u001b[A\n",
      "training:  61%|██████▏   | 3836/6250 [07:35<04:50,  8.32it/s]\u001b[A\n",
      "training:  63%|██████▎   | 3925/6250 [07:46<04:42,  8.24it/s]\u001b[A\n",
      "training:  64%|██████▍   | 4006/6250 [07:56<04:33,  8.19it/s]\u001b[A\n",
      "training:  65%|██████▌   | 4089/6250 [08:06<04:23,  8.21it/s]\u001b[A\n",
      "training:  67%|██████▋   | 4172/6250 [08:17<04:16,  8.11it/s]\u001b[A\n",
      "training:  68%|██████▊   | 4258/6250 [08:27<04:01,  8.25it/s]\u001b[A\n",
      "training:  70%|██████▉   | 4345/6250 [08:37<03:47,  8.36it/s]\u001b[A\n",
      "training:  71%|███████   | 4432/6250 [08:47<03:38,  8.31it/s]\u001b[A\n",
      "training:  72%|███████▏  | 4514/6250 [08:58<03:33,  8.12it/s]\u001b[A\n",
      "training:  74%|███████▎  | 4597/6250 [09:08<03:22,  8.17it/s]\u001b[A\n",
      "training:  75%|███████▍  | 4682/6250 [09:18<03:10,  8.24it/s]\u001b[A\n",
      "training:  76%|███████▋  | 4767/6250 [09:29<03:00,  8.20it/s]\u001b[A\n",
      "training:  78%|███████▊  | 4849/6250 [09:39<02:52,  8.14it/s]\u001b[A\n",
      "training:  79%|███████▉  | 4930/6250 [09:49<02:45,  7.99it/s]\u001b[A\n",
      "training:  80%|████████  | 5017/6250 [09:59<02:30,  8.18it/s]\u001b[A\n",
      "training:  82%|████████▏ | 5107/6250 [10:10<02:16,  8.40it/s]\u001b[A\n",
      "training:  82%|████████▏ | 5107/6250 [10:20<02:16,  8.40it/s]\u001b[A\n",
      "training:  83%|████████▎ | 5197/6250 [10:20<02:04,  8.43it/s]\u001b[A\n",
      "training:  85%|████████▍ | 5282/6250 [10:31<01:55,  8.36it/s]\u001b[A\n",
      "training:  86%|████████▌ | 5365/6250 [10:42<01:50,  7.99it/s]\u001b[A\n",
      "training:  87%|████████▋ | 5464/6250 [10:52<01:32,  8.51it/s]\u001b[A\n",
      "training:  89%|████████▉ | 5563/6250 [11:05<01:24,  8.17it/s]\u001b[A\n",
      "training:  90%|█████████ | 5643/6250 [11:15<01:15,  8.09it/s]\u001b[A\n",
      "training:  92%|█████████▏| 5725/6250 [11:25<01:04,  8.12it/s]\u001b[A\n",
      "training:  93%|█████████▎| 5807/6250 [11:36<00:55,  7.94it/s]\u001b[A\n",
      "training:  94%|█████████▍| 5884/6250 [11:46<00:46,  7.84it/s]\u001b[A\n",
      "training:  95%|█████████▌| 5965/6250 [11:56<00:36,  7.91it/s]\u001b[A\n",
      "training:  97%|█████████▋| 6047/6250 [12:06<00:25,  7.99it/s]\u001b[A\n",
      "training:  98%|█████████▊| 6130/6250 [12:17<00:14,  8.07it/s]\u001b[A\n",
      "training:  99%|█████████▉| 6213/6250 [12:27<00:04,  8.06it/s]\u001b[A\n",
      "                                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Train Loss: 1.782 | Time: 752.0548079013824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  45%|████▌     | 9/20 [1:52:27<2:20:43, 767.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장한 파일 이름 : ./epoch_8.pth  저장하는 데 걸린 시간 : 2.662026882171631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training:   0%|          | 0/6250 [00:00<?, ?it/s]\u001b[A\n",
      "training:   1%|▏         | 80/6250 [00:10<12:55,  7.96it/s]\u001b[A\n",
      "training:   3%|▎         | 162/6250 [00:20<12:34,  8.07it/s]\u001b[A\n",
      "training:   4%|▍         | 250/6250 [00:30<11:58,  8.36it/s]\u001b[A\n",
      "training:   5%|▌         | 337/6250 [00:40<11:45,  8.38it/s]\u001b[A\n",
      "training:   7%|▋         | 422/6250 [00:50<11:36,  8.37it/s]\u001b[A\n",
      "training:   8%|▊         | 506/6250 [01:01<11:48,  8.11it/s]\u001b[A\n",
      "training:   9%|▉         | 583/6250 [01:12<12:15,  7.71it/s]\u001b[A\n",
      "training:  11%|█         | 660/6250 [01:22<12:05,  7.70it/s]\u001b[A\n",
      "training:  12%|█▏        | 737/6250 [01:33<12:13,  7.51it/s]\u001b[A\n",
      "training:  13%|█▎        | 809/6250 [01:43<12:14,  7.41it/s]\u001b[A\n",
      "training:  14%|█▍        | 882/6250 [01:53<12:07,  7.37it/s]\u001b[A\n",
      "training:  15%|█▌        | 955/6250 [02:04<12:19,  7.16it/s]\u001b[A\n",
      "training:  16%|█▋        | 1023/6250 [02:14<12:22,  7.04it/s]\u001b[A\n",
      "training:  17%|█▋        | 1091/6250 [02:24<12:22,  6.95it/s]\u001b[A\n",
      "training:  19%|█▊        | 1160/6250 [02:34<12:14,  6.93it/s]\u001b[A\n",
      "training:  20%|█▉        | 1231/6250 [02:44<12:00,  6.97it/s]\u001b[A\n",
      "training:  21%|██        | 1320/6250 [02:54<10:53,  7.54it/s]\u001b[A\n",
      "training:  23%|██▎       | 1411/6250 [03:04<10:05,  8.00it/s]\u001b[A\n",
      "training:  24%|██▍       | 1509/6250 [03:14<09:15,  8.53it/s]\u001b[A\n",
      "training:  24%|██▍       | 1509/6250 [03:25<09:15,  8.53it/s]\u001b[A\n",
      "training:  26%|██▌       | 1605/6250 [03:26<09:02,  8.57it/s]\u001b[A\n",
      "training:  27%|██▋       | 1692/6250 [03:36<09:04,  8.38it/s]\u001b[A\n",
      "training:  28%|██▊       | 1774/6250 [03:47<08:59,  8.29it/s]\u001b[A\n",
      "training:  30%|██▉       | 1856/6250 [03:57<08:52,  8.24it/s]\u001b[A\n",
      "training:  31%|███       | 1938/6250 [04:08<09:03,  7.94it/s]\u001b[A\n",
      "training:  32%|███▏      | 2018/6250 [04:18<08:52,  7.94it/s]\u001b[A\n",
      "training:  34%|███▎      | 2098/6250 [04:29<08:58,  7.72it/s]\u001b[A\n",
      "training:  35%|███▍      | 2171/6250 [04:39<09:02,  7.52it/s]\u001b[A\n",
      "training:  36%|███▌      | 2251/6250 [04:49<08:42,  7.65it/s]\u001b[A\n",
      "training:  37%|███▋      | 2341/6250 [05:00<08:07,  8.02it/s]\u001b[A\n",
      "training:  39%|███▉      | 2430/6250 [05:10<07:50,  8.12it/s]\u001b[A\n",
      "training:  40%|████      | 2514/6250 [05:21<07:51,  7.92it/s]\u001b[A\n",
      "training:  42%|████▏     | 2605/6250 [05:32<07:21,  8.25it/s]\u001b[A\n",
      "training:  43%|████▎     | 2696/6250 [05:42<07:01,  8.44it/s]\u001b[A\n",
      "training:  45%|████▍     | 2785/6250 [05:52<06:44,  8.57it/s]\u001b[A\n",
      "training:  46%|████▌     | 2888/6250 [06:02<06:10,  9.07it/s]\u001b[A\n",
      "training:  48%|████▊     | 2991/6250 [06:13<05:55,  9.16it/s]\u001b[A\n",
      "training:  50%|████▉     | 3095/6250 [06:23<05:31,  9.50it/s]\u001b[A\n",
      "training:  51%|█████     | 3199/6250 [06:33<05:14,  9.71it/s]\u001b[A\n",
      "training:  53%|█████▎    | 3301/6250 [06:45<05:14,  9.37it/s]\u001b[A\n",
      "training:  54%|█████▍    | 3396/6250 [06:55<05:03,  9.40it/s]\u001b[A\n",
      "training:  56%|█████▌    | 3491/6250 [07:05<04:54,  9.37it/s]\u001b[A\n",
      "training:  56%|█████▌    | 3491/6250 [07:15<04:54,  9.37it/s]\u001b[A\n",
      "training:  57%|█████▋    | 3585/6250 [07:15<04:47,  9.28it/s]\u001b[A\n",
      "training:  59%|█████▉    | 3676/6250 [07:26<04:41,  9.13it/s]\u001b[A\n",
      "training:  60%|██████    | 3769/6250 [07:36<04:30,  9.17it/s]\u001b[A\n",
      "training:  62%|██████▏   | 3862/6250 [07:46<04:20,  9.17it/s]\u001b[A\n",
      "training:  63%|██████▎   | 3959/6250 [07:56<04:05,  9.32it/s]\u001b[A\n",
      "training:  65%|██████▍   | 4061/6250 [08:06<03:48,  9.57it/s]\u001b[A\n",
      "training:  67%|██████▋   | 4163/6250 [08:17<03:42,  9.38it/s]\u001b[A\n",
      "training:  68%|██████▊   | 4253/6250 [08:28<03:36,  9.23it/s]\u001b[A\n",
      "training:  70%|██████▉   | 4346/6250 [08:38<03:26,  9.24it/s]\u001b[A\n",
      "training:  71%|███████   | 4439/6250 [08:48<03:17,  9.17it/s]\u001b[A\n",
      "training:  72%|███████▏  | 4529/6250 [08:58<03:09,  9.07it/s]\u001b[A\n",
      "training:  74%|███████▍  | 4620/6250 [09:08<02:59,  9.07it/s]\u001b[A\n",
      "training:  75%|███████▌  | 4711/6250 [09:18<02:49,  9.07it/s]\u001b[A\n",
      "training:  77%|███████▋  | 4809/6250 [09:28<02:35,  9.26it/s]\u001b[A\n",
      "training:  79%|███████▊  | 4907/6250 [09:39<02:25,  9.20it/s]\u001b[A\n",
      "training:  80%|███████▉  | 4998/6250 [09:49<02:17,  9.08it/s]\u001b[A\n",
      "training:  81%|████████▏ | 5086/6250 [10:00<02:10,  8.91it/s]\u001b[A\n",
      "training:  83%|████████▎ | 5181/6250 [10:10<01:57,  9.06it/s]\u001b[A\n",
      "training:  84%|████████▍ | 5276/6250 [10:20<01:46,  9.14it/s]\u001b[A\n",
      "training:  86%|████████▌ | 5370/6250 [10:31<01:37,  8.98it/s]\u001b[A\n",
      "training:  87%|████████▋ | 5458/6250 [10:41<01:28,  8.91it/s]\u001b[A\n",
      "training:  89%|████████▊ | 5546/6250 [10:52<01:21,  8.62it/s]\u001b[A\n",
      "training:  90%|█████████ | 5626/6250 [11:02<01:14,  8.35it/s]\u001b[A\n",
      "training:  91%|█████████▏| 5704/6250 [11:13<01:07,  8.13it/s]\u001b[A\n",
      "training:  93%|█████████▎| 5790/6250 [11:23<00:55,  8.23it/s]\u001b[A\n",
      "training:  94%|█████████▍| 5875/6250 [11:34<00:46,  8.04it/s]\u001b[A\n",
      "training:  95%|█████████▌| 5952/6250 [11:44<00:37,  7.93it/s]\u001b[A\n",
      "training:  97%|█████████▋| 6034/6250 [11:54<00:27,  7.99it/s]\u001b[A\n",
      "training:  98%|█████████▊| 6116/6250 [12:05<00:17,  7.88it/s]\u001b[A\n",
      "training:  99%|█████████▉| 6194/6250 [12:15<00:07,  7.85it/s]\u001b[A\n",
      "                                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Train Loss: 1.353 | Time: 741.8992011547089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  50%|█████     | 10/20 [2:04:52<2:06:44, 760.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장한 파일 이름 : ./epoch_9.pth  저장하는 데 걸린 시간 : 2.6702630519866943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training:   0%|          | 0/6250 [00:00<?, ?it/s]\u001b[A\n",
      "training:   1%|▏         | 82/6250 [00:10<12:41,  8.10it/s]\u001b[A\n",
      "training:   3%|▎         | 164/6250 [00:20<12:29,  8.12it/s]\u001b[A\n",
      "training:   4%|▍         | 246/6250 [00:30<12:23,  8.08it/s]\u001b[A\n",
      "training:   5%|▌         | 330/6250 [00:40<12:03,  8.19it/s]\u001b[A\n",
      "training:   7%|▋         | 414/6250 [00:50<11:57,  8.14it/s]\u001b[A\n",
      "training:   8%|▊         | 495/6250 [01:01<11:52,  8.08it/s]\u001b[A\n",
      "training:   8%|▊         | 495/6250 [01:11<11:52,  8.08it/s]\u001b[A\n",
      "training:   9%|▉         | 572/6250 [01:11<12:00,  7.88it/s]\u001b[A\n",
      "training:  11%|█         | 658/6250 [01:21<11:32,  8.08it/s]\u001b[A\n",
      "training:  12%|█▏        | 751/6250 [01:31<10:52,  8.42it/s]\u001b[A\n",
      "training:  13%|█▎        | 843/6250 [01:42<10:33,  8.53it/s]\u001b[A\n",
      "training:  15%|█▍        | 931/6250 [01:53<10:42,  8.28it/s]\u001b[A\n",
      "training:  16%|█▌        | 1013/6250 [02:03<10:34,  8.25it/s]\u001b[A\n",
      "training:  18%|█▊        | 1098/6250 [02:13<10:20,  8.30it/s]\u001b[A\n",
      "training:  19%|█▉        | 1183/6250 [02:23<10:11,  8.28it/s]\u001b[A\n",
      "training:  20%|██        | 1276/6250 [02:34<09:41,  8.55it/s]\u001b[A\n",
      "training:  22%|██▏       | 1368/6250 [02:44<09:31,  8.55it/s]\u001b[A\n",
      "training:  23%|██▎       | 1454/6250 [02:55<09:26,  8.46it/s]\u001b[A\n",
      "training:  25%|██▍       | 1537/6250 [03:05<09:23,  8.36it/s]\u001b[A\n",
      "training:  26%|██▌       | 1619/6250 [03:15<09:18,  8.30it/s]\u001b[A\n",
      "training:  27%|██▋       | 1701/6250 [03:25<09:11,  8.25it/s]\u001b[A\n",
      "training:  29%|██▊       | 1789/6250 [03:35<08:50,  8.41it/s]\u001b[A\n",
      "training:  30%|███       | 1877/6250 [03:47<08:59,  8.10it/s]\u001b[A\n",
      "training:  31%|███▏      | 1955/6250 [03:57<08:57,  8.00it/s]\u001b[A\n",
      "training:  33%|███▎      | 2033/6250 [04:07<08:51,  7.93it/s]\u001b[A\n",
      "training:  34%|███▍      | 2125/6250 [04:17<08:18,  8.27it/s]\u001b[A\n",
      "training:  35%|███▌      | 2216/6250 [04:28<08:12,  8.20it/s]\u001b[A\n",
      "training:  37%|███▋      | 2297/6250 [04:39<08:12,  8.03it/s]\u001b[A\n",
      "training:  38%|███▊      | 2377/6250 [04:49<08:03,  8.01it/s]\u001b[A\n",
      "training:  39%|███▉      | 2462/6250 [04:59<07:44,  8.15it/s]\u001b[A\n",
      "training:  41%|████      | 2555/6250 [05:09<07:15,  8.48it/s]\u001b[A\n",
      "training:  41%|████      | 2555/6250 [05:21<07:15,  8.48it/s]\u001b[A\n",
      "training:  42%|████▏     | 2635/6250 [05:21<07:37,  7.91it/s]\u001b[A\n",
      "training:  43%|████▎     | 2712/6250 [05:31<07:31,  7.83it/s]\u001b[A\n",
      "training:  45%|████▍     | 2800/6250 [05:41<07:06,  8.09it/s]\u001b[A\n",
      "training:  46%|████▌     | 2888/6250 [05:52<06:58,  8.02it/s]\u001b[A\n",
      "training:  47%|████▋     | 2968/6250 [06:02<06:49,  8.01it/s]\u001b[A\n",
      "training:  49%|████▉     | 3048/6250 [06:13<06:47,  7.86it/s]\u001b[A\n",
      "training:  50%|█████     | 3126/6250 [06:23<06:38,  7.84it/s]\u001b[A\n",
      "training:  51%|█████▏    | 3206/6250 [06:33<06:26,  7.88it/s]\u001b[A\n",
      "training:  53%|█████▎    | 3292/6250 [06:43<06:06,  8.08it/s]\u001b[A\n",
      "training:  54%|█████▍    | 3380/6250 [06:53<05:46,  8.28it/s]\u001b[A\n",
      "training:  56%|█████▌    | 3470/6250 [07:03<05:28,  8.47it/s]\u001b[A\n",
      "training:  57%|█████▋    | 3560/6250 [07:14<05:14,  8.55it/s]\u001b[A\n",
      "training:  58%|█████▊    | 3648/6250 [07:24<05:05,  8.52it/s]\u001b[A\n",
      "training:  60%|█████▉    | 3746/6250 [07:34<04:41,  8.89it/s]\u001b[A\n",
      "training:  62%|██████▏   | 3844/6250 [07:44<04:26,  9.03it/s]\u001b[A\n",
      "training:  63%|██████▎   | 3939/6250 [07:54<04:12,  9.16it/s]\u001b[A\n",
      "training:  65%|██████▍   | 4034/6250 [08:05<04:03,  9.09it/s]\u001b[A\n",
      "training:  66%|██████▌   | 4124/6250 [08:15<03:57,  8.97it/s]\u001b[A\n",
      "training:  67%|██████▋   | 4211/6250 [08:25<03:49,  8.89it/s]\u001b[A\n",
      "training:  69%|██████▉   | 4304/6250 [08:36<03:36,  9.00it/s]\u001b[A\n",
      "training:  70%|███████   | 4397/6250 [08:46<03:26,  8.96it/s]\u001b[A\n",
      "training:  72%|███████▏  | 4487/6250 [08:56<03:16,  8.97it/s]\u001b[A\n",
      "training:  73%|███████▎  | 4577/6250 [09:06<03:07,  8.91it/s]\u001b[A\n",
      "training:  75%|███████▍  | 4669/6250 [09:16<02:56,  8.97it/s]\u001b[A\n",
      "training:  76%|███████▋  | 4771/6250 [09:26<02:38,  9.32it/s]\u001b[A\n",
      "training:  78%|███████▊  | 4873/6250 [09:38<02:28,  9.28it/s]\u001b[A\n",
      "training:  79%|███████▉  | 4965/6250 [09:48<02:19,  9.18it/s]\u001b[A\n",
      "training:  81%|████████  | 5058/6250 [09:58<02:09,  9.21it/s]\u001b[A\n",
      "training:  82%|████████▏ | 5151/6250 [10:08<01:59,  9.18it/s]\u001b[A\n",
      "training:  84%|████████▍ | 5248/6250 [10:18<01:47,  9.32it/s]\u001b[A\n",
      "training:  86%|████████▌ | 5345/6250 [10:28<01:36,  9.37it/s]\u001b[A\n",
      "training:  87%|████████▋ | 5440/6250 [10:39<01:28,  9.19it/s]\u001b[A\n",
      "training:  89%|████████▊ | 5542/6250 [10:49<01:14,  9.47it/s]\u001b[A\n",
      "training:  90%|█████████ | 5644/6250 [10:59<01:02,  9.63it/s]\u001b[A\n",
      "training:  92%|█████████▏| 5745/6250 [11:10<00:53,  9.51it/s]\u001b[A\n",
      "training:  93%|█████████▎| 5838/6250 [11:21<00:44,  9.29it/s]\u001b[A\n",
      "training:  95%|█████████▍| 5929/6250 [11:31<00:34,  9.23it/s]\u001b[A\n",
      "training:  95%|█████████▍| 5929/6250 [11:41<00:34,  9.23it/s]\u001b[A\n",
      "training:  96%|█████████▋| 6021/6250 [11:41<00:24,  9.22it/s]\u001b[A\n",
      "training:  98%|█████████▊| 6113/6250 [11:51<00:14,  9.14it/s]\u001b[A\n",
      "training:  99%|█████████▉| 6207/6250 [12:01<00:04,  9.19it/s]\u001b[A\n",
      "                                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Train Loss: 1.263 | Time: 726.3470296859741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  55%|█████▌    | 11/20 [2:17:01<1:52:37, 750.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장한 파일 이름 : ./epoch_10.pth  저장하는 데 걸린 시간 : 2.665971279144287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training:   0%|          | 0/6250 [00:00<?, ?it/s]\u001b[A\n",
      "training:   2%|▏         | 94/6250 [00:10<10:56,  9.38it/s]\u001b[A\n",
      "training:   3%|▎         | 188/6250 [00:21<11:23,  8.87it/s]\u001b[A\n",
      "training:   4%|▍         | 275/6250 [00:31<11:21,  8.76it/s]\u001b[A\n",
      "training:   6%|▌         | 364/6250 [00:41<11:07,  8.81it/s]\u001b[A\n",
      "training:   7%|▋         | 453/6250 [00:51<11:04,  8.73it/s]\u001b[A\n",
      "training:   9%|▊         | 540/6250 [01:01<10:56,  8.69it/s]\u001b[A\n",
      "training:  10%|█         | 634/6250 [01:11<10:31,  8.89it/s]\u001b[A\n",
      "training:  10%|█         | 634/6250 [01:22<10:31,  8.89it/s]\u001b[A\n",
      "training:  11%|█▏        | 717/6250 [01:22<10:49,  8.51it/s]\u001b[A\n",
      "training:  13%|█▎        | 795/6250 [01:32<10:59,  8.27it/s]\u001b[A\n",
      "training:  14%|█▍        | 875/6250 [01:42<10:57,  8.18it/s]\u001b[A\n",
      "training:  15%|█▌        | 957/6250 [01:52<10:49,  8.15it/s]\u001b[A\n",
      "training:  17%|█▋        | 1042/6250 [02:02<10:30,  8.25it/s]\u001b[A\n",
      "training:  18%|█▊        | 1127/6250 [02:13<10:22,  8.23it/s]\u001b[A\n",
      "training:  19%|█▉        | 1209/6250 [02:23<10:25,  8.05it/s]\u001b[A\n",
      "training:  21%|██        | 1290/6250 [02:33<10:16,  8.05it/s]\u001b[A\n",
      "training:  22%|██▏       | 1376/6250 [02:43<09:54,  8.20it/s]\u001b[A\n",
      "training:  23%|██▎       | 1462/6250 [02:54<09:53,  8.07it/s]\u001b[A\n",
      "training:  25%|██▍       | 1544/6250 [03:05<09:42,  8.07it/s]\u001b[A\n",
      "training:  26%|██▌       | 1626/6250 [03:15<09:30,  8.10it/s]\u001b[A\n",
      "training:  27%|██▋       | 1708/6250 [03:25<09:22,  8.07it/s]\u001b[A\n",
      "training:  29%|██▊       | 1788/6250 [03:35<09:20,  7.96it/s]\u001b[A\n",
      "training:  30%|██▉       | 1873/6250 [03:45<09:00,  8.10it/s]\u001b[A\n",
      "training:  31%|███▏      | 1958/6250 [03:56<08:50,  8.09it/s]\u001b[A\n",
      "training:  33%|███▎      | 2039/6250 [04:06<08:46,  8.00it/s]\u001b[A\n",
      "training:  34%|███▍      | 2122/6250 [04:16<08:33,  8.04it/s]\u001b[A\n",
      "training:  36%|███▌      | 2221/6250 [04:26<07:49,  8.58it/s]\u001b[A\n",
      "training:  37%|███▋      | 2320/6250 [04:39<07:50,  8.36it/s]\u001b[A\n",
      "training:  38%|███▊      | 2400/6250 [04:49<07:49,  8.21it/s]\u001b[A\n",
      "training:  40%|███▉      | 2479/6250 [04:59<07:46,  8.08it/s]\u001b[A\n",
      "training:  41%|████      | 2558/6250 [05:09<07:41,  8.01it/s]\u001b[A\n",
      "training:  42%|████▏     | 2641/6250 [05:20<07:26,  8.08it/s]\u001b[A\n",
      "training:  44%|████▎     | 2726/6250 [05:30<07:09,  8.20it/s]\u001b[A\n",
      "training:  45%|████▍     | 2811/6250 [05:40<06:58,  8.22it/s]\u001b[A\n",
      "training:  46%|████▋     | 2897/6250 [05:50<06:43,  8.32it/s]\u001b[A\n",
      "training:  48%|████▊     | 2983/6250 [06:01<06:41,  8.13it/s]\u001b[A\n",
      "training:  49%|████▉     | 3070/6250 [06:11<06:23,  8.29it/s]\u001b[A\n",
      "training:  49%|████▉     | 3070/6250 [06:22<06:23,  8.29it/s]\u001b[A\n",
      "training:  50%|█████     | 3152/6250 [06:22<06:24,  8.05it/s]\u001b[A\n",
      "training:  52%|█████▏    | 3232/6250 [06:32<06:15,  8.03it/s]\u001b[A\n",
      "training:  53%|█████▎    | 3314/6250 [06:42<06:04,  8.06it/s]\u001b[A\n",
      "training:  54%|█████▍    | 3398/6250 [06:52<05:50,  8.14it/s]\u001b[A\n",
      "training:  56%|█████▌    | 3484/6250 [07:02<05:34,  8.26it/s]\u001b[A\n",
      "training:  57%|█████▋    | 3570/6250 [07:13<05:25,  8.24it/s]\u001b[A\n",
      "training:  58%|█████▊    | 3656/6250 [07:23<05:11,  8.33it/s]\u001b[A\n",
      "training:  60%|█████▉    | 3742/6250 [07:33<05:04,  8.24it/s]\u001b[A\n",
      "training:  61%|██████    | 3823/6250 [07:44<04:57,  8.15it/s]\u001b[A\n",
      "training:  63%|██████▎   | 3908/6250 [07:54<04:44,  8.24it/s]\u001b[A\n",
      "training:  64%|██████▍   | 3993/6250 [08:05<04:38,  8.10it/s]\u001b[A\n",
      "training:  65%|██████▌   | 4086/6250 [08:15<04:16,  8.44it/s]\u001b[A\n",
      "training:  67%|██████▋   | 4179/6250 [08:26<04:08,  8.33it/s]\u001b[A\n",
      "training:  68%|██████▊   | 4261/6250 [08:36<04:00,  8.26it/s]\u001b[A\n",
      "training:  69%|██████▉   | 4342/6250 [08:46<03:53,  8.18it/s]\u001b[A\n",
      "training:  71%|███████   | 4422/6250 [08:57<03:47,  8.05it/s]\u001b[A\n",
      "training:  72%|███████▏  | 4502/6250 [09:07<03:38,  8.01it/s]\u001b[A\n",
      "training:  73%|███████▎  | 4582/6250 [09:17<03:28,  7.99it/s]\u001b[A\n",
      "training:  75%|███████▍  | 4664/6250 [09:27<03:17,  8.04it/s]\u001b[A\n",
      "training:  76%|███████▌  | 4747/6250 [09:37<03:05,  8.11it/s]\u001b[A\n",
      "training:  77%|███████▋  | 4830/6250 [09:47<02:56,  8.05it/s]\u001b[A\n",
      "training:  79%|███████▊  | 4912/6250 [09:58<02:45,  8.08it/s]\u001b[A\n",
      "training:  80%|███████▉  | 4994/6250 [10:08<02:36,  8.03it/s]\u001b[A\n",
      "training:  81%|████████  | 5074/6250 [10:19<02:29,  7.86it/s]\u001b[A\n",
      "training:  83%|████████▎ | 5162/6250 [10:29<02:13,  8.14it/s]\u001b[A\n",
      "training:  84%|████████▍ | 5250/6250 [10:39<02:01,  8.26it/s]\u001b[A\n",
      "training:  85%|████████▌ | 5341/6250 [10:49<01:46,  8.50it/s]\u001b[A\n",
      "training:  87%|████████▋ | 5432/6250 [11:00<01:36,  8.51it/s]\u001b[A\n",
      "training:  88%|████████▊ | 5518/6250 [11:10<01:25,  8.52it/s]\u001b[A\n",
      "training:  90%|████████▉ | 5604/6250 [11:21<01:19,  8.16it/s]\u001b[A\n",
      "training:  91%|█████████ | 5698/6250 [11:31<01:04,  8.50it/s]\u001b[A\n",
      "training:  93%|█████████▎| 5792/6250 [11:42<00:52,  8.70it/s]\u001b[A\n",
      "training:  93%|█████████▎| 5792/6250 [11:52<00:52,  8.70it/s]\u001b[A\n",
      "training:  94%|█████████▍| 5877/6250 [11:52<00:43,  8.55it/s]\u001b[A\n",
      "training:  95%|█████████▌| 5960/6250 [12:02<00:34,  8.35it/s]\u001b[A\n",
      "training:  97%|█████████▋| 6039/6250 [12:13<00:26,  8.03it/s]\u001b[A\n",
      "training:  98%|█████████▊| 6113/6250 [12:25<00:18,  7.53it/s]\u001b[A\n",
      "training:  99%|█████████▉| 6208/6250 [12:35<00:05,  8.09it/s]\u001b[A\n",
      "                                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Train Loss: 1.571 | Time: 759.4693460464478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  60%|██████    | 12/20 [2:29:43<1:40:34, 754.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장한 파일 이름 : ./epoch_11.pth  저장하는 데 걸린 시간 : 2.671729564666748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training:   0%|          | 0/6250 [00:00<?, ?it/s]\u001b[A\n",
      "training:   1%|          | 71/6250 [00:10<14:35,  7.06it/s]\u001b[A\n",
      "training:   1%|          | 71/6250 [00:20<14:35,  7.06it/s]\u001b[A\n",
      "training:   2%|▏         | 141/6250 [00:20<14:46,  6.89it/s]\u001b[A\n",
      "training:   3%|▎         | 213/6250 [00:30<14:20,  7.02it/s]\u001b[A\n",
      "training:   5%|▍         | 285/6250 [00:40<14:04,  7.06it/s]\u001b[A\n",
      "training:   6%|▌         | 359/6250 [00:50<13:43,  7.15it/s]\u001b[A\n",
      "training:   7%|▋         | 433/6250 [01:00<13:24,  7.23it/s]\u001b[A\n",
      "training:   8%|▊         | 507/6250 [01:11<13:23,  7.15it/s]\u001b[A\n",
      "training:   9%|▉         | 577/6250 [01:21<13:32,  6.98it/s]\u001b[A\n",
      "training:  11%|█         | 657/6250 [01:31<12:48,  7.28it/s]\u001b[A\n",
      "training:  12%|█▏        | 741/6250 [01:41<12:03,  7.61it/s]\u001b[A\n",
      "training:  13%|█▎        | 825/6250 [01:52<11:36,  7.79it/s]\u001b[A\n",
      "training:  15%|█▍        | 918/6250 [02:02<10:47,  8.24it/s]\u001b[A\n",
      "training:  16%|█▌        | 1015/6250 [02:12<10:04,  8.66it/s]\u001b[A\n",
      "training:  18%|█▊        | 1112/6250 [02:22<09:39,  8.87it/s]\u001b[A\n",
      "training:  19%|█▉        | 1206/6250 [02:33<09:32,  8.81it/s]\u001b[A\n",
      "training:  21%|██        | 1300/6250 [02:43<09:12,  8.96it/s]\u001b[A\n",
      "training:  22%|██▏       | 1394/6250 [02:54<09:04,  8.92it/s]\u001b[A\n",
      "training:  24%|██▍       | 1487/6250 [03:04<08:47,  9.03it/s]\u001b[A\n",
      "training:  25%|██▌       | 1589/6250 [03:14<08:17,  9.36it/s]\u001b[A\n",
      "training:  27%|██▋       | 1693/6250 [03:24<07:51,  9.66it/s]\u001b[A\n",
      "training:  29%|██▉       | 1797/6250 [03:34<07:31,  9.87it/s]\u001b[A\n",
      "training:  30%|███       | 1901/6250 [03:45<07:24,  9.78it/s]\u001b[A\n",
      "training:  32%|███▏      | 2005/6250 [03:55<07:06,  9.95it/s]\u001b[A\n",
      "training:  34%|███▎      | 2109/6250 [04:06<07:03,  9.77it/s]\u001b[A\n",
      "training:  35%|███▌      | 2203/6250 [04:16<07:02,  9.57it/s]\u001b[A\n",
      "training:  37%|███▋      | 2295/6250 [04:27<07:02,  9.35it/s]\u001b[A\n",
      "training:  38%|███▊      | 2395/6250 [04:37<06:45,  9.52it/s]\u001b[A\n",
      "training:  40%|████      | 2500/6250 [04:47<06:22,  9.80it/s]\u001b[A\n",
      "training:  42%|████▏     | 2605/6250 [04:57<06:04,  9.99it/s]\u001b[A\n",
      "training:  43%|████▎     | 2710/6250 [05:07<05:49, 10.13it/s]\u001b[A\n",
      "training:  45%|████▌     | 2815/6250 [05:18<05:44,  9.96it/s]\u001b[A\n",
      "training:  47%|████▋     | 2911/6250 [05:28<05:43,  9.72it/s]\u001b[A\n",
      "training:  48%|████▊     | 3009/6250 [05:38<05:32,  9.74it/s]\u001b[A\n",
      "training:  50%|████▉     | 3107/6250 [05:49<05:30,  9.50it/s]\u001b[A\n",
      "training:  51%|█████     | 3200/6250 [05:59<05:24,  9.41it/s]\u001b[A\n",
      "training:  51%|█████     | 3200/6250 [06:10<05:24,  9.41it/s]\u001b[A\n",
      "training:  53%|█████▎    | 3291/6250 [06:10<05:22,  9.16it/s]\u001b[A\n",
      "training:  54%|█████▍    | 3378/6250 [06:20<05:19,  8.99it/s]\u001b[A\n",
      "training:  55%|█████▌    | 3465/6250 [06:30<05:13,  8.90it/s]\u001b[A\n",
      "training:  57%|█████▋    | 3555/6250 [06:40<05:02,  8.92it/s]\u001b[A\n",
      "training:  58%|█████▊    | 3645/6250 [06:50<04:54,  8.83it/s]\u001b[A\n",
      "training:  60%|█████▉    | 3732/6250 [07:00<04:46,  8.78it/s]\u001b[A\n",
      "training:  61%|██████▏   | 3837/6250 [07:11<04:20,  9.27it/s]\u001b[A\n",
      "training:  63%|██████▎   | 3942/6250 [07:21<04:00,  9.60it/s]\u001b[A\n",
      "training:  65%|██████▍   | 4046/6250 [07:31<03:44,  9.83it/s]\u001b[A\n",
      "training:  66%|██████▋   | 4150/6250 [07:41<03:29, 10.00it/s]\u001b[A\n",
      "training:  68%|██████▊   | 4254/6250 [07:51<03:17, 10.12it/s]\u001b[A\n",
      "training:  70%|██████▉   | 4358/6250 [08:02<03:13,  9.77it/s]\u001b[A\n",
      "training:  71%|███████   | 4451/6250 [08:12<03:06,  9.63it/s]\u001b[A\n",
      "training:  73%|███████▎  | 4553/6250 [08:22<02:53,  9.77it/s]\u001b[A\n",
      "training:  74%|███████▍  | 4655/6250 [08:34<02:47,  9.53it/s]\u001b[A\n",
      "training:  76%|███████▌  | 4756/6250 [08:44<02:34,  9.68it/s]\u001b[A\n",
      "training:  78%|███████▊  | 4861/6250 [08:54<02:20,  9.91it/s]\u001b[A\n",
      "training:  79%|███████▉  | 4966/6250 [09:05<02:10,  9.83it/s]\u001b[A\n",
      "training:  81%|████████  | 5071/6250 [09:15<01:57, 10.01it/s]\u001b[A\n",
      "training:  83%|████████▎ | 5176/6250 [09:26<01:49,  9.78it/s]\u001b[A\n",
      "training:  84%|████████▍ | 5274/6250 [09:36<01:39,  9.78it/s]\u001b[A\n",
      "training:  86%|████████▌ | 5377/6250 [09:46<01:27,  9.92it/s]\u001b[A\n",
      "training:  88%|████████▊ | 5480/6250 [09:57<01:19,  9.65it/s]\u001b[A\n",
      "training:  89%|████████▉ | 5571/6250 [10:08<01:12,  9.40it/s]\u001b[A\n",
      "training:  91%|█████████ | 5660/6250 [10:18<01:03,  9.25it/s]\u001b[A\n",
      "training:  92%|█████████▏| 5760/6250 [10:28<00:51,  9.44it/s]\u001b[A\n",
      "training:  94%|█████████▍| 5861/6250 [10:38<00:40,  9.63it/s]\u001b[A\n",
      "training:  95%|█████████▌| 5962/6250 [10:49<00:30,  9.40it/s]\u001b[A\n",
      "training:  97%|█████████▋| 6060/6250 [10:59<00:19,  9.50it/s]\u001b[A\n",
      "training:  99%|█████████▊| 6165/6250 [11:09<00:08,  9.78it/s]\u001b[A\n",
      "                                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Train Loss: 1.643 | Time: 677.8174583911896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  65%|██████▌   | 13/20 [2:41:04<1:25:23, 731.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장한 파일 이름 : ./epoch_12.pth  저장하는 데 걸린 시간 : 2.645444631576538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training:   0%|          | 0/6250 [00:00<?, ?it/s]\u001b[A\n",
      "training:   1%|▏         | 91/6250 [00:10<11:18,  9.07it/s]\u001b[A\n",
      "training:   3%|▎         | 182/6250 [00:20<11:18,  8.94it/s]\u001b[A\n",
      "training:   4%|▍         | 281/6250 [00:30<10:38,  9.35it/s]\u001b[A\n",
      "training:   6%|▌         | 386/6250 [00:40<09:58,  9.79it/s]\u001b[A\n",
      "training:   8%|▊         | 491/6250 [00:50<09:33, 10.04it/s]\u001b[A\n",
      "training:  10%|▉         | 596/6250 [01:00<09:14, 10.19it/s]\u001b[A\n",
      "training:  11%|█         | 701/6250 [01:12<09:35,  9.65it/s]\u001b[A\n",
      "training:  13%|█▎        | 804/6250 [01:22<09:14,  9.83it/s]\u001b[A\n",
      "training:  15%|█▍        | 907/6250 [01:34<09:29,  9.39it/s]\u001b[A\n",
      "training:  16%|█▌        | 993/6250 [01:44<09:34,  9.16it/s]\u001b[A\n",
      "training:  17%|█▋        | 1088/6250 [01:54<09:19,  9.22it/s]\u001b[A\n",
      "training:  19%|█▉        | 1182/6250 [02:05<09:15,  9.13it/s]\u001b[A\n",
      "training:  20%|██        | 1273/6250 [02:15<09:07,  9.10it/s]\u001b[A\n",
      "training:  22%|██▏       | 1373/6250 [02:25<08:41,  9.36it/s]\u001b[A\n",
      "training:  24%|██▎       | 1473/6250 [02:36<08:41,  9.17it/s]\u001b[A\n",
      "training:  25%|██▍       | 1561/6250 [02:46<08:40,  9.02it/s]\u001b[A\n",
      "training:  26%|██▋       | 1651/6250 [02:57<08:31,  9.00it/s]\u001b[A\n",
      "training:  28%|██▊       | 1749/6250 [03:07<08:07,  9.23it/s]\u001b[A\n",
      "training:  30%|██▉       | 1847/6250 [03:17<07:50,  9.35it/s]\u001b[A\n",
      "training:  31%|███       | 1944/6250 [03:28<07:52,  9.12it/s]\u001b[A\n",
      "training:  33%|███▎      | 2037/6250 [03:38<07:40,  9.16it/s]\u001b[A\n",
      "training:  34%|███▍      | 2139/6250 [03:48<07:15,  9.45it/s]\u001b[A\n",
      "training:  36%|███▌      | 2244/6250 [03:58<06:51,  9.74it/s]\u001b[A\n",
      "training:  38%|███▊      | 2349/6250 [04:09<06:37,  9.82it/s]\u001b[A\n",
      "training:  39%|███▉      | 2450/6250 [04:19<06:28,  9.79it/s]\u001b[A\n",
      "training:  41%|████      | 2547/6250 [04:29<06:21,  9.72it/s]\u001b[A\n",
      "training:  42%|████▏     | 2652/6250 [04:39<06:01,  9.94it/s]\u001b[A\n",
      "training:  42%|████▏     | 2652/6250 [04:49<06:01,  9.94it/s]\u001b[A\n",
      "training:  44%|████▍     | 2742/6250 [04:49<06:05,  9.60it/s]\u001b[A\n",
      "training:  45%|████▌     | 2831/6250 [04:59<06:04,  9.38it/s]\u001b[A\n",
      "training:  47%|████▋     | 2920/6250 [05:09<06:00,  9.23it/s]\u001b[A\n",
      "training:  48%|████▊     | 3015/6250 [05:20<05:47,  9.30it/s]\u001b[A\n",
      "training:  50%|████▉     | 3110/6250 [05:30<05:37,  9.31it/s]\u001b[A\n",
      "training:  51%|█████▏    | 3207/6250 [05:40<05:23,  9.40it/s]\u001b[A\n",
      "training:  53%|█████▎    | 3304/6250 [05:50<05:11,  9.45it/s]\u001b[A\n",
      "training:  55%|█████▍    | 3407/6250 [06:00<04:53,  9.70it/s]\u001b[A\n",
      "training:  56%|█████▌    | 3510/6250 [06:12<04:53,  9.33it/s]\u001b[A\n",
      "training:  58%|█████▊    | 3603/6250 [06:22<04:44,  9.32it/s]\u001b[A\n",
      "training:  59%|█████▉    | 3708/6250 [06:32<04:23,  9.65it/s]\u001b[A\n",
      "training:  61%|██████    | 3813/6250 [06:42<04:06,  9.90it/s]\u001b[A\n",
      "training:  63%|██████▎   | 3918/6250 [06:52<03:51, 10.07it/s]\u001b[A\n",
      "training:  64%|██████▍   | 4023/6250 [07:02<03:38, 10.19it/s]\u001b[A\n",
      "training:  66%|██████▌   | 4128/6250 [07:12<03:26, 10.26it/s]\u001b[A\n",
      "training:  68%|██████▊   | 4233/6250 [07:22<03:15, 10.32it/s]\u001b[A\n",
      "training:  69%|██████▉   | 4338/6250 [07:33<03:07, 10.22it/s]\u001b[A\n",
      "training:  71%|███████   | 4438/6250 [07:43<03:01, 10.01it/s]\u001b[A\n",
      "training:  73%|███████▎  | 4543/6250 [07:53<02:48, 10.14it/s]\u001b[A\n",
      "training:  74%|███████▍  | 4648/6250 [08:05<02:44,  9.73it/s]\u001b[A\n",
      "training:  76%|███████▌  | 4741/6250 [08:15<02:37,  9.60it/s]\u001b[A\n",
      "training:  77%|███████▋  | 4834/6250 [08:25<02:29,  9.46it/s]\u001b[A\n",
      "training:  79%|███████▉  | 4929/6250 [08:35<02:19,  9.47it/s]\u001b[A\n",
      "training:  81%|████████  | 5034/6250 [08:45<02:04,  9.76it/s]\u001b[A\n",
      "training:  82%|████████▏ | 5139/6250 [08:55<01:51,  9.97it/s]\u001b[A\n",
      "training:  84%|████████▍ | 5244/6250 [09:05<01:39, 10.12it/s]\u001b[A\n",
      "training:  86%|████████▌ | 5349/6250 [09:15<01:28, 10.23it/s]\u001b[A\n",
      "training:  87%|████████▋ | 5454/6250 [09:26<01:18, 10.17it/s]\u001b[A\n",
      "training:  89%|████████▉ | 5555/6250 [09:37<01:11,  9.68it/s]\u001b[A\n",
      "training:  90%|█████████ | 5642/6250 [09:48<01:05,  9.28it/s]\u001b[A\n",
      "training:  92%|█████████▏| 5731/6250 [09:58<00:56,  9.17it/s]\u001b[A\n",
      "training:  93%|█████████▎| 5823/6250 [10:08<00:46,  9.16it/s]\u001b[A\n",
      "training:  95%|█████████▍| 5927/6250 [10:18<00:33,  9.51it/s]\u001b[A\n",
      "training:  96%|█████████▋| 6031/6250 [10:28<00:22,  9.77it/s]\u001b[A\n",
      "training:  98%|█████████▊| 6135/6250 [10:38<00:11,  9.93it/s]\u001b[A\n",
      "training:  98%|█████████▊| 6135/6250 [10:49<00:11,  9.93it/s]\u001b[A\n",
      "training: 100%|█████████▉| 6237/6250 [10:49<00:01,  9.64it/s]\u001b[A\n",
      "                                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Train Loss: 1.334 | Time: 651.3061213493347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  70%|███████   | 14/20 [2:51:58<1:10:50, 708.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장한 파일 이름 : ./epoch_13.pth  저장하는 데 걸린 시간 : 2.6476504802703857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training:   0%|          | 0/6250 [00:00<?, ?it/s]\u001b[A\n",
      "training:   1%|▏         | 89/6250 [00:10<11:35,  8.86it/s]\u001b[A\n",
      "training:   3%|▎         | 178/6250 [00:20<11:33,  8.75it/s]\u001b[A\n",
      "training:   4%|▍         | 265/6250 [00:30<11:28,  8.70it/s]\u001b[A\n",
      "training:   6%|▌         | 352/6250 [00:40<11:19,  8.68it/s]\u001b[A\n",
      "training:   7%|▋         | 440/6250 [00:50<11:08,  8.69it/s]\u001b[A\n",
      "training:   9%|▊         | 535/6250 [01:00<10:38,  8.96it/s]\u001b[A\n",
      "training:  10%|█         | 630/6250 [01:11<10:24,  9.00it/s]\u001b[A\n",
      "training:  12%|█▏        | 734/6250 [01:21<09:45,  9.42it/s]\u001b[A\n",
      "training:  13%|█▎        | 838/6250 [01:31<09:28,  9.53it/s]\u001b[A\n",
      "training:  15%|█▍        | 936/6250 [01:42<09:33,  9.26it/s]\u001b[A\n",
      "training:  16%|█▋        | 1024/6250 [01:53<09:41,  8.99it/s]\u001b[A\n",
      "training:  18%|█▊        | 1115/6250 [02:03<09:30,  9.00it/s]\u001b[A\n",
      "training:  19%|█▉        | 1213/6250 [02:13<09:06,  9.22it/s]\u001b[A\n",
      "training:  21%|██        | 1318/6250 [02:23<08:34,  9.60it/s]\u001b[A\n",
      "training:  23%|██▎       | 1423/6250 [02:35<08:29,  9.47it/s]\u001b[A\n",
      "training:  24%|██▍       | 1516/6250 [02:45<08:25,  9.37it/s]\u001b[A\n",
      "training:  26%|██▌       | 1610/6250 [02:55<08:15,  9.36it/s]\u001b[A\n",
      "training:  27%|██▋       | 1705/6250 [03:05<08:04,  9.37it/s]\u001b[A\n",
      "training:  29%|██▉       | 1802/6250 [03:15<07:51,  9.44it/s]\u001b[A\n",
      "training:  29%|██▉       | 1802/6250 [03:25<07:51,  9.44it/s]\u001b[A\n",
      "training:  30%|███       | 1899/6250 [03:26<07:43,  9.39it/s]\u001b[A\n",
      "training:  32%|███▏      | 1992/6250 [03:36<07:38,  9.29it/s]\u001b[A\n",
      "training:  33%|███▎      | 2088/6250 [03:46<07:24,  9.36it/s]\u001b[A\n",
      "training:  35%|███▌      | 2193/6250 [03:56<06:58,  9.69it/s]\u001b[A\n",
      "training:  37%|███▋      | 2298/6250 [04:06<06:38,  9.92it/s]\u001b[A\n",
      "training:  38%|███▊      | 2403/6250 [04:16<06:21, 10.09it/s]\u001b[A\n",
      "training:  40%|████      | 2508/6250 [04:26<06:06, 10.20it/s]\u001b[A\n",
      "training:  42%|████▏     | 2613/6250 [04:36<05:53, 10.29it/s]\u001b[A\n",
      "training:  43%|████▎     | 2718/6250 [04:46<05:41, 10.33it/s]\u001b[A\n",
      "training:  45%|████▌     | 2823/6250 [04:56<05:30, 10.35it/s]\u001b[A\n",
      "training:  47%|████▋     | 2928/6250 [05:06<05:20, 10.37it/s]\u001b[A\n",
      "training:  49%|████▊     | 3033/6250 [05:16<05:09, 10.40it/s]\u001b[A\n",
      "training:  50%|█████     | 3138/6250 [05:26<04:59, 10.37it/s]\u001b[A\n",
      "training:  52%|█████▏    | 3243/6250 [05:36<04:48, 10.41it/s]\u001b[A\n",
      "training:  54%|█████▎    | 3348/6250 [05:46<04:38, 10.43it/s]\u001b[A\n",
      "training:  55%|█████▌    | 3453/6250 [05:58<04:39, 10.02it/s]\u001b[A\n",
      "training:  57%|█████▋    | 3545/6250 [06:08<04:37,  9.73it/s]\u001b[A\n",
      "training:  58%|█████▊    | 3637/6250 [06:18<04:33,  9.55it/s]\u001b[A\n",
      "training:  60%|█████▉    | 3730/6250 [06:28<04:26,  9.46it/s]\u001b[A\n",
      "training:  61%|██████    | 3823/6250 [06:38<04:18,  9.40it/s]\u001b[A\n",
      "training:  63%|██████▎   | 3916/6250 [06:49<04:11,  9.27it/s]\u001b[A\n",
      "training:  64%|██████▍   | 4016/6250 [06:59<03:55,  9.48it/s]\u001b[A\n",
      "training:  66%|██████▌   | 4118/6250 [07:09<03:40,  9.67it/s]\u001b[A\n",
      "training:  68%|██████▊   | 4220/6250 [07:20<03:32,  9.55it/s]\u001b[A\n",
      "training:  69%|██████▉   | 4313/6250 [07:30<03:26,  9.40it/s]\u001b[A\n",
      "training:  70%|███████   | 4404/6250 [07:40<03:19,  9.23it/s]\u001b[A\n",
      "training:  72%|███████▏  | 4496/6250 [07:50<03:10,  9.22it/s]\u001b[A\n",
      "training:  74%|███████▎  | 4601/6250 [08:00<02:52,  9.59it/s]\u001b[A\n",
      "training:  75%|███████▌  | 4706/6250 [08:10<02:37,  9.81it/s]\u001b[A\n",
      "training:  77%|███████▋  | 4810/6250 [08:21<02:27,  9.77it/s]\u001b[A\n",
      "training:  79%|███████▊  | 4908/6250 [08:32<02:18,  9.67it/s]\u001b[A\n",
      "training:  80%|████████  | 5003/6250 [08:42<02:12,  9.43it/s]\u001b[A\n",
      "training:  81%|████████▏ | 5092/6250 [08:52<02:05,  9.25it/s]\u001b[A\n",
      "training:  83%|████████▎ | 5182/6250 [09:02<01:56,  9.17it/s]\u001b[A\n",
      "training:  85%|████████▍ | 5284/6250 [09:12<01:42,  9.46it/s]\u001b[A\n",
      "training:  86%|████████▌ | 5388/6250 [09:23<01:28,  9.71it/s]\u001b[A\n",
      "training:  88%|████████▊ | 5493/6250 [09:33<01:16,  9.93it/s]\u001b[A\n",
      "training:  90%|████████▉ | 5598/6250 [09:43<01:04, 10.05it/s]\u001b[A\n",
      "training:  91%|█████████ | 5702/6250 [09:53<00:54, 10.02it/s]\u001b[A\n",
      "training:  93%|█████████▎| 5802/6250 [10:04<00:45,  9.77it/s]\u001b[A\n",
      "training:  94%|█████████▍| 5901/6250 [10:14<00:35,  9.79it/s]\u001b[A\n",
      "training:  96%|█████████▌| 6006/6250 [10:24<00:24,  9.99it/s]\u001b[A\n",
      "training:  98%|█████████▊| 6111/6250 [10:34<00:13, 10.14it/s]\u001b[A\n",
      "training:  99%|█████████▉| 6216/6250 [10:44<00:03, 10.20it/s]\u001b[A\n",
      "                                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Train Loss: 1.063 | Time: 648.5670912265778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  75%|███████▌  | 15/20 [3:02:49<57:35, 691.14s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장한 파일 이름 : ./epoch_14.pth  저장하는 데 걸린 시간 : 2.6358911991119385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training:   0%|          | 0/6250 [00:00<?, ?it/s]\u001b[A\n",
      "training:   1%|▏         | 84/6250 [00:10<12:22,  8.31it/s]\u001b[A\n",
      "training:   3%|▎         | 173/6250 [00:20<11:44,  8.63it/s]\u001b[A\n",
      "training:   4%|▍         | 262/6250 [00:30<11:31,  8.66it/s]\u001b[A\n",
      "training:   6%|▌         | 350/6250 [00:40<11:19,  8.68it/s]\u001b[A\n",
      "training:   7%|▋         | 438/6250 [00:50<11:14,  8.62it/s]\u001b[A\n",
      "training:   8%|▊         | 524/6250 [01:00<11:05,  8.60it/s]\u001b[A\n",
      "training:  10%|▉         | 613/6250 [01:10<10:50,  8.67it/s]\u001b[A\n",
      "training:  11%|█▏        | 712/6250 [01:21<10:13,  9.03it/s]\u001b[A\n",
      "training:  13%|█▎        | 817/6250 [01:31<09:32,  9.48it/s]\u001b[A\n",
      "training:  15%|█▍        | 922/6250 [01:41<09:04,  9.79it/s]\u001b[A\n",
      "training:  16%|█▋        | 1027/6250 [01:52<09:01,  9.64it/s]\u001b[A\n",
      "training:  18%|█▊        | 1121/6250 [02:02<09:04,  9.42it/s]\u001b[A\n",
      "training:  19%|█▉        | 1211/6250 [02:12<09:02,  9.29it/s]\u001b[A\n",
      "training:  21%|██        | 1302/6250 [02:22<08:55,  9.23it/s]\u001b[A\n",
      "training:  22%|██▏       | 1401/6250 [02:32<08:34,  9.43it/s]\u001b[A\n",
      "training:  24%|██▍       | 1500/6250 [02:43<08:30,  9.30it/s]\u001b[A\n",
      "training:  25%|██▌       | 1593/6250 [02:53<08:21,  9.29it/s]\u001b[A\n",
      "training:  27%|██▋       | 1686/6250 [03:03<08:11,  9.29it/s]\u001b[A\n",
      "training:  29%|██▊       | 1785/6250 [03:13<07:51,  9.46it/s]\u001b[A\n",
      "training:  30%|███       | 1884/6250 [03:24<07:44,  9.39it/s]\u001b[A\n",
      "training:  30%|███       | 1884/6250 [03:34<07:44,  9.39it/s]\u001b[A\n",
      "training:  32%|███▏      | 1978/6250 [03:34<07:36,  9.37it/s]\u001b[A\n",
      "training:  33%|███▎      | 2072/6250 [03:45<07:36,  9.16it/s]\u001b[A\n",
      "training:  35%|███▍      | 2162/6250 [03:55<07:29,  9.10it/s]\u001b[A\n",
      "training:  36%|███▌      | 2261/6250 [04:05<07:07,  9.32it/s]\u001b[A\n",
      "training:  38%|███▊      | 2366/6250 [04:15<06:41,  9.66it/s]\u001b[A\n",
      "training:  40%|███▉      | 2471/6250 [04:26<06:33,  9.60it/s]\u001b[A\n",
      "training:  41%|████      | 2566/6250 [04:37<06:34,  9.34it/s]\u001b[A\n",
      "training:  43%|████▎     | 2662/6250 [04:47<06:21,  9.42it/s]\u001b[A\n",
      "training:  44%|████▍     | 2758/6250 [04:57<06:11,  9.40it/s]\u001b[A\n",
      "training:  46%|████▌     | 2858/6250 [05:07<05:54,  9.58it/s]\u001b[A\n",
      "training:  47%|████▋     | 2963/6250 [05:17<05:34,  9.84it/s]\u001b[A\n",
      "training:  49%|████▉     | 3068/6250 [05:29<05:28,  9.68it/s]\u001b[A\n",
      "training:  51%|█████     | 3162/6250 [05:40<05:30,  9.33it/s]\u001b[A\n",
      "training:  52%|█████▏    | 3264/6250 [05:50<05:12,  9.56it/s]\u001b[A\n",
      "training:  54%|█████▍    | 3366/6250 [06:01<05:05,  9.43it/s]\u001b[A\n",
      "training:  55%|█████▌    | 3458/6250 [06:11<04:58,  9.36it/s]\u001b[A\n",
      "training:  57%|█████▋    | 3550/6250 [06:21<04:52,  9.23it/s]\u001b[A\n",
      "training:  58%|█████▊    | 3652/6250 [06:31<04:33,  9.49it/s]\u001b[A\n",
      "training:  60%|██████    | 3757/6250 [06:41<04:15,  9.77it/s]\u001b[A\n",
      "training:  62%|██████▏   | 3862/6250 [06:52<04:06,  9.67it/s]\u001b[A\n",
      "training:  63%|██████▎   | 3957/6250 [07:03<03:59,  9.57it/s]\u001b[A\n",
      "training:  65%|██████▍   | 4052/6250 [07:13<03:50,  9.54it/s]\u001b[A\n",
      "training:  66%|██████▋   | 4147/6250 [07:23<03:41,  9.47it/s]\u001b[A\n",
      "training:  68%|██████▊   | 4241/6250 [07:33<03:33,  9.40it/s]\u001b[A\n",
      "training:  69%|██████▉   | 4336/6250 [07:43<03:23,  9.42it/s]\u001b[A\n",
      "training:  71%|███████   | 4431/6250 [07:53<03:14,  9.35it/s]\u001b[A\n",
      "training:  73%|███████▎  | 4535/6250 [08:03<02:57,  9.64it/s]\u001b[A\n",
      "training:  74%|███████▍  | 4639/6250 [08:14<02:43,  9.86it/s]\u001b[A\n",
      "training:  76%|███████▌  | 4744/6250 [08:24<02:30, 10.03it/s]\u001b[A\n",
      "training:  78%|███████▊  | 4849/6250 [08:34<02:18, 10.14it/s]\u001b[A\n",
      "training:  79%|███████▉  | 4954/6250 [08:44<02:06, 10.23it/s]\u001b[A\n",
      "training:  81%|████████  | 5059/6250 [08:54<01:55, 10.28it/s]\u001b[A\n",
      "training:  83%|████████▎ | 5163/6250 [09:04<01:45, 10.31it/s]\u001b[A\n",
      "training:  84%|████████▍ | 5267/6250 [09:14<01:35, 10.33it/s]\u001b[A\n",
      "training:  84%|████████▍ | 5267/6250 [09:24<01:35, 10.33it/s]\u001b[A\n",
      "training:  86%|████████▌ | 5365/6250 [09:24<01:28, 10.03it/s]\u001b[A\n",
      "training:  87%|████████▋ | 5459/6250 [09:35<01:20,  9.78it/s]\u001b[A\n",
      "training:  89%|████████▉ | 5551/6250 [09:45<01:13,  9.56it/s]\u001b[A\n",
      "training:  90%|█████████ | 5642/6250 [09:55<01:05,  9.34it/s]\u001b[A\n",
      "training:  92%|█████████▏| 5739/6250 [10:05<00:54,  9.43it/s]\u001b[A\n",
      "training:  93%|█████████▎| 5842/6250 [10:15<00:42,  9.67it/s]\u001b[A\n",
      "training:  95%|█████████▌| 5945/6250 [10:26<00:31,  9.55it/s]\u001b[A\n",
      "training:  97%|█████████▋| 6039/6250 [10:37<00:22,  9.35it/s]\u001b[A\n",
      "training:  98%|█████████▊| 6129/6250 [10:47<00:13,  9.22it/s]\u001b[A\n",
      "training:  99%|█████████▉| 6218/6250 [10:57<00:03,  9.11it/s]\u001b[A\n",
      "                                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Train Loss: 1.136 | Time: 660.973176240921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  80%|████████  | 16/20 [3:13:52<45:31, 682.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장한 파일 이름 : ./epoch_15.pth  저장하는 데 걸린 시간 : 2.63864803314209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training:   0%|          | 0/6250 [00:00<?, ?it/s]\u001b[A\n",
      "training:   1%|▏         | 93/6250 [00:10<11:04,  9.27it/s]\u001b[A\n",
      "training:   3%|▎         | 186/6250 [00:20<11:18,  8.94it/s]\u001b[A\n",
      "training:   4%|▍         | 278/6250 [00:30<11:02,  9.01it/s]\u001b[A\n",
      "training:   4%|▍         | 278/6250 [00:41<11:02,  9.01it/s]\u001b[A\n",
      "training:   6%|▌         | 368/6250 [00:41<11:02,  8.88it/s]\u001b[A\n",
      "training:   7%|▋         | 459/6250 [00:51<10:47,  8.94it/s]\u001b[A\n",
      "training:   9%|▉         | 556/6250 [01:01<10:20,  9.18it/s]\u001b[A\n",
      "training:  10%|█         | 653/6250 [01:11<10:06,  9.23it/s]\u001b[A\n",
      "training:  12%|█▏        | 749/6250 [01:21<09:49,  9.33it/s]\u001b[A\n",
      "training:  14%|█▎        | 854/6250 [01:31<09:17,  9.69it/s]\u001b[A\n",
      "epoch:  80%|████████  | 16/20 [3:15:25<48:51, 732.82s/it]   \u001b[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [60]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m tgt = [배치 사이즈, 타겟 문장 길이] \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 16\u001b[0m logits, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# logits = [배치 사이즈, 타겟 문장 길이, 은닉 차원]\u001b[39;00m\n\u001b[1;32m     19\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, tgt)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m tgt = [배치 사이즈, 타겟 문장 길이] \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m enc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(src)\n\u001b[0;32m---> 13\u001b[0m dec_out, attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dec_out, attn\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [46]\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, tgt, src, enc_out)\u001b[0m\n\u001b[1;32m     52\u001b[0m tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtok_embedding(tgt) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding(tgt)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     tgt, attn_map \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m tgt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(tgt, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtok_embedding\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# tgt = [배치 사이즈, 타겟 문장 길이, 은닉 차원]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [46]\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[0;34m(self, x, tgt_mask, enc_output, src_mask)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m x = [배치 사이즈, 문장 길이, 은닉 차원] \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m residual \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m---> 20\u001b[0m x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m x\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m     20\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_w(q)\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     21\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_w(k)\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# q, k, v = [배치 사이즈, 헤드 갯수, 문장 길이, 어텐션 차원]\u001b[39;00m\n\u001b[1;32m     25\u001b[0m attn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(q, k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 훈련 로직\n",
    "\n",
    "start_tic = time.time()\n",
    "\n",
    "for epoch in tqdm(range(params['num_epoch']), desc=\"epoch\"):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    tic = time.time()\n",
    "    for src, tgt in tqdm(zip(src_iter, tgt_iter), total=len(src_iter), desc=\"training\", leave=False, mininterval=10):\n",
    "        \" src = [배치 사이즈, 소스 문장 길이] \"\n",
    "        \" tgt = [배치 사이즈, 타겟 문장 길이] \"\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, _ = model(src, tgt[:, :-1])\n",
    "        # logits = [배치 사이즈, 타겟 문장 길이, 은닉 차원]\n",
    "\n",
    "        logits = logits.contiguous().view(-1, logits.size(-1))\n",
    "        # logits = [(배치 사이즈 * 타겟 문장 길이) - 1, 은닉 차원]\n",
    "        golds = tgt[:, 1:].contiguous().view(-1)\n",
    "        # golds = [(배치 사이즈 * 타겟 문장 길이) - 1]\n",
    "\n",
    "        loss = criterion(logits, golds)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), self.params.clip)\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = epoch_loss / len(src_iter)\n",
    "    toc = time.time()\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Time: {toc - tic}')\n",
    "\n",
    "    tic = time.time()\n",
    "    # torch.save(model.state_dict(), f'/content/drive/MyDrive/transformer/epoch_{epoch}.pth')\n",
    "    torch.save(model.state_dict(), f'./epoch_{epoch}.pth')\n",
    "    toc = time.time()\n",
    "    print(f\"저장한 파일 이름 : ./epoch_{epoch}.pth  저장하는 데 걸린 시간 : {toc-tic}\")\n",
    "\n",
    "end_tic = time.time()\n",
    "print(\"총 걸린 시간 : \", end_tic - start_tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "j8NlFq58mVz0"
   },
   "outputs": [],
   "source": [
    "# 모델의 state_dict 저장\n",
    "torch.save(model.state_dict(), './final.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6ZPlqnkmVz1"
   },
   "outputs": [],
   "source": [
    "# Transformer 모델 인스턴스 생성 후에, state_dict 불러오기\n",
    "state_dict = torch.load('./my_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ApwKe8hRmVz1",
    "outputId": "e273df54-7085-48e5-d22a-e84aa6f265a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델에 state_dict 로드\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "TB99u3JwmVz1",
    "outputId": "db331a72-e7c8-4513-cdee-a6da836cc4d1",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(20000, 512, padding_idx=0)\n",
       "    (pos_embedding): PositionalEncoding(\n",
       "      (embedding): Embedding(32, 512)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (q_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (o_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (q_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (o_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (q_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (o_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): EncoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (q_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (o_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): EncoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (q_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (o_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): EncoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (q_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (o_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(20000, 512, padding_idx=0)\n",
       "    (pos_embedding): PositionalEncoding(\n",
       "      (embedding): Embedding(32, 512)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (q_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (o_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_dec_attn): MultiHeadAttention(\n",
       "          (q_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (o_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (q_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (o_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_dec_attn): MultiHeadAttention(\n",
       "          (q_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (o_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (q_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (o_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_dec_attn): MultiHeadAttention(\n",
       "          (q_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (o_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (q_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (o_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_dec_attn): MultiHeadAttention(\n",
       "          (q_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (o_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (q_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (o_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_dec_attn): MultiHeadAttention(\n",
       "          (q_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (o_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (q_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (o_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_dec_attn): MultiHeadAttention(\n",
       "          (q_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (o_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델을 평가 모드로 설정\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VjEK924GmVz1",
    "outputId": "76d6b12e-7773-44e9-a30e-67c75acc3d8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Transformer"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghxa3q5XmVz1"
   },
   "source": [
    "## 7. 학습 결과 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40g9qzH3mVz1"
   },
   "source": [
    "이제 학습된 모델이 어느 정도 성능을 보이는지 확인해 볼 차례입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kfCxGkBUmVz2"
   },
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ctWjz23EmVz2",
    "outputId": "f39fa505-756f-4648-94ac-2d8846c84b88"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(20000, 512, padding_idx=0)\n",
       "    (pos_embedding): PositionalEncoding(\n",
       "      (embedding): Embedding(32, 512)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x EncoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (q_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (o_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(20000, 512, padding_idx=0)\n",
       "    (pos_embedding): PositionalEncoding(\n",
       "      (embedding): Embedding(32, 512)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x DecoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (q_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (o_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_dec_attn): MultiHeadAttention(\n",
       "          (q_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (k_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (o_w): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "6GeL9j8nmVz2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델을 평가 모드로 설정\n",
    "model.eval()\n",
    "\n",
    "# 입력 문장\n",
    "sent = '나는 행'\n",
    "\n",
    "# 문장 토큰화\n",
    "proc_sent = kor_tokenizer.encode(sent)\n",
    "\n",
    "# 후처리\n",
    "post_proc_sent = postprocess(proc_sent.ids)\n",
    "\n",
    "# Tensor로 변환 및 배치 차원 추가\n",
    "input_tensor = torch.LongTensor(post_proc_sent).to(device)\n",
    "input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "# 출력 문장 초기화\n",
    "output_tensor = torch.LongTensor([1]).to(device).unsqueeze(0)  # <sos> 토큰 + 배치 차원 추가\n",
    "\n",
    "\n",
    "# 디코딩\n",
    "with torch.no_grad():\n",
    "    for _ in range(1):  # 최대 길이\n",
    "        logits, _ = model(input_tensor, output_tensor)\n",
    "        next_token = logits.argmax(-1)[:,-1]\n",
    "        output_tensor = torch.cat([output_tensor, next_token.unsqueeze(0)], dim=-1)\n",
    "        if next_token.item() == 2:  # <eos> 토큰\n",
    "            break\n",
    "\n",
    "            \n",
    "# 결과 디코딩\n",
    "decoded_output = eng_tokenizer.decode(output_tensor.squeeze().tolist())\n",
    "print(decoded_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ijjgyIn4mVz2",
    "outputId": "db702cab-d073-41d8-e320-956c7a0ace20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s.\n"
     ]
    }
   ],
   "source": [
    "# 모델을 평가 모드로 설정\n",
    "model.eval()\n",
    "\n",
    "# 입력 문장\n",
    "sent = '나'\n",
    "\n",
    "# 문장 토큰화\n",
    "proc_sent = kor_tokenizer.encode(sent)\n",
    "\n",
    "# 후처리\n",
    "post_proc_sent = postprocess(proc_sent.ids)\n",
    "\n",
    "# Tensor로 변환 및 배치 차원 추가\n",
    "input_tensor = torch.LongTensor(post_proc_sent).to(device)\n",
    "input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "# 출력 문장 초기화\n",
    "output_tensor = torch.LongTensor([1]).to(device).unsqueeze(0)  # <sos> 토큰 + 배치 차원 추가\n",
    "\n",
    "# 디코딩\n",
    "with torch.no_grad():\n",
    "    for _ in range(50):  # 최대 길이\n",
    "        logits, _ = model(input_tensor, output_tensor)\n",
    "        next_token = logits.argmax(-1)[:,-1]\n",
    "        output_tensor = torch.cat([output_tensor, next_token.unsqueeze(-1)], dim=-1)\n",
    "        if next_token.item() == 2:  # <eos> 토큰\n",
    "            break\n",
    "\n",
    "# 결과 디코딩\n",
    "decoded_output = eng_tokenizer.decode(output_tensor.squeeze().tolist())\n",
    "print(decoded_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "o6hEglPKmVz3",
    "outputId": "22df4feb-87ca-4117-a4bc-126968056eb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits:  tensor([[[-4.7171,  2.0480, 18.4399,  ..., -0.2575, -0.9721,  2.2967],\n",
      "         [-4.9445,  2.1594, 18.9438,  ..., -0.2129, -0.9844,  2.3649]]],\n",
      "       device='cuda:0')\n",
      "Argmax:  tensor([[2, 2]], device='cuda:0')\n",
      "Output Tensor:  tensor([[1, 2, 2]], device='cuda:0')\n",
      "Decoding Output Tensor:  [1, 2, 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ...\n",
    "# 디코딩\n",
    "with torch.no_grad():\n",
    "    for _ in range(50):  # 최대 길이\n",
    "        logits, _ = model(input_tensor, output_tensor)\n",
    "\n",
    "        print(\"Logits: \", logits)  # 로짓 값 출력\n",
    "        print(\"Argmax: \", logits.argmax(-1))  # 로짓의 argmax 결과 출력\n",
    "\n",
    "        next_token = logits.argmax(-1)[:,-1]\n",
    "        output_tensor = torch.cat([output_tensor, next_token.unsqueeze(0)], dim=-1)\n",
    "\n",
    "        print(\"Output Tensor: \", output_tensor)  # 현재까지의 출력 텐서 값 출력\n",
    "\n",
    "        if next_token.item() == 2:  # <eos> 토큰\n",
    "            break\n",
    "\n",
    "# 결과 디코딩\n",
    "print(\"Decoding Output Tensor: \", output_tensor.squeeze().tolist())\n",
    "decoded_output = eng_tokenizer.decode(output_tensor.squeeze().tolist())\n",
    "print(decoded_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VE_Iwz_0mVz3"
   },
   "source": [
    "### 참고자료\n",
    "- [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "- [jadore801120/attention-is-all-you-need-pytorch](https://github.com/jadore801120/attention-is-all-you-need-pytorch)\n",
    "- [tunz/transformer-pytorch](https://github.com/tunz/transformer-pytorch)\n",
    "- [IgorSusmelj/pytorch-styleguide](https://github.com/IgorSusmelj/pytorch-styleguide)\n",
    "\n",
    "\n",
    "### TODO: 더 추가할 수 있는 것들 !\n",
    "- **Beam Search** 디코딩 추가\n",
    "- **Label Smoothing** 기법 추가"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
